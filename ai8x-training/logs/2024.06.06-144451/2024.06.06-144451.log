2024-06-06 14:44:51,747 - Log file for this run: C:\BChimgam\projectAI\ai8x-training\logs\2024.06.06-144451\2024.06.06-144451.log
2024-06-06 14:44:52,423 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2024-06-06 14:44:52,425 - Optimizer Args: {'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0, 'amsgrad': False}
2024-06-06 14:44:53,653 - Reading compression schedule from: policies/schedule-catsdogs.yaml
2024-06-06 14:44:53,658 - Dataset sizes:
	training=12313
	validation=2172
	test=725
2024-06-06 14:44:53,658 - 

2024-06-06 14:44:53,658 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 14:45:23,334 - Epoch: [0][   10/  193]    Overall Loss 2.082781    Objective Loss 2.082781                                        LR 0.001000    Time 2.967534    
2024-06-06 14:45:31,469 - Epoch: [0][   20/  193]    Overall Loss 1.898534    Objective Loss 1.898534                                        LR 0.001000    Time 1.888451    
2024-06-06 14:45:39,551 - Epoch: [0][   30/  193]    Overall Loss 1.715062    Objective Loss 1.715062                                        LR 0.001000    Time 1.527234    
2024-06-06 14:45:47,780 - Epoch: [0][   40/  193]    Overall Loss 1.527211    Objective Loss 1.527211                                        LR 0.001000    Time 1.350138    
2024-06-06 14:45:56,186 - Epoch: [0][   50/  193]    Overall Loss 1.365395    Objective Loss 1.365395                                        LR 0.001000    Time 1.247509    
2024-06-06 14:46:04,694 - Epoch: [0][   60/  193]    Overall Loss 1.242489    Objective Loss 1.242489                                        LR 0.001000    Time 1.180863    
2024-06-06 14:46:13,215 - Epoch: [0][   70/  193]    Overall Loss 1.132125    Objective Loss 1.132125                                        LR 0.001000    Time 1.133373    
2024-06-06 14:46:21,778 - Epoch: [0][   80/  193]    Overall Loss 1.043259    Objective Loss 1.043259                                        LR 0.001000    Time 1.098113    
2024-06-06 14:46:30,519 - Epoch: [0][   90/  193]    Overall Loss 0.965966    Objective Loss 0.965966                                        LR 0.001000    Time 1.072610    
2024-06-06 14:46:39,735 - Epoch: [0][  100/  193]    Overall Loss 0.901791    Objective Loss 0.901791                                        LR 0.001000    Time 1.056936    
2024-06-06 14:46:48,711 - Epoch: [0][  110/  193]    Overall Loss 0.844653    Objective Loss 0.844653                                        LR 0.001000    Time 1.042079    
2024-06-06 14:46:57,650 - Epoch: [0][  120/  193]    Overall Loss 0.793795    Objective Loss 0.793795                                        LR 0.001000    Time 1.029314    
2024-06-06 14:47:06,226 - Epoch: [0][  130/  193]    Overall Loss 0.749770    Objective Loss 0.749770                                        LR 0.001000    Time 1.015735    
2024-06-06 14:47:16,379 - Epoch: [0][  140/  193]    Overall Loss 0.709787    Objective Loss 0.709787                                        LR 0.001000    Time 1.015422    
2024-06-06 14:47:25,290 - Epoch: [0][  150/  193]    Overall Loss 0.673197    Objective Loss 0.673197                                        LR 0.001000    Time 1.006831    
2024-06-06 14:47:34,151 - Epoch: [0][  160/  193]    Overall Loss 0.640993    Objective Loss 0.640993                                        LR 0.001000    Time 0.999019    
2024-06-06 14:47:43,271 - Epoch: [0][  170/  193]    Overall Loss 0.611378    Objective Loss 0.611378                                        LR 0.001000    Time 0.993584    
2024-06-06 14:47:52,221 - Epoch: [0][  180/  193]    Overall Loss 0.584507    Objective Loss 0.584507                                        LR 0.001000    Time 0.987878    
2024-06-06 14:48:01,263 - Epoch: [0][  190/  193]    Overall Loss 0.558970    Objective Loss 0.558970                                        LR 0.001000    Time 0.983264    
2024-06-06 14:48:03,353 - Epoch: [0][  193/  193]    Overall Loss 0.552208    Objective Loss 0.552208    Top1 96.629213    Top5 100.000000    LR 0.001000    Time 0.978599    
2024-06-06 14:48:04,064 - --- validate (epoch=0)-----------
2024-06-06 14:48:04,065 - 2172 samples (64 per mini-batch)
2024-06-06 14:48:31,268 - Epoch: [0][   10/   34]    Loss 0.113057    Top1 97.812500    Top5 100.000000    
2024-06-06 14:48:37,811 - Epoch: [0][   20/   34]    Loss 0.119479    Top1 97.421875    Top5 100.000000    
2024-06-06 14:48:43,043 - Epoch: [0][   30/   34]    Loss 0.118966    Top1 97.500000    Top5 100.000000    
2024-06-06 14:48:44,569 - Epoch: [0][   34/   34]    Loss 0.120406    Top1 97.421731    Top5 100.000000    
2024-06-06 14:48:45,912 - ==> Top1: 97.422    Top5: 100.000    Loss: 0.120

2024-06-06 14:48:45,913 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 227   1   0   0   0   0   0]
 [  0   0   0 315   2   2   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  1   1   0   0   0   0 244   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  1   0   0   0   0   0  48   0 242]]

2024-06-06 14:48:46,175 - ==> Best [Top1: 97.422   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 0]
2024-06-06 14:48:46,176 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 14:48:46,192 - 

2024-06-06 14:48:46,193 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 14:49:16,841 - Epoch: [1][   10/  193]    Overall Loss 0.100103    Objective Loss 0.100103                                        LR 0.001000    Time 3.064804    
2024-06-06 14:49:26,522 - Epoch: [1][   20/  193]    Overall Loss 0.098760    Objective Loss 0.098760                                        LR 0.001000    Time 2.014424    
2024-06-06 14:49:42,304 - Epoch: [1][   30/  193]    Overall Loss 0.089432    Objective Loss 0.089432                                        LR 0.001000    Time 1.867076    
2024-06-06 14:49:51,600 - Epoch: [1][   40/  193]    Overall Loss 0.086827    Objective Loss 0.086827                                        LR 0.001000    Time 1.630904    
2024-06-06 14:50:00,883 - Epoch: [1][   50/  193]    Overall Loss 0.084193    Objective Loss 0.084193                                        LR 0.001000    Time 1.489326    
2024-06-06 14:50:09,628 - Epoch: [1][   60/  193]    Overall Loss 0.081295    Objective Loss 0.081295                                        LR 0.001000    Time 1.386315    
2024-06-06 14:50:18,258 - Epoch: [1][   70/  193]    Overall Loss 0.076429    Objective Loss 0.076429                                        LR 0.001000    Time 1.311185    
2024-06-06 14:50:27,003 - Epoch: [1][   80/  193]    Overall Loss 0.072885    Objective Loss 0.072885                                        LR 0.001000    Time 1.256056    
2024-06-06 14:50:35,532 - Epoch: [1][   90/  193]    Overall Loss 0.070104    Objective Loss 0.070104                                        LR 0.001000    Time 1.210928    
2024-06-06 14:50:43,948 - Epoch: [1][  100/  193]    Overall Loss 0.067833    Objective Loss 0.067833                                        LR 0.001000    Time 1.173643    
2024-06-06 14:50:52,483 - Epoch: [1][  110/  193]    Overall Loss 0.065679    Objective Loss 0.065679                                        LR 0.001000    Time 1.144235    
2024-06-06 14:51:01,289 - Epoch: [1][  120/  193]    Overall Loss 0.064466    Objective Loss 0.064466                                        LR 0.001000    Time 1.121925    
2024-06-06 14:51:09,710 - Epoch: [1][  130/  193]    Overall Loss 0.062389    Objective Loss 0.062389                                        LR 0.001000    Time 1.100137    
2024-06-06 14:51:18,103 - Epoch: [1][  140/  193]    Overall Loss 0.060761    Objective Loss 0.060761                                        LR 0.001000    Time 1.081269    
2024-06-06 14:51:26,509 - Epoch: [1][  150/  193]    Overall Loss 0.058451    Objective Loss 0.058451                                        LR 0.001000    Time 1.065001    
2024-06-06 14:51:34,859 - Epoch: [1][  160/  193]    Overall Loss 0.056639    Objective Loss 0.056639                                        LR 0.001000    Time 1.050307    
2024-06-06 14:51:42,924 - Epoch: [1][  170/  193]    Overall Loss 0.054920    Objective Loss 0.054920                                        LR 0.001000    Time 1.035785    
2024-06-06 14:51:51,259 - Epoch: [1][  180/  193]    Overall Loss 0.053171    Objective Loss 0.053171                                        LR 0.001000    Time 1.024287    
2024-06-06 14:51:59,556 - Epoch: [1][  190/  193]    Overall Loss 0.051357    Objective Loss 0.051357                                        LR 0.001000    Time 1.013871    
2024-06-06 14:52:01,626 - Epoch: [1][  193/  193]    Overall Loss 0.050717    Objective Loss 0.050717    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 1.008660    
2024-06-06 14:52:02,278 - --- validate (epoch=1)-----------
2024-06-06 14:52:02,278 - 2172 samples (64 per mini-batch)
2024-06-06 14:52:26,756 - Epoch: [1][   10/   34]    Loss 0.015479    Top1 100.000000    Top5 100.000000    
2024-06-06 14:52:30,495 - Epoch: [1][   20/   34]    Loss 0.017390    Top1 100.000000    Top5 100.000000    
2024-06-06 14:52:34,159 - Epoch: [1][   30/   34]    Loss 0.017047    Top1 99.843750    Top5 100.000000    
2024-06-06 14:52:35,424 - Epoch: [1][   34/   34]    Loss 0.017216    Top1 99.815838    Top5 100.000000    
2024-06-06 14:52:36,386 - ==> Top1: 99.816    Top5: 100.000    Loss: 0.017

2024-06-06 14:52:36,390 - ==> Confusion:
[[226   0   0   0   0   0   1   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 243   0   3]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 14:52:36,578 - ==> Best [Top1: 99.816   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 1]
2024-06-06 14:52:36,578 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 14:52:36,593 - 

2024-06-06 14:52:36,593 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 14:53:06,404 - Epoch: [2][   10/  193]    Overall Loss 0.019050    Objective Loss 0.019050                                        LR 0.001000    Time 2.981013    
2024-06-06 14:53:15,121 - Epoch: [2][   20/  193]    Overall Loss 0.022028    Objective Loss 0.022028                                        LR 0.001000    Time 1.924975    
2024-06-06 14:53:23,716 - Epoch: [2][   30/  193]    Overall Loss 0.022754    Objective Loss 0.022754                                        LR 0.001000    Time 1.568813    
2024-06-06 14:53:32,505 - Epoch: [2][   40/  193]    Overall Loss 0.027488    Objective Loss 0.027488                                        LR 0.001000    Time 1.395543    
2024-06-06 14:53:41,060 - Epoch: [2][   50/  193]    Overall Loss 0.027876    Objective Loss 0.027876                                        LR 0.001000    Time 1.286939    
2024-06-06 14:53:49,550 - Epoch: [2][   60/  193]    Overall Loss 0.029730    Objective Loss 0.029730                                        LR 0.001000    Time 1.213515    
2024-06-06 14:53:58,338 - Epoch: [2][   70/  193]    Overall Loss 0.030572    Objective Loss 0.030572                                        LR 0.001000    Time 1.165388    
2024-06-06 14:54:07,222 - Epoch: [2][   80/  193]    Overall Loss 0.029929    Objective Loss 0.029929                                        LR 0.001000    Time 1.130334    
2024-06-06 14:54:16,672 - Epoch: [2][   90/  193]    Overall Loss 0.028186    Objective Loss 0.028186                                        LR 0.001000    Time 1.109308    
2024-06-06 14:54:25,803 - Epoch: [2][  100/  193]    Overall Loss 0.026892    Objective Loss 0.026892                                        LR 0.001000    Time 1.089438    
2024-06-06 14:54:34,664 - Epoch: [2][  110/  193]    Overall Loss 0.025236    Objective Loss 0.025236                                        LR 0.001000    Time 1.070736    
2024-06-06 14:54:43,628 - Epoch: [2][  120/  193]    Overall Loss 0.024190    Objective Loss 0.024190                                        LR 0.001000    Time 1.055929    
2024-06-06 14:54:58,475 - Epoch: [2][  130/  193]    Overall Loss 0.023270    Objective Loss 0.023270                                        LR 0.001000    Time 1.088632    
2024-06-06 14:55:08,887 - Epoch: [2][  140/  193]    Overall Loss 0.022352    Objective Loss 0.022352                                        LR 0.001000    Time 1.084713    
2024-06-06 14:55:18,180 - Epoch: [2][  150/  193]    Overall Loss 0.021715    Objective Loss 0.021715                                        LR 0.001000    Time 1.074139    
2024-06-06 14:55:27,877 - Epoch: [2][  160/  193]    Overall Loss 0.021710    Objective Loss 0.021710                                        LR 0.001000    Time 1.067324    
2024-06-06 14:55:37,669 - Epoch: [2][  170/  193]    Overall Loss 0.021027    Objective Loss 0.021027                                        LR 0.001000    Time 1.061782    
2024-06-06 14:55:47,806 - Epoch: [2][  180/  193]    Overall Loss 0.020455    Objective Loss 0.020455                                        LR 0.001000    Time 1.058892    
2024-06-06 14:55:58,159 - Epoch: [2][  190/  193]    Overall Loss 0.019900    Objective Loss 0.019900                                        LR 0.001000    Time 1.057353    
2024-06-06 14:56:00,582 - Epoch: [2][  193/  193]    Overall Loss 0.019652    Objective Loss 0.019652    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 1.053056    
2024-06-06 14:56:01,296 - --- validate (epoch=2)-----------
2024-06-06 14:56:01,296 - 2172 samples (64 per mini-batch)
2024-06-06 14:56:38,694 - Epoch: [2][   10/   34]    Loss 0.008383    Top1 100.000000    Top5 100.000000    
2024-06-06 14:56:42,693 - Epoch: [2][   20/   34]    Loss 0.010021    Top1 100.000000    Top5 100.000000    
2024-06-06 14:56:46,427 - Epoch: [2][   30/   34]    Loss 0.009339    Top1 100.000000    Top5 100.000000    
2024-06-06 14:56:47,758 - Epoch: [2][   34/   34]    Loss 0.011982    Top1 99.953959    Top5 100.000000    
2024-06-06 14:56:48,841 - ==> Top1: 99.954    Top5: 100.000    Loss: 0.012

2024-06-06 14:56:48,842 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 318   0   0   1   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 14:56:49,110 - ==> Best [Top1: 99.954   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 2]
2024-06-06 14:56:49,110 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 14:56:49,126 - 

2024-06-06 14:56:49,127 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 14:57:25,873 - Epoch: [3][   10/  193]    Overall Loss 0.009717    Objective Loss 0.009717                                        LR 0.001000    Time 3.674588    
2024-06-06 14:57:38,258 - Epoch: [3][   20/  193]    Overall Loss 0.011964    Objective Loss 0.011964                                        LR 0.001000    Time 2.454647    
2024-06-06 14:57:48,270 - Epoch: [3][   30/  193]    Overall Loss 0.010713    Objective Loss 0.010713                                        LR 0.001000    Time 1.968526    
2024-06-06 14:57:57,913 - Epoch: [3][   40/  193]    Overall Loss 0.010311    Objective Loss 0.010311                                        LR 0.001000    Time 1.716519    
2024-06-06 14:58:09,194 - Epoch: [3][   50/  193]    Overall Loss 0.010842    Objective Loss 0.010842                                        LR 0.001000    Time 1.597745    
2024-06-06 14:58:24,179 - Epoch: [3][   60/  193]    Overall Loss 0.011069    Objective Loss 0.011069                                        LR 0.001000    Time 1.579990    
2024-06-06 14:58:36,310 - Epoch: [3][   70/  193]    Overall Loss 0.011263    Objective Loss 0.011263                                        LR 0.001000    Time 1.526690    
2024-06-06 14:58:45,901 - Epoch: [3][   80/  193]    Overall Loss 0.011648    Objective Loss 0.011648                                        LR 0.001000    Time 1.455072    
2024-06-06 14:58:55,300 - Epoch: [3][   90/  193]    Overall Loss 0.011494    Objective Loss 0.011494                                        LR 0.001000    Time 1.397357    
2024-06-06 14:59:05,103 - Epoch: [3][  100/  193]    Overall Loss 0.011411    Objective Loss 0.011411                                        LR 0.001000    Time 1.355252    
2024-06-06 14:59:15,491 - Epoch: [3][  110/  193]    Overall Loss 0.011178    Objective Loss 0.011178                                        LR 0.001000    Time 1.326029    
2024-06-06 14:59:24,841 - Epoch: [3][  120/  193]    Overall Loss 0.010687    Objective Loss 0.010687                                        LR 0.001000    Time 1.293134    
2024-06-06 14:59:34,522 - Epoch: [3][  130/  193]    Overall Loss 0.010275    Objective Loss 0.010275                                        LR 0.001000    Time 1.267748    
2024-06-06 14:59:44,206 - Epoch: [3][  140/  193]    Overall Loss 0.009830    Objective Loss 0.009830                                        LR 0.001000    Time 1.246112    
2024-06-06 14:59:53,731 - Epoch: [3][  150/  193]    Overall Loss 0.009509    Objective Loss 0.009509                                        LR 0.001000    Time 1.226261    
2024-06-06 15:00:03,628 - Epoch: [3][  160/  193]    Overall Loss 0.009710    Objective Loss 0.009710                                        LR 0.001000    Time 1.211151    
2024-06-06 15:00:13,505 - Epoch: [3][  170/  193]    Overall Loss 0.009554    Objective Loss 0.009554                                        LR 0.001000    Time 1.197693    
2024-06-06 15:00:25,539 - Epoch: [3][  180/  193]    Overall Loss 0.009319    Objective Loss 0.009319                                        LR 0.001000    Time 1.197778    
2024-06-06 15:00:35,608 - Epoch: [3][  190/  193]    Overall Loss 0.009074    Objective Loss 0.009074                                        LR 0.001000    Time 1.187150    
2024-06-06 15:00:37,781 - Epoch: [3][  193/  193]    Overall Loss 0.009007    Objective Loss 0.009007    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 1.179737    
2024-06-06 15:00:38,510 - --- validate (epoch=3)-----------
2024-06-06 15:00:38,510 - 2172 samples (64 per mini-batch)
2024-06-06 15:01:06,091 - Epoch: [3][   10/   34]    Loss 0.003595    Top1 100.000000    Top5 100.000000    
2024-06-06 15:01:10,406 - Epoch: [3][   20/   34]    Loss 0.004533    Top1 100.000000    Top5 100.000000    
2024-06-06 15:01:14,567 - Epoch: [3][   30/   34]    Loss 0.004103    Top1 100.000000    Top5 100.000000    
2024-06-06 15:01:16,038 - Epoch: [3][   34/   34]    Loss 0.004090    Top1 100.000000    Top5 100.000000    
2024-06-06 15:01:17,092 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.004

2024-06-06 15:01:17,092 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 15:01:17,328 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 3]
2024-06-06 15:01:17,328 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 15:01:17,347 - 

2024-06-06 15:01:17,347 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 15:01:49,476 - Epoch: [4][   10/  193]    Overall Loss 0.003748    Objective Loss 0.003748                                        LR 0.001000    Time 3.212936    
2024-06-06 15:01:59,496 - Epoch: [4][   20/  193]    Overall Loss 0.004066    Objective Loss 0.004066                                        LR 0.001000    Time 2.105583    
2024-06-06 15:02:09,722 - Epoch: [4][   30/  193]    Overall Loss 0.003783    Objective Loss 0.003783                                        LR 0.001000    Time 1.743056    
2024-06-06 15:02:19,831 - Epoch: [4][   40/  193]    Overall Loss 0.003987    Objective Loss 0.003987                                        LR 0.001000    Time 1.558970    
2024-06-06 15:02:29,684 - Epoch: [4][   50/  193]    Overall Loss 0.006849    Objective Loss 0.006849                                        LR 0.001000    Time 1.443468    
2024-06-06 15:02:39,376 - Epoch: [4][   60/  193]    Overall Loss 0.019866    Objective Loss 0.019866                                        LR 0.001000    Time 1.363760    
2024-06-06 15:02:49,282 - Epoch: [4][   70/  193]    Overall Loss 0.022151    Objective Loss 0.022151                                        LR 0.001000    Time 1.309777    
2024-06-06 15:03:00,322 - Epoch: [4][   80/  193]    Overall Loss 0.022914    Objective Loss 0.022914                                        LR 0.001000    Time 1.283673    
2024-06-06 15:03:10,394 - Epoch: [4][   90/  193]    Overall Loss 0.021829    Objective Loss 0.021829                                        LR 0.001000    Time 1.252468    
2024-06-06 15:03:20,561 - Epoch: [4][  100/  193]    Overall Loss 0.020485    Objective Loss 0.020485                                        LR 0.001000    Time 1.228510    
2024-06-06 15:03:30,498 - Epoch: [4][  110/  193]    Overall Loss 0.019398    Objective Loss 0.019398                                        LR 0.001000    Time 1.206839    
2024-06-06 15:03:40,769 - Epoch: [4][  120/  193]    Overall Loss 0.018412    Objective Loss 0.018412                                        LR 0.001000    Time 1.191523    
2024-06-06 15:03:50,754 - Epoch: [4][  130/  193]    Overall Loss 0.018019    Objective Loss 0.018019                                        LR 0.001000    Time 1.176384    
2024-06-06 15:04:01,145 - Epoch: [4][  140/  193]    Overall Loss 0.017418    Objective Loss 0.017418                                        LR 0.001000    Time 1.166266    
2024-06-06 15:04:11,526 - Epoch: [4][  150/  193]    Overall Loss 0.016751    Objective Loss 0.016751                                        LR 0.001000    Time 1.157460    
2024-06-06 15:04:21,580 - Epoch: [4][  160/  193]    Overall Loss 0.016195    Objective Loss 0.016195                                        LR 0.001000    Time 1.147729    
2024-06-06 15:04:31,911 - Epoch: [4][  170/  193]    Overall Loss 0.015598    Objective Loss 0.015598                                        LR 0.001000    Time 1.140730    
2024-06-06 15:04:41,903 - Epoch: [4][  180/  193]    Overall Loss 0.015107    Objective Loss 0.015107                                        LR 0.001000    Time 1.132641    
2024-06-06 15:04:51,516 - Epoch: [4][  190/  193]    Overall Loss 0.014526    Objective Loss 0.014526                                        LR 0.001000    Time 1.123431    
2024-06-06 15:04:53,844 - Epoch: [4][  193/  193]    Overall Loss 0.014359    Objective Loss 0.014359    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 1.117772    
2024-06-06 15:04:54,569 - --- validate (epoch=4)-----------
2024-06-06 15:04:54,570 - 2172 samples (64 per mini-batch)
2024-06-06 15:05:23,797 - Epoch: [4][   10/   34]    Loss 0.010538    Top1 99.843750    Top5 100.000000    
2024-06-06 15:05:28,867 - Epoch: [4][   20/   34]    Loss 0.009064    Top1 99.843750    Top5 100.000000    
2024-06-06 15:05:33,120 - Epoch: [4][   30/   34]    Loss 0.008525    Top1 99.895833    Top5 100.000000    
2024-06-06 15:05:34,731 - Epoch: [4][   34/   34]    Loss 0.009240    Top1 99.815838    Top5 100.000000    
2024-06-06 15:05:35,990 - ==> Top1: 99.816    Top5: 100.000    Loss: 0.009

2024-06-06 15:05:35,991 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   4 309   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 15:05:36,631 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 3]
2024-06-06 15:05:36,632 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 15:05:36,647 - 

2024-06-06 15:05:36,648 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 15:06:14,643 - Epoch: [5][   10/  193]    Overall Loss 0.011936    Objective Loss 0.011936                                        LR 0.001000    Time 3.799434    
2024-06-06 15:06:26,471 - Epoch: [5][   20/  193]    Overall Loss 0.010050    Objective Loss 0.010050                                        LR 0.001000    Time 2.487814    
2024-06-06 15:06:39,116 - Epoch: [5][   30/  193]    Overall Loss 0.008204    Objective Loss 0.008204                                        LR 0.001000    Time 2.077187    
2024-06-06 15:06:49,498 - Epoch: [5][   40/  193]    Overall Loss 0.006915    Objective Loss 0.006915                                        LR 0.001000    Time 1.815991    
2024-06-06 15:06:59,620 - Epoch: [5][   50/  193]    Overall Loss 0.005927    Objective Loss 0.005927                                        LR 0.001000    Time 1.654384    
2024-06-06 15:07:10,247 - Epoch: [5][   60/  193]    Overall Loss 0.005229    Objective Loss 0.005229                                        LR 0.001000    Time 1.555208    
2024-06-06 15:07:21,199 - Epoch: [5][   70/  193]    Overall Loss 0.005171    Objective Loss 0.005171                                        LR 0.001000    Time 1.488757    
2024-06-06 15:07:32,877 - Epoch: [5][   80/  193]    Overall Loss 0.004838    Objective Loss 0.004838                                        LR 0.001000    Time 1.447866    
2024-06-06 15:07:43,602 - Epoch: [5][   90/  193]    Overall Loss 0.004662    Objective Loss 0.004662                                        LR 0.001000    Time 1.405803    
2024-06-06 15:07:54,489 - Epoch: [5][  100/  193]    Overall Loss 0.004432    Objective Loss 0.004432                                        LR 0.001000    Time 1.373611    
2024-06-06 15:08:05,847 - Epoch: [5][  110/  193]    Overall Loss 0.004153    Objective Loss 0.004153                                        LR 0.001000    Time 1.351417    
2024-06-06 15:08:16,150 - Epoch: [5][  120/  193]    Overall Loss 0.003923    Objective Loss 0.003923                                        LR 0.001000    Time 1.324330    
2024-06-06 15:08:26,410 - Epoch: [5][  130/  193]    Overall Loss 0.003783    Objective Loss 0.003783                                        LR 0.001000    Time 1.301096    
2024-06-06 15:08:37,504 - Epoch: [5][  140/  193]    Overall Loss 0.003632    Objective Loss 0.003632                                        LR 0.001000    Time 1.287127    
2024-06-06 15:08:48,780 - Epoch: [5][  150/  193]    Overall Loss 0.003517    Objective Loss 0.003517                                        LR 0.001000    Time 1.276118    
2024-06-06 15:09:00,494 - Epoch: [5][  160/  193]    Overall Loss 0.003433    Objective Loss 0.003433                                        LR 0.001000    Time 1.269143    
2024-06-06 15:09:13,386 - Epoch: [5][  170/  193]    Overall Loss 0.003383    Objective Loss 0.003383                                        LR 0.001000    Time 1.269913    
2024-06-06 15:09:26,034 - Epoch: [5][  180/  193]    Overall Loss 0.003556    Objective Loss 0.003556                                        LR 0.001000    Time 1.269290    
2024-06-06 15:09:36,944 - Epoch: [5][  190/  193]    Overall Loss 0.003615    Objective Loss 0.003615                                        LR 0.001000    Time 1.259598    
2024-06-06 15:09:39,577 - Epoch: [5][  193/  193]    Overall Loss 0.003585    Objective Loss 0.003585    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 1.253324    
2024-06-06 15:09:40,350 - --- validate (epoch=5)-----------
2024-06-06 15:09:40,350 - 2172 samples (64 per mini-batch)
2024-06-06 15:10:14,952 - Epoch: [5][   10/   34]    Loss 0.001715    Top1 100.000000    Top5 100.000000    
2024-06-06 15:10:20,473 - Epoch: [5][   20/   34]    Loss 0.002665    Top1 100.000000    Top5 100.000000    
2024-06-06 15:10:25,607 - Epoch: [5][   30/   34]    Loss 0.002588    Top1 100.000000    Top5 100.000000    
2024-06-06 15:10:27,390 - Epoch: [5][   34/   34]    Loss 0.002480    Top1 100.000000    Top5 100.000000    
2024-06-06 15:10:28,526 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.002

2024-06-06 15:10:28,526 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 15:10:28,790 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 5]
2024-06-06 15:10:28,790 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 15:10:28,811 - 

2024-06-06 15:10:28,811 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 15:11:04,252 - Epoch: [6][   10/  193]    Overall Loss 0.001635    Objective Loss 0.001635                                        LR 0.001000    Time 3.543944    
2024-06-06 15:11:14,697 - Epoch: [6][   20/  193]    Overall Loss 0.001794    Objective Loss 0.001794                                        LR 0.001000    Time 2.291504    
2024-06-06 15:11:25,340 - Epoch: [6][   30/  193]    Overall Loss 0.001676    Objective Loss 0.001676                                        LR 0.001000    Time 1.880757    
2024-06-06 15:11:36,213 - Epoch: [6][   40/  193]    Overall Loss 0.001695    Objective Loss 0.001695                                        LR 0.001000    Time 1.681282    
2024-06-06 15:11:46,815 - Epoch: [6][   50/  193]    Overall Loss 0.001654    Objective Loss 0.001654                                        LR 0.001000    Time 1.556097    
2024-06-06 15:11:57,869 - Epoch: [6][   60/  193]    Overall Loss 0.001605    Objective Loss 0.001605                                        LR 0.001000    Time 1.480283    
2024-06-06 15:12:10,555 - Epoch: [6][   70/  193]    Overall Loss 0.001702    Objective Loss 0.001702                                        LR 0.001000    Time 1.449083    
2024-06-06 15:12:21,351 - Epoch: [6][   80/  193]    Overall Loss 0.001674    Objective Loss 0.001674                                        LR 0.001000    Time 1.402029    
2024-06-06 15:12:32,850 - Epoch: [6][   90/  193]    Overall Loss 0.001710    Objective Loss 0.001710                                        LR 0.001000    Time 1.373615    
2024-06-06 15:12:44,016 - Epoch: [6][  100/  193]    Overall Loss 0.001673    Objective Loss 0.001673                                        LR 0.001000    Time 1.347291    
2024-06-06 15:12:55,365 - Epoch: [6][  110/  193]    Overall Loss 0.001631    Objective Loss 0.001631                                        LR 0.001000    Time 1.327422    
2024-06-06 15:13:07,289 - Epoch: [6][  120/  193]    Overall Loss 0.001587    Objective Loss 0.001587                                        LR 0.001000    Time 1.315637    
2024-06-06 15:13:18,861 - Epoch: [6][  130/  193]    Overall Loss 0.001551    Objective Loss 0.001551                                        LR 0.001000    Time 1.302986    
2024-06-06 15:13:30,616 - Epoch: [6][  140/  193]    Overall Loss 0.001505    Objective Loss 0.001505                                        LR 0.001000    Time 1.293430    
2024-06-06 15:13:41,814 - Epoch: [6][  150/  193]    Overall Loss 0.001488    Objective Loss 0.001488                                        LR 0.001000    Time 1.281154    
2024-06-06 15:13:53,334 - Epoch: [6][  160/  193]    Overall Loss 0.001504    Objective Loss 0.001504                                        LR 0.001000    Time 1.272573    
2024-06-06 15:14:05,011 - Epoch: [6][  170/  193]    Overall Loss 0.001478    Objective Loss 0.001478                                        LR 0.001000    Time 1.266174    
2024-06-06 15:14:16,741 - Epoch: [6][  180/  193]    Overall Loss 0.001443    Objective Loss 0.001443                                        LR 0.001000    Time 1.260697    
2024-06-06 15:14:28,039 - Epoch: [6][  190/  193]    Overall Loss 0.001423    Objective Loss 0.001423                                        LR 0.001000    Time 1.253480    
2024-06-06 15:14:31,096 - Epoch: [6][  193/  193]    Overall Loss 0.001412    Objective Loss 0.001412    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 1.249495    
2024-06-06 15:14:32,120 - --- validate (epoch=6)-----------
2024-06-06 15:14:32,121 - 2172 samples (64 per mini-batch)
2024-06-06 15:15:09,641 - Epoch: [6][   10/   34]    Loss 0.003463    Top1 99.843750    Top5 100.000000    
2024-06-06 15:15:15,532 - Epoch: [6][   20/   34]    Loss 0.003356    Top1 99.921875    Top5 100.000000    
2024-06-06 15:15:21,753 - Epoch: [6][   30/   34]    Loss 0.002629    Top1 99.947917    Top5 100.000000    
2024-06-06 15:15:25,135 - Epoch: [6][   34/   34]    Loss 0.002422    Top1 99.953959    Top5 100.000000    
2024-06-06 15:15:27,030 - ==> Top1: 99.954    Top5: 100.000    Loss: 0.002

2024-06-06 15:15:27,031 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  1   0   0   0   0  53   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 15:15:27,612 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 5]
2024-06-06 15:15:27,614 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 15:15:27,630 - 

2024-06-06 15:15:27,630 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 15:16:11,581 - Epoch: [7][   10/  193]    Overall Loss 0.001105    Objective Loss 0.001105                                        LR 0.001000    Time 4.394841    
2024-06-06 15:16:22,334 - Epoch: [7][   20/  193]    Overall Loss 0.002093    Objective Loss 0.002093                                        LR 0.001000    Time 2.731379    
2024-06-06 15:16:33,382 - Epoch: [7][   30/  193]    Overall Loss 0.003639    Objective Loss 0.003639                                        LR 0.001000    Time 2.187803    
2024-06-06 15:16:43,940 - Epoch: [7][   40/  193]    Overall Loss 0.004682    Objective Loss 0.004682                                        LR 0.001000    Time 1.903906    
2024-06-06 15:16:54,910 - Epoch: [7][   50/  193]    Overall Loss 0.004543    Objective Loss 0.004543                                        LR 0.001000    Time 1.741691    
2024-06-06 15:17:06,025 - Epoch: [7][   60/  193]    Overall Loss 0.004251    Objective Loss 0.004251                                        LR 0.001000    Time 1.635867    
2024-06-06 15:17:17,650 - Epoch: [7][   70/  193]    Overall Loss 0.004504    Objective Loss 0.004504                                        LR 0.001000    Time 1.567245    
2024-06-06 15:17:29,685 - Epoch: [7][   80/  193]    Overall Loss 0.004205    Objective Loss 0.004205                                        LR 0.001000    Time 1.520922    
2024-06-06 15:17:42,494 - Epoch: [7][   90/  193]    Overall Loss 0.003968    Objective Loss 0.003968                                        LR 0.001000    Time 1.492999    
2024-06-06 15:17:53,901 - Epoch: [7][  100/  193]    Overall Loss 0.003754    Objective Loss 0.003754                                        LR 0.001000    Time 1.457039    
2024-06-06 15:18:05,125 - Epoch: [7][  110/  193]    Overall Loss 0.003496    Objective Loss 0.003496                                        LR 0.001000    Time 1.426087    
2024-06-06 15:18:16,476 - Epoch: [7][  120/  193]    Overall Loss 0.003306    Objective Loss 0.003306                                        LR 0.001000    Time 1.401591    
2024-06-06 15:18:27,565 - Epoch: [7][  130/  193]    Overall Loss 0.003143    Objective Loss 0.003143                                        LR 0.001000    Time 1.378712    
2024-06-06 15:18:38,861 - Epoch: [7][  140/  193]    Overall Loss 0.002988    Objective Loss 0.002988                                        LR 0.001000    Time 1.360506    
2024-06-06 15:18:50,247 - Epoch: [7][  150/  193]    Overall Loss 0.003003    Objective Loss 0.003003                                        LR 0.001000    Time 1.345417    
2024-06-06 15:19:01,663 - Epoch: [7][  160/  193]    Overall Loss 0.003104    Objective Loss 0.003104                                        LR 0.001000    Time 1.332272    
2024-06-06 15:19:12,804 - Epoch: [7][  170/  193]    Overall Loss 0.003285    Objective Loss 0.003285                                        LR 0.001000    Time 1.319128    
2024-06-06 15:19:23,131 - Epoch: [7][  180/  193]    Overall Loss 0.003363    Objective Loss 0.003363                                        LR 0.001000    Time 1.302965    
2024-06-06 15:19:33,379 - Epoch: [7][  190/  193]    Overall Loss 0.003496    Objective Loss 0.003496                                        LR 0.001000    Time 1.288063    
2024-06-06 15:19:35,932 - Epoch: [7][  193/  193]    Overall Loss 0.003486    Objective Loss 0.003486    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 1.281035    
2024-06-06 15:19:36,729 - --- validate (epoch=7)-----------
2024-06-06 15:19:36,730 - 2172 samples (64 per mini-batch)
2024-06-06 15:20:08,624 - Epoch: [7][   10/   34]    Loss 0.001588    Top1 100.000000    Top5 100.000000    
2024-06-06 15:20:14,739 - Epoch: [7][   20/   34]    Loss 0.003019    Top1 100.000000    Top5 100.000000    
2024-06-06 15:20:21,027 - Epoch: [7][   30/   34]    Loss 0.003788    Top1 99.947917    Top5 100.000000    
2024-06-06 15:20:23,077 - Epoch: [7][   34/   34]    Loss 0.003590    Top1 99.953959    Top5 100.000000    
2024-06-06 15:20:24,446 - ==> Top1: 99.954    Top5: 100.000    Loss: 0.004

2024-06-06 15:20:24,447 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 318   0   1   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 15:20:24,912 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 5]
2024-06-06 15:20:24,913 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 15:20:24,920 - 

2024-06-06 15:20:24,920 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 15:21:03,935 - Epoch: [8][   10/  193]    Overall Loss 0.001922    Objective Loss 0.001922                                        LR 0.001000    Time 3.901402    
2024-06-06 15:21:18,493 - Epoch: [8][   20/  193]    Overall Loss 0.001769    Objective Loss 0.001769                                        LR 0.001000    Time 2.675571    
2024-06-06 15:21:33,291 - Epoch: [8][   30/  193]    Overall Loss 0.001538    Objective Loss 0.001538                                        LR 0.001000    Time 2.273989    
2024-06-06 15:21:45,108 - Epoch: [8][   40/  193]    Overall Loss 0.001729    Objective Loss 0.001729                                        LR 0.001000    Time 1.999219    
2024-06-06 15:21:57,269 - Epoch: [8][   50/  193]    Overall Loss 0.001491    Objective Loss 0.001491                                        LR 0.001000    Time 1.841632    
2024-06-06 15:22:08,351 - Epoch: [8][   60/  193]    Overall Loss 0.001423    Objective Loss 0.001423                                        LR 0.001000    Time 1.718473    
2024-06-06 15:22:20,382 - Epoch: [8][   70/  193]    Overall Loss 0.001475    Objective Loss 0.001475                                        LR 0.001000    Time 1.644218    
2024-06-06 15:22:31,558 - Epoch: [8][   80/  193]    Overall Loss 0.001401    Objective Loss 0.001401                                        LR 0.001000    Time 1.577925    
2024-06-06 15:22:42,775 - Epoch: [8][   90/  193]    Overall Loss 0.001427    Objective Loss 0.001427                                        LR 0.001000    Time 1.526687    
2024-06-06 15:22:54,372 - Epoch: [8][  100/  193]    Overall Loss 0.001412    Objective Loss 0.001412                                        LR 0.001000    Time 1.489413    
2024-06-06 15:23:06,782 - Epoch: [8][  110/  193]    Overall Loss 0.001351    Objective Loss 0.001351                                        LR 0.001000    Time 1.466394    
2024-06-06 15:23:17,783 - Epoch: [8][  120/  193]    Overall Loss 0.001298    Objective Loss 0.001298                                        LR 0.001000    Time 1.435391    
2024-06-06 15:23:29,112 - Epoch: [8][  130/  193]    Overall Loss 0.001250    Objective Loss 0.001250                                        LR 0.001000    Time 1.411663    
2024-06-06 15:23:40,277 - Epoch: [8][  140/  193]    Overall Loss 0.001198    Objective Loss 0.001198                                        LR 0.001000    Time 1.390292    
2024-06-06 15:23:53,955 - Epoch: [8][  150/  193]    Overall Loss 0.001190    Objective Loss 0.001190                                        LR 0.001000    Time 1.388351    
2024-06-06 15:24:07,445 - Epoch: [8][  160/  193]    Overall Loss 0.001200    Objective Loss 0.001200                                        LR 0.001000    Time 1.385495    
2024-06-06 15:24:19,745 - Epoch: [8][  170/  193]    Overall Loss 0.001178    Objective Loss 0.001178                                        LR 0.001000    Time 1.376079    
2024-06-06 15:24:31,742 - Epoch: [8][  180/  193]    Overall Loss 0.001148    Objective Loss 0.001148                                        LR 0.001000    Time 1.365945    
2024-06-06 15:24:42,776 - Epoch: [8][  190/  193]    Overall Loss 0.001121    Objective Loss 0.001121                                        LR 0.001000    Time 1.351677    
2024-06-06 15:24:45,257 - Epoch: [8][  193/  193]    Overall Loss 0.001110    Objective Loss 0.001110    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 1.343229    
2024-06-06 15:24:46,055 - --- validate (epoch=8)-----------
2024-06-06 15:24:46,055 - 2172 samples (64 per mini-batch)
2024-06-06 15:25:15,887 - Epoch: [8][   10/   34]    Loss 0.000526    Top1 100.000000    Top5 100.000000    
2024-06-06 15:25:20,921 - Epoch: [8][   20/   34]    Loss 0.000786    Top1 100.000000    Top5 100.000000    
2024-06-06 15:25:25,633 - Epoch: [8][   30/   34]    Loss 0.000734    Top1 100.000000    Top5 100.000000    
2024-06-06 15:25:27,349 - Epoch: [8][   34/   34]    Loss 0.000738    Top1 100.000000    Top5 100.000000    
2024-06-06 15:25:28,515 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.001

2024-06-06 15:25:28,516 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 15:25:28,814 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 8]
2024-06-06 15:25:28,814 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 15:25:28,835 - 

2024-06-06 15:25:28,835 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 15:26:15,131 - Epoch: [9][   10/  193]    Overall Loss 0.000493    Objective Loss 0.000493                                        LR 0.001000    Time 4.629404    
2024-06-06 15:26:26,735 - Epoch: [9][   20/  193]    Overall Loss 0.000491    Objective Loss 0.000491                                        LR 0.001000    Time 2.891255    
2024-06-06 15:26:38,983 - Epoch: [9][   30/  193]    Overall Loss 0.000466    Objective Loss 0.000466                                        LR 0.001000    Time 2.333786    
2024-06-06 15:26:51,472 - Epoch: [9][   40/  193]    Overall Loss 0.000428    Objective Loss 0.000428                                        LR 0.001000    Time 2.060188    
2024-06-06 15:27:03,966 - Epoch: [9][   50/  193]    Overall Loss 0.000415    Objective Loss 0.000415                                        LR 0.001000    Time 1.896156    
2024-06-06 15:27:16,187 - Epoch: [9][   60/  193]    Overall Loss 0.000485    Objective Loss 0.000485                                        LR 0.001000    Time 1.782410    
2024-06-06 15:27:28,261 - Epoch: [9][   70/  193]    Overall Loss 0.000635    Objective Loss 0.000635                                        LR 0.001000    Time 1.698800    
2024-06-06 15:27:40,031 - Epoch: [9][   80/  193]    Overall Loss 0.000652    Objective Loss 0.000652                                        LR 0.001000    Time 1.632672    
2024-06-06 15:27:52,034 - Epoch: [9][   90/  193]    Overall Loss 0.000646    Objective Loss 0.000646                                        LR 0.001000    Time 1.584000    
2024-06-06 15:28:05,184 - Epoch: [9][  100/  193]    Overall Loss 0.000653    Objective Loss 0.000653                                        LR 0.001000    Time 1.556304    
2024-06-06 15:28:18,938 - Epoch: [9][  110/  193]    Overall Loss 0.000633    Objective Loss 0.000633                                        LR 0.001000    Time 1.538899    
2024-06-06 15:28:33,523 - Epoch: [9][  120/  193]    Overall Loss 0.000615    Objective Loss 0.000615                                        LR 0.001000    Time 1.531310    
2024-06-06 15:28:48,622 - Epoch: [9][  130/  193]    Overall Loss 0.000598    Objective Loss 0.000598                                        LR 0.001000    Time 1.529040    
2024-06-06 15:29:02,183 - Epoch: [9][  140/  193]    Overall Loss 0.000595    Objective Loss 0.000595                                        LR 0.001000    Time 1.516097    
2024-06-06 15:29:15,707 - Epoch: [9][  150/  193]    Overall Loss 0.000591    Objective Loss 0.000591                                        LR 0.001000    Time 1.504636    
2024-06-06 15:29:29,061 - Epoch: [9][  160/  193]    Overall Loss 0.000592    Objective Loss 0.000592                                        LR 0.001000    Time 1.493611    
2024-06-06 15:29:42,028 - Epoch: [9][  170/  193]    Overall Loss 0.000574    Objective Loss 0.000574                                        LR 0.001000    Time 1.481535    
2024-06-06 15:29:55,990 - Epoch: [9][  180/  193]    Overall Loss 0.000574    Objective Loss 0.000574                                        LR 0.001000    Time 1.476342    
2024-06-06 15:30:09,972 - Epoch: [9][  190/  193]    Overall Loss 0.000577    Objective Loss 0.000577                                        LR 0.001000    Time 1.471811    
2024-06-06 15:30:13,144 - Epoch: [9][  193/  193]    Overall Loss 0.000579    Objective Loss 0.000579    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 1.464855    
2024-06-06 15:30:14,291 - --- validate (epoch=9)-----------
2024-06-06 15:30:14,291 - 2172 samples (64 per mini-batch)
2024-06-06 15:30:59,473 - Epoch: [9][   10/   34]    Loss 0.000365    Top1 100.000000    Top5 100.000000    
2024-06-06 15:31:06,356 - Epoch: [9][   20/   34]    Loss 0.000381    Top1 100.000000    Top5 100.000000    
2024-06-06 15:31:13,062 - Epoch: [9][   30/   34]    Loss 0.000442    Top1 100.000000    Top5 100.000000    
2024-06-06 15:31:15,494 - Epoch: [9][   34/   34]    Loss 0.000431    Top1 100.000000    Top5 100.000000    
2024-06-06 15:31:17,155 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 15:31:17,156 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 15:31:17,677 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 9]
2024-06-06 15:31:17,678 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 15:31:17,706 - 

2024-06-06 15:31:17,707 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 15:32:24,397 - Epoch: [10][   10/  193]    Overall Loss 0.000567    Objective Loss 0.000567                                        LR 0.001000    Time 6.668849    
2024-06-06 15:32:39,943 - Epoch: [10][   20/  193]    Overall Loss 0.000565    Objective Loss 0.000565                                        LR 0.001000    Time 4.107417    
2024-06-06 15:32:54,936 - Epoch: [10][   30/  193]    Overall Loss 0.000578    Objective Loss 0.000578                                        LR 0.001000    Time 3.235307    
2024-06-06 15:33:09,702 - Epoch: [10][   40/  193]    Overall Loss 0.000541    Objective Loss 0.000541                                        LR 0.001000    Time 2.793732    
2024-06-06 15:33:24,609 - Epoch: [10][   50/  193]    Overall Loss 0.000483    Objective Loss 0.000483                                        LR 0.001000    Time 2.531805    
2024-06-06 15:33:37,988 - Epoch: [10][   60/  193]    Overall Loss 0.000468    Objective Loss 0.000468                                        LR 0.001000    Time 2.331790    
2024-06-06 15:33:49,734 - Epoch: [10][   70/  193]    Overall Loss 0.000505    Objective Loss 0.000505                                        LR 0.001000    Time 2.165666    
2024-06-06 15:34:01,351 - Epoch: [10][   80/  193]    Overall Loss 0.000524    Objective Loss 0.000524                                        LR 0.001000    Time 2.039431    
2024-06-06 15:34:13,265 - Epoch: [10][   90/  193]    Overall Loss 0.000521    Objective Loss 0.000521                                        LR 0.001000    Time 1.944719    
2024-06-06 15:34:26,047 - Epoch: [10][  100/  193]    Overall Loss 0.000530    Objective Loss 0.000530                                        LR 0.001000    Time 1.877414    
2024-06-06 15:34:41,854 - Epoch: [10][  110/  193]    Overall Loss 0.000505    Objective Loss 0.000505                                        LR 0.001000    Time 1.849305    
2024-06-06 15:34:53,940 - Epoch: [10][  120/  193]    Overall Loss 0.000492    Objective Loss 0.000492                                        LR 0.001000    Time 1.795393    
2024-06-06 15:35:05,795 - Epoch: [10][  130/  193]    Overall Loss 0.000598    Objective Loss 0.000598                                        LR 0.001000    Time 1.748000    
2024-06-06 15:35:17,785 - Epoch: [10][  140/  193]    Overall Loss 0.000604    Objective Loss 0.000604                                        LR 0.001000    Time 1.708435    
2024-06-06 15:35:30,261 - Epoch: [10][  150/  193]    Overall Loss 0.000607    Objective Loss 0.000607                                        LR 0.001000    Time 1.677167    
2024-06-06 15:35:42,982 - Epoch: [10][  160/  193]    Overall Loss 0.000599    Objective Loss 0.000599                                        LR 0.001000    Time 1.651346    
2024-06-06 15:35:55,222 - Epoch: [10][  170/  193]    Overall Loss 0.000595    Objective Loss 0.000595                                        LR 0.001000    Time 1.625797    
2024-06-06 15:36:07,071 - Epoch: [10][  180/  193]    Overall Loss 0.000608    Objective Loss 0.000608                                        LR 0.001000    Time 1.601050    
2024-06-06 15:36:18,994 - Epoch: [10][  190/  193]    Overall Loss 0.000596    Objective Loss 0.000596                                        LR 0.001000    Time 1.579125    
2024-06-06 15:36:21,866 - Epoch: [10][  193/  193]    Overall Loss 0.000592    Objective Loss 0.000592    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 1.569048    
2024-06-06 15:36:22,874 - --- validate (epoch=10)-----------
2024-06-06 15:36:22,875 - 2172 samples (64 per mini-batch)
2024-06-06 15:36:55,545 - Epoch: [10][   10/   34]    Loss 0.000335    Top1 100.000000    Top5 100.000000    
2024-06-06 15:37:01,383 - Epoch: [10][   20/   34]    Loss 0.000757    Top1 100.000000    Top5 100.000000    
2024-06-06 15:37:06,420 - Epoch: [10][   30/   34]    Loss 0.000626    Top1 100.000000    Top5 100.000000    
2024-06-06 15:37:08,140 - Epoch: [10][   34/   34]    Loss 0.000576    Top1 100.000000    Top5 100.000000    
2024-06-06 15:37:09,494 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.001

2024-06-06 15:37:09,495 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 15:37:09,855 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 10]
2024-06-06 15:37:09,855 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 15:37:09,884 - 

2024-06-06 15:37:09,884 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 15:37:46,724 - Epoch: [11][   10/  193]    Overall Loss 0.000478    Objective Loss 0.000478                                        LR 0.001000    Time 3.683887    
2024-06-06 15:37:57,064 - Epoch: [11][   20/  193]    Overall Loss 0.000564    Objective Loss 0.000564                                        LR 0.001000    Time 2.357092    
2024-06-06 15:38:07,145 - Epoch: [11][   30/  193]    Overall Loss 0.000505    Objective Loss 0.000505                                        LR 0.001000    Time 1.906065    
2024-06-06 15:38:17,332 - Epoch: [11][   40/  193]    Overall Loss 0.000438    Objective Loss 0.000438                                        LR 0.001000    Time 1.683384    
2024-06-06 15:38:27,390 - Epoch: [11][   50/  193]    Overall Loss 0.000424    Objective Loss 0.000424                                        LR 0.001000    Time 1.546660    
2024-06-06 15:38:37,488 - Epoch: [11][   60/  193]    Overall Loss 0.000443    Objective Loss 0.000443                                        LR 0.001000    Time 1.456618    
2024-06-06 15:38:48,049 - Epoch: [11][   70/  193]    Overall Loss 0.000530    Objective Loss 0.000530                                        LR 0.001000    Time 1.398720    
2024-06-06 15:38:58,278 - Epoch: [11][   80/  193]    Overall Loss 0.000559    Objective Loss 0.000559                                        LR 0.001000    Time 1.351207    
2024-06-06 15:39:08,321 - Epoch: [11][   90/  193]    Overall Loss 0.000583    Objective Loss 0.000583                                        LR 0.001000    Time 1.312238    
2024-06-06 15:39:18,427 - Epoch: [11][  100/  193]    Overall Loss 0.000562    Objective Loss 0.000562                                        LR 0.001000    Time 1.281759    
2024-06-06 15:39:28,559 - Epoch: [11][  110/  193]    Overall Loss 0.000538    Objective Loss 0.000538                                        LR 0.001000    Time 1.256957    
2024-06-06 15:39:38,526 - Epoch: [11][  120/  193]    Overall Loss 0.000517    Objective Loss 0.000517                                        LR 0.001000    Time 1.234979    
2024-06-06 15:39:48,471 - Epoch: [11][  130/  193]    Overall Loss 0.000506    Objective Loss 0.000506                                        LR 0.001000    Time 1.216137    
2024-06-06 15:39:58,524 - Epoch: [11][  140/  193]    Overall Loss 0.000485    Objective Loss 0.000485                                        LR 0.001000    Time 1.200826    
2024-06-06 15:40:08,504 - Epoch: [11][  150/  193]    Overall Loss 0.000471    Objective Loss 0.000471                                        LR 0.001000    Time 1.187065    
2024-06-06 15:40:18,523 - Epoch: [11][  160/  193]    Overall Loss 0.000461    Objective Loss 0.000461                                        LR 0.001000    Time 1.175216    
2024-06-06 15:40:29,715 - Epoch: [11][  170/  193]    Overall Loss 0.000447    Objective Loss 0.000447                                        LR 0.001000    Time 1.171674    
2024-06-06 15:40:40,101 - Epoch: [11][  180/  193]    Overall Loss 0.000470    Objective Loss 0.000470                                        LR 0.001000    Time 1.163976    
2024-06-06 15:40:50,215 - Epoch: [11][  190/  193]    Overall Loss 0.000549    Objective Loss 0.000549                                        LR 0.001000    Time 1.155515    
2024-06-06 15:40:52,497 - Epoch: [11][  193/  193]    Overall Loss 0.000695    Objective Loss 0.000695    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 1.149191    
2024-06-06 15:40:53,258 - --- validate (epoch=11)-----------
2024-06-06 15:40:53,258 - 2172 samples (64 per mini-batch)
2024-06-06 15:41:32,536 - Epoch: [11][   10/   34]    Loss 0.000345    Top1 100.000000    Top5 100.000000    
2024-06-06 15:41:37,702 - Epoch: [11][   20/   34]    Loss 0.000378    Top1 100.000000    Top5 100.000000    
2024-06-06 15:41:42,583 - Epoch: [11][   30/   34]    Loss 0.000508    Top1 100.000000    Top5 100.000000    
2024-06-06 15:41:44,322 - Epoch: [11][   34/   34]    Loss 0.000471    Top1 100.000000    Top5 100.000000    
2024-06-06 15:41:45,521 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 15:41:45,521 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 15:41:45,819 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 11]
2024-06-06 15:41:45,820 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 15:41:45,845 - 

2024-06-06 15:41:45,845 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 15:42:21,753 - Epoch: [12][   10/  193]    Overall Loss 0.075817    Objective Loss 0.075817                                        LR 0.001000    Time 3.590782    
2024-06-06 15:42:32,019 - Epoch: [12][   20/  193]    Overall Loss 0.059850    Objective Loss 0.059850                                        LR 0.001000    Time 2.306584    
2024-06-06 15:42:43,151 - Epoch: [12][   30/  193]    Overall Loss 0.067828    Objective Loss 0.067828                                        LR 0.001000    Time 1.907346    
2024-06-06 15:43:00,932 - Epoch: [12][   40/  193]    Overall Loss 0.058178    Objective Loss 0.058178                                        LR 0.001000    Time 1.868497    
2024-06-06 15:43:11,965 - Epoch: [12][   50/  193]    Overall Loss 0.048512    Objective Loss 0.048512                                        LR 0.001000    Time 1.714582    
2024-06-06 15:43:25,841 - Epoch: [12][   60/  193]    Overall Loss 0.042031    Objective Loss 0.042031                                        LR 0.001000    Time 1.658559    
2024-06-06 15:43:37,405 - Epoch: [12][   70/  193]    Overall Loss 0.037112    Objective Loss 0.037112                                        LR 0.001000    Time 1.586239    
2024-06-06 15:43:50,007 - Epoch: [12][   80/  193]    Overall Loss 0.032892    Objective Loss 0.032892                                        LR 0.001000    Time 1.544485    
2024-06-06 15:44:02,777 - Epoch: [12][   90/  193]    Overall Loss 0.029588    Objective Loss 0.029588                                        LR 0.001000    Time 1.514284    
2024-06-06 15:44:14,180 - Epoch: [12][  100/  193]    Overall Loss 0.027497    Objective Loss 0.027497                                        LR 0.001000    Time 1.476500    
2024-06-06 15:44:25,441 - Epoch: [12][  110/  193]    Overall Loss 0.025164    Objective Loss 0.025164                                        LR 0.001000    Time 1.444237    
2024-06-06 15:44:35,838 - Epoch: [12][  120/  193]    Overall Loss 0.023174    Objective Loss 0.023174                                        LR 0.001000    Time 1.410209    
2024-06-06 15:44:46,450 - Epoch: [12][  130/  193]    Overall Loss 0.021479    Objective Loss 0.021479                                        LR 0.001000    Time 1.383021    
2024-06-06 15:44:57,348 - Epoch: [12][  140/  193]    Overall Loss 0.020018    Objective Loss 0.020018                                        LR 0.001000    Time 1.361788    
2024-06-06 15:45:08,322 - Epoch: [12][  150/  193]    Overall Loss 0.018774    Objective Loss 0.018774                                        LR 0.001000    Time 1.343670    
2024-06-06 15:45:19,743 - Epoch: [12][  160/  193]    Overall Loss 0.017758    Objective Loss 0.017758                                        LR 0.001000    Time 1.330821    
2024-06-06 15:45:31,130 - Epoch: [12][  170/  193]    Overall Loss 0.016750    Objective Loss 0.016750                                        LR 0.001000    Time 1.319323    
2024-06-06 15:45:42,900 - Epoch: [12][  180/  193]    Overall Loss 0.015939    Objective Loss 0.015939                                        LR 0.001000    Time 1.310926    
2024-06-06 15:45:52,879 - Epoch: [12][  190/  193]    Overall Loss 0.015213    Objective Loss 0.015213                                        LR 0.001000    Time 1.294259    
2024-06-06 15:45:55,386 - Epoch: [12][  193/  193]    Overall Loss 0.015008    Objective Loss 0.015008    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 1.286890    
2024-06-06 15:45:56,351 - --- validate (epoch=12)-----------
2024-06-06 15:45:56,351 - 2172 samples (64 per mini-batch)
2024-06-06 15:46:31,691 - Epoch: [12][   10/   34]    Loss 0.015931    Top1 99.687500    Top5 100.000000    
2024-06-06 15:46:38,719 - Epoch: [12][   20/   34]    Loss 0.014498    Top1 99.687500    Top5 100.000000    
2024-06-06 15:46:43,428 - Epoch: [12][   30/   34]    Loss 0.013681    Top1 99.635417    Top5 100.000000    
2024-06-06 15:46:45,527 - Epoch: [12][   34/   34]    Loss 0.015597    Top1 99.631676    Top5 100.000000    
2024-06-06 15:46:46,916 - ==> Top1: 99.632    Top5: 100.000    Loss: 0.016

2024-06-06 15:46:46,917 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   8   0 283]]

2024-06-06 15:46:47,288 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 11]
2024-06-06 15:46:47,289 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 15:46:47,301 - 

2024-06-06 15:46:47,302 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 15:47:25,573 - Epoch: [13][   10/  193]    Overall Loss 0.011683    Objective Loss 0.011683                                        LR 0.001000    Time 3.826913    
2024-06-06 15:47:36,156 - Epoch: [13][   20/  193]    Overall Loss 0.008082    Objective Loss 0.008082                                        LR 0.001000    Time 2.440067    
2024-06-06 15:47:46,396 - Epoch: [13][   30/  193]    Overall Loss 0.006002    Objective Loss 0.006002                                        LR 0.001000    Time 1.966502    
2024-06-06 15:47:56,863 - Epoch: [13][   40/  193]    Overall Loss 0.005273    Objective Loss 0.005273                                        LR 0.001000    Time 1.735542    
2024-06-06 15:48:07,136 - Epoch: [13][   50/  193]    Overall Loss 0.004369    Objective Loss 0.004369                                        LR 0.001000    Time 1.593072    
2024-06-06 15:48:17,252 - Epoch: [13][   60/  193]    Overall Loss 0.003772    Objective Loss 0.003772                                        LR 0.001000    Time 1.495608    
2024-06-06 15:48:27,333 - Epoch: [13][   70/  193]    Overall Loss 0.003527    Objective Loss 0.003527                                        LR 0.001000    Time 1.425324    
2024-06-06 15:48:37,357 - Epoch: [13][   80/  193]    Overall Loss 0.003314    Objective Loss 0.003314                                        LR 0.001000    Time 1.371957    
2024-06-06 15:48:47,360 - Epoch: [13][   90/  193]    Overall Loss 0.003105    Objective Loss 0.003105                                        LR 0.001000    Time 1.330180    
2024-06-06 15:48:57,476 - Epoch: [13][  100/  193]    Overall Loss 0.002862    Objective Loss 0.002862                                        LR 0.001000    Time 1.297935    
2024-06-06 15:49:08,185 - Epoch: [13][  110/  193]    Overall Loss 0.002651    Objective Loss 0.002651                                        LR 0.001000    Time 1.276732    
2024-06-06 15:49:18,648 - Epoch: [13][  120/  193]    Overall Loss 0.002486    Objective Loss 0.002486                                        LR 0.001000    Time 1.257203    
2024-06-06 15:49:29,003 - Epoch: [13][  130/  193]    Overall Loss 0.002324    Objective Loss 0.002324                                        LR 0.001000    Time 1.239904    
2024-06-06 15:49:39,642 - Epoch: [13][  140/  193]    Overall Loss 0.002204    Objective Loss 0.002204                                        LR 0.001000    Time 1.227030    
2024-06-06 15:49:50,819 - Epoch: [13][  150/  193]    Overall Loss 0.002120    Objective Loss 0.002120                                        LR 0.001000    Time 1.219473    
2024-06-06 15:50:01,185 - Epoch: [13][  160/  193]    Overall Loss 0.002027    Objective Loss 0.002027                                        LR 0.001000    Time 1.207797    
2024-06-06 15:50:11,675 - Epoch: [13][  170/  193]    Overall Loss 0.001924    Objective Loss 0.001924                                        LR 0.001000    Time 1.198123    
2024-06-06 15:50:22,681 - Epoch: [13][  180/  193]    Overall Loss 0.001846    Objective Loss 0.001846                                        LR 0.001000    Time 1.192478    
2024-06-06 15:50:33,110 - Epoch: [13][  190/  193]    Overall Loss 0.001796    Objective Loss 0.001796                                        LR 0.001000    Time 1.184161    
2024-06-06 15:50:35,479 - Epoch: [13][  193/  193]    Overall Loss 0.001772    Objective Loss 0.001772    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 1.177883    
2024-06-06 15:50:36,244 - --- validate (epoch=13)-----------
2024-06-06 15:50:36,244 - 2172 samples (64 per mini-batch)
2024-06-06 15:51:05,128 - Epoch: [13][   10/   34]    Loss 0.001123    Top1 100.000000    Top5 100.000000    
2024-06-06 15:51:10,327 - Epoch: [13][   20/   34]    Loss 0.000822    Top1 100.000000    Top5 100.000000    
2024-06-06 15:51:14,989 - Epoch: [13][   30/   34]    Loss 0.000630    Top1 100.000000    Top5 100.000000    
2024-06-06 15:51:16,601 - Epoch: [13][   34/   34]    Loss 0.000638    Top1 100.000000    Top5 100.000000    
2024-06-06 15:51:17,768 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.001

2024-06-06 15:51:17,768 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 15:51:18,040 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 13]
2024-06-06 15:51:18,040 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 15:51:18,060 - 

2024-06-06 15:51:18,060 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 15:51:57,461 - Epoch: [14][   10/  193]    Overall Loss 0.001238    Objective Loss 0.001238                                        LR 0.001000    Time 3.940085    
2024-06-06 15:52:09,042 - Epoch: [14][   20/  193]    Overall Loss 0.001226    Objective Loss 0.001226                                        LR 0.001000    Time 2.546625    
2024-06-06 15:52:20,285 - Epoch: [14][   30/  193]    Overall Loss 0.001200    Objective Loss 0.001200                                        LR 0.001000    Time 2.070729    
2024-06-06 15:52:33,361 - Epoch: [14][   40/  193]    Overall Loss 0.001223    Objective Loss 0.001223                                        LR 0.001000    Time 1.878376    
2024-06-06 15:52:47,414 - Epoch: [14][   50/  193]    Overall Loss 0.001115    Objective Loss 0.001115                                        LR 0.001000    Time 1.782587    
2024-06-06 15:52:58,902 - Epoch: [14][   60/  193]    Overall Loss 0.001025    Objective Loss 0.001025                                        LR 0.001000    Time 1.675676    
2024-06-06 15:53:09,920 - Epoch: [14][   70/  193]    Overall Loss 0.000961    Objective Loss 0.000961                                        LR 0.001000    Time 1.593019    
2024-06-06 15:53:20,414 - Epoch: [14][   80/  193]    Overall Loss 0.000892    Objective Loss 0.000892                                        LR 0.001000    Time 1.524315    
2024-06-06 15:53:31,135 - Epoch: [14][   90/  193]    Overall Loss 0.000828    Objective Loss 0.000828                                        LR 0.001000    Time 1.473539    
2024-06-06 15:53:41,691 - Epoch: [14][  100/  193]    Overall Loss 0.000774    Objective Loss 0.000774                                        LR 0.001000    Time 1.431231    
2024-06-06 15:53:52,933 - Epoch: [14][  110/  193]    Overall Loss 0.000749    Objective Loss 0.000749                                        LR 0.001000    Time 1.402970    
2024-06-06 15:54:03,231 - Epoch: [14][  120/  193]    Overall Loss 0.000719    Objective Loss 0.000719                                        LR 0.001000    Time 1.371511    
2024-06-06 15:54:14,330 - Epoch: [14][  130/  193]    Overall Loss 0.000684    Objective Loss 0.000684                                        LR 0.001000    Time 1.350867    
2024-06-06 15:54:25,303 - Epoch: [14][  140/  193]    Overall Loss 0.000663    Objective Loss 0.000663                                        LR 0.001000    Time 1.332433    
2024-06-06 15:54:35,566 - Epoch: [14][  150/  193]    Overall Loss 0.000644    Objective Loss 0.000644                                        LR 0.001000    Time 1.311795    
2024-06-06 15:54:45,850 - Epoch: [14][  160/  193]    Overall Loss 0.000729    Objective Loss 0.000729                                        LR 0.001000    Time 1.293849    
2024-06-06 15:54:56,771 - Epoch: [14][  170/  193]    Overall Loss 0.000745    Objective Loss 0.000745                                        LR 0.001000    Time 1.281679    
2024-06-06 15:55:09,009 - Epoch: [14][  180/  193]    Overall Loss 0.000748    Objective Loss 0.000748                                        LR 0.001000    Time 1.277861    
2024-06-06 15:55:19,444 - Epoch: [14][  190/  193]    Overall Loss 0.000770    Objective Loss 0.000770                                        LR 0.001000    Time 1.265250    
2024-06-06 15:55:21,918 - Epoch: [14][  193/  193]    Overall Loss 0.000765    Objective Loss 0.000765    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 1.258081    
2024-06-06 15:55:22,665 - --- validate (epoch=14)-----------
2024-06-06 15:55:22,665 - 2172 samples (64 per mini-batch)
2024-06-06 15:56:02,789 - Epoch: [14][   10/   34]    Loss 0.000240    Top1 100.000000    Top5 100.000000    
2024-06-06 15:56:07,847 - Epoch: [14][   20/   34]    Loss 0.000666    Top1 100.000000    Top5 100.000000    
2024-06-06 15:56:12,766 - Epoch: [14][   30/   34]    Loss 0.000553    Top1 100.000000    Top5 100.000000    
2024-06-06 15:56:14,460 - Epoch: [14][   34/   34]    Loss 0.000522    Top1 100.000000    Top5 100.000000    
2024-06-06 15:56:15,621 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.001

2024-06-06 15:56:15,622 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 15:56:15,921 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 14]
2024-06-06 15:56:15,921 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 15:56:15,940 - 

2024-06-06 15:56:15,940 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 15:56:55,915 - Epoch: [15][   10/  193]    Overall Loss 0.004425    Objective Loss 0.004425                                        LR 0.001000    Time 3.997321    
2024-06-06 15:57:07,686 - Epoch: [15][   20/  193]    Overall Loss 0.003832    Objective Loss 0.003832                                        LR 0.001000    Time 2.584589    
2024-06-06 15:57:19,129 - Epoch: [15][   30/  193]    Overall Loss 0.003369    Objective Loss 0.003369                                        LR 0.001000    Time 2.101399    
2024-06-06 15:57:30,713 - Epoch: [15][   40/  193]    Overall Loss 0.002745    Objective Loss 0.002745                                        LR 0.001000    Time 1.864612    
2024-06-06 15:57:41,577 - Epoch: [15][   50/  193]    Overall Loss 0.002385    Objective Loss 0.002385                                        LR 0.001000    Time 1.707642    
2024-06-06 15:57:52,312 - Epoch: [15][   60/  193]    Overall Loss 0.002083    Objective Loss 0.002083                                        LR 0.001000    Time 1.601106    
2024-06-06 15:58:02,875 - Epoch: [15][   70/  193]    Overall Loss 0.001892    Objective Loss 0.001892                                        LR 0.001000    Time 1.522703    
2024-06-06 15:58:13,247 - Epoch: [15][   80/  193]    Overall Loss 0.001699    Objective Loss 0.001699                                        LR 0.001000    Time 1.461445    
2024-06-06 15:58:24,454 - Epoch: [15][   90/  193]    Overall Loss 0.001568    Objective Loss 0.001568                                        LR 0.001000    Time 1.422978    
2024-06-06 15:58:35,339 - Epoch: [15][  100/  193]    Overall Loss 0.001446    Objective Loss 0.001446                                        LR 0.001000    Time 1.387879    
2024-06-06 15:58:46,069 - Epoch: [15][  110/  193]    Overall Loss 0.001624    Objective Loss 0.001624                                        LR 0.001000    Time 1.358900    
2024-06-06 15:58:56,842 - Epoch: [15][  120/  193]    Overall Loss 0.001960    Objective Loss 0.001960                                        LR 0.001000    Time 1.334999    
2024-06-06 15:59:07,480 - Epoch: [15][  130/  193]    Overall Loss 0.001936    Objective Loss 0.001936                                        LR 0.001000    Time 1.313848    
2024-06-06 15:59:18,130 - Epoch: [15][  140/  193]    Overall Loss 0.001999    Objective Loss 0.001999                                        LR 0.001000    Time 1.295728    
2024-06-06 15:59:28,474 - Epoch: [15][  150/  193]    Overall Loss 0.002300    Objective Loss 0.002300                                        LR 0.001000    Time 1.277753    
2024-06-06 15:59:39,060 - Epoch: [15][  160/  193]    Overall Loss 0.002271    Objective Loss 0.002271                                        LR 0.001000    Time 1.263824    
2024-06-06 15:59:49,837 - Epoch: [15][  170/  193]    Overall Loss 0.002178    Objective Loss 0.002178                                        LR 0.001000    Time 1.252652    
2024-06-06 16:00:00,496 - Epoch: [15][  180/  193]    Overall Loss 0.002113    Objective Loss 0.002113                                        LR 0.001000    Time 1.242031    
2024-06-06 16:00:10,840 - Epoch: [15][  190/  193]    Overall Loss 0.002031    Objective Loss 0.002031                                        LR 0.001000    Time 1.230814    
2024-06-06 16:00:13,258 - Epoch: [15][  193/  193]    Overall Loss 0.002007    Objective Loss 0.002007    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 1.223912    
2024-06-06 16:00:14,066 - --- validate (epoch=15)-----------
2024-06-06 16:00:14,067 - 2172 samples (64 per mini-batch)
2024-06-06 16:00:56,651 - Epoch: [15][   10/   34]    Loss 0.001006    Top1 100.000000    Top5 100.000000    
2024-06-06 16:01:02,833 - Epoch: [15][   20/   34]    Loss 0.001139    Top1 100.000000    Top5 100.000000    
2024-06-06 16:01:09,388 - Epoch: [15][   30/   34]    Loss 0.000840    Top1 100.000000    Top5 100.000000    
2024-06-06 16:01:11,829 - Epoch: [15][   34/   34]    Loss 0.000797    Top1 100.000000    Top5 100.000000    
2024-06-06 16:01:13,511 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.001

2024-06-06 16:01:13,511 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 16:01:14,044 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 15]
2024-06-06 16:01:14,044 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 16:01:14,089 - 

2024-06-06 16:01:14,090 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 16:01:53,591 - Epoch: [16][   10/  193]    Overall Loss 0.000371    Objective Loss 0.000371                                        LR 0.001000    Time 3.949926    
2024-06-06 16:02:08,134 - Epoch: [16][   20/  193]    Overall Loss 0.000511    Objective Loss 0.000511                                        LR 0.001000    Time 2.697842    
2024-06-06 16:02:23,715 - Epoch: [16][   30/  193]    Overall Loss 0.000490    Objective Loss 0.000490                                        LR 0.001000    Time 2.314648    
2024-06-06 16:02:38,154 - Epoch: [16][   40/  193]    Overall Loss 0.000584    Objective Loss 0.000584                                        LR 0.001000    Time 2.095104    
2024-06-06 16:02:53,674 - Epoch: [16][   50/  193]    Overall Loss 0.000527    Objective Loss 0.000527                                        LR 0.001000    Time 1.985280    
2024-06-06 16:03:09,374 - Epoch: [16][   60/  193]    Overall Loss 0.001260    Objective Loss 0.001260                                        LR 0.001000    Time 1.914764    
2024-06-06 16:03:24,702 - Epoch: [16][   70/  193]    Overall Loss 0.001495    Objective Loss 0.001495                                        LR 0.001000    Time 1.858965    
2024-06-06 16:03:40,240 - Epoch: [16][   80/  193]    Overall Loss 0.001918    Objective Loss 0.001918                                        LR 0.001000    Time 1.819827    
2024-06-06 16:03:56,043 - Epoch: [16][   90/  193]    Overall Loss 0.002259    Objective Loss 0.002259                                        LR 0.001000    Time 1.792260    
2024-06-06 16:04:11,373 - Epoch: [16][  100/  193]    Overall Loss 0.002127    Objective Loss 0.002127                                        LR 0.001000    Time 1.765669    
2024-06-06 16:04:26,833 - Epoch: [16][  110/  193]    Overall Loss 0.002084    Objective Loss 0.002084                                        LR 0.001000    Time 1.744847    
2024-06-06 16:04:42,199 - Epoch: [16][  120/  193]    Overall Loss 0.001959    Objective Loss 0.001959                                        LR 0.001000    Time 1.726753    
2024-06-06 16:04:56,543 - Epoch: [16][  130/  193]    Overall Loss 0.001854    Objective Loss 0.001854                                        LR 0.001000    Time 1.703701    
2024-06-06 16:05:12,089 - Epoch: [16][  140/  193]    Overall Loss 0.001760    Objective Loss 0.001760                                        LR 0.001000    Time 1.692417    
2024-06-06 16:05:27,519 - Epoch: [16][  150/  193]    Overall Loss 0.001664    Objective Loss 0.001664                                        LR 0.001000    Time 1.681846    
2024-06-06 16:05:41,970 - Epoch: [16][  160/  193]    Overall Loss 0.001589    Objective Loss 0.001589                                        LR 0.001000    Time 1.666585    
2024-06-06 16:05:57,033 - Epoch: [16][  170/  193]    Overall Loss 0.001517    Objective Loss 0.001517                                        LR 0.001000    Time 1.656662    
2024-06-06 16:06:11,146 - Epoch: [16][  180/  193]    Overall Loss 0.001464    Objective Loss 0.001464                                        LR 0.001000    Time 1.642600    
2024-06-06 16:06:26,354 - Epoch: [16][  190/  193]    Overall Loss 0.001423    Objective Loss 0.001423                                        LR 0.001000    Time 1.635777    
2024-06-06 16:06:30,187 - Epoch: [16][  193/  193]    Overall Loss 0.001404    Objective Loss 0.001404    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 1.629802    
2024-06-06 16:06:31,224 - --- validate (epoch=16)-----------
2024-06-06 16:06:31,225 - 2172 samples (64 per mini-batch)
2024-06-06 16:07:03,365 - Epoch: [16][   10/   34]    Loss 0.000546    Top1 100.000000    Top5 100.000000    
2024-06-06 16:07:08,859 - Epoch: [16][   20/   34]    Loss 0.000516    Top1 100.000000    Top5 100.000000    
2024-06-06 16:07:13,989 - Epoch: [16][   30/   34]    Loss 0.000451    Top1 100.000000    Top5 100.000000    
2024-06-06 16:07:15,889 - Epoch: [16][   34/   34]    Loss 0.000422    Top1 100.000000    Top5 100.000000    
2024-06-06 16:07:17,789 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 16:07:17,790 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 16:07:18,361 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 16]
2024-06-06 16:07:18,362 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 16:07:18,405 - 

2024-06-06 16:07:18,406 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 16:08:00,090 - Epoch: [17][   10/  193]    Overall Loss 0.000900    Objective Loss 0.000900                                        LR 0.001000    Time 4.168212    
2024-06-06 16:08:13,849 - Epoch: [17][   20/  193]    Overall Loss 0.000636    Objective Loss 0.000636                                        LR 0.001000    Time 2.768928    
2024-06-06 16:08:30,651 - Epoch: [17][   30/  193]    Overall Loss 0.002729    Objective Loss 0.002729                                        LR 0.001000    Time 2.403138    
2024-06-06 16:08:42,153 - Epoch: [17][   40/  193]    Overall Loss 0.005051    Objective Loss 0.005051                                        LR 0.001000    Time 2.088641    
2024-06-06 16:08:51,209 - Epoch: [17][   50/  193]    Overall Loss 0.005367    Objective Loss 0.005367                                        LR 0.001000    Time 1.851453    
2024-06-06 16:09:03,789 - Epoch: [17][   60/  193]    Overall Loss 0.005225    Objective Loss 0.005225                                        LR 0.001000    Time 1.751970    
2024-06-06 16:09:19,196 - Epoch: [17][   70/  193]    Overall Loss 0.004888    Objective Loss 0.004888                                        LR 0.001000    Time 1.720758    
2024-06-06 16:09:33,572 - Epoch: [17][   80/  193]    Overall Loss 0.004661    Objective Loss 0.004661                                        LR 0.001000    Time 1.684299    
2024-06-06 16:09:49,263 - Epoch: [17][   90/  193]    Overall Loss 0.004306    Objective Loss 0.004306                                        LR 0.001000    Time 1.670729    
2024-06-06 16:10:04,666 - Epoch: [17][  100/  193]    Overall Loss 0.003940    Objective Loss 0.003940                                        LR 0.001000    Time 1.656999    
2024-06-06 16:10:20,050 - Epoch: [17][  110/  193]    Overall Loss 0.003829    Objective Loss 0.003829                                        LR 0.001000    Time 1.645356    
2024-06-06 16:10:35,623 - Epoch: [17][  120/  193]    Overall Loss 0.003662    Objective Loss 0.003662                                        LR 0.001000    Time 1.637371    
2024-06-06 16:10:51,043 - Epoch: [17][  130/  193]    Overall Loss 0.003447    Objective Loss 0.003447                                        LR 0.001000    Time 1.629449    
2024-06-06 16:11:05,984 - Epoch: [17][  140/  193]    Overall Loss 0.003269    Objective Loss 0.003269                                        LR 0.001000    Time 1.619119    
2024-06-06 16:11:19,978 - Epoch: [17][  150/  193]    Overall Loss 0.003192    Objective Loss 0.003192                                        LR 0.001000    Time 1.604057    
2024-06-06 16:11:35,170 - Epoch: [17][  160/  193]    Overall Loss 0.003088    Objective Loss 0.003088                                        LR 0.001000    Time 1.598332    
2024-06-06 16:11:50,349 - Epoch: [17][  170/  193]    Overall Loss 0.002925    Objective Loss 0.002925                                        LR 0.001000    Time 1.593163    
2024-06-06 16:12:04,245 - Epoch: [17][  180/  193]    Overall Loss 0.002797    Objective Loss 0.002797                                        LR 0.001000    Time 1.581465    
2024-06-06 16:12:19,753 - Epoch: [17][  190/  193]    Overall Loss 0.002671    Objective Loss 0.002671                                        LR 0.001000    Time 1.579573    
2024-06-06 16:12:23,573 - Epoch: [17][  193/  193]    Overall Loss 0.002635    Objective Loss 0.002635    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 1.574479    
2024-06-06 16:12:24,622 - --- validate (epoch=17)-----------
2024-06-06 16:12:24,623 - 2172 samples (64 per mini-batch)
2024-06-06 16:13:22,645 - Epoch: [17][   10/   34]    Loss 0.000269    Top1 100.000000    Top5 100.000000    
2024-06-06 16:13:28,845 - Epoch: [17][   20/   34]    Loss 0.000540    Top1 100.000000    Top5 100.000000    
2024-06-06 16:13:35,131 - Epoch: [17][   30/   34]    Loss 0.000558    Top1 100.000000    Top5 100.000000    
2024-06-06 16:13:37,797 - Epoch: [17][   34/   34]    Loss 0.000528    Top1 100.000000    Top5 100.000000    
2024-06-06 16:13:39,465 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.001

2024-06-06 16:13:39,466 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 16:13:39,947 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 17]
2024-06-06 16:13:39,949 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 16:13:39,989 - 

2024-06-06 16:13:39,990 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 16:14:18,670 - Epoch: [18][   10/  193]    Overall Loss 0.000286    Objective Loss 0.000286                                        LR 0.001000    Time 3.867818    
2024-06-06 16:14:33,752 - Epoch: [18][   20/  193]    Overall Loss 0.000451    Objective Loss 0.000451                                        LR 0.001000    Time 2.684929    
2024-06-06 16:14:48,902 - Epoch: [18][   30/  193]    Overall Loss 0.000398    Objective Loss 0.000398                                        LR 0.001000    Time 2.292397    
2024-06-06 16:15:04,001 - Epoch: [18][   40/  193]    Overall Loss 0.000335    Objective Loss 0.000335                                        LR 0.001000    Time 2.094290    
2024-06-06 16:15:19,008 - Epoch: [18][   50/  193]    Overall Loss 0.000320    Objective Loss 0.000320                                        LR 0.001000    Time 1.974363    
2024-06-06 16:15:33,997 - Epoch: [18][   60/  193]    Overall Loss 0.000317    Objective Loss 0.000317                                        LR 0.001000    Time 1.893767    
2024-06-06 16:15:49,166 - Epoch: [18][   70/  193]    Overall Loss 0.000391    Objective Loss 0.000391                                        LR 0.001000    Time 1.838877    
2024-06-06 16:16:02,294 - Epoch: [18][   80/  193]    Overall Loss 0.000376    Objective Loss 0.000376                                        LR 0.001000    Time 1.772316    
2024-06-06 16:16:17,557 - Epoch: [18][   90/  193]    Overall Loss 0.000366    Objective Loss 0.000366                                        LR 0.001000    Time 1.744188    
2024-06-06 16:16:31,632 - Epoch: [18][  100/  193]    Overall Loss 0.000349    Objective Loss 0.000349                                        LR 0.001000    Time 1.709714    
2024-06-06 16:16:46,711 - Epoch: [18][  110/  193]    Overall Loss 0.000349    Objective Loss 0.000349                                        LR 0.001000    Time 1.690663    
2024-06-06 16:17:01,895 - Epoch: [18][  120/  193]    Overall Loss 0.000333    Objective Loss 0.000333                                        LR 0.001000    Time 1.675744    
2024-06-06 16:17:17,260 - Epoch: [18][  130/  193]    Overall Loss 0.000321    Objective Loss 0.000321                                        LR 0.001000    Time 1.664431    
2024-06-06 16:17:32,035 - Epoch: [18][  140/  193]    Overall Loss 0.000308    Objective Loss 0.000308                                        LR 0.001000    Time 1.650569    
2024-06-06 16:17:47,059 - Epoch: [18][  150/  193]    Overall Loss 0.000298    Objective Loss 0.000298                                        LR 0.001000    Time 1.640291    
2024-06-06 16:18:02,490 - Epoch: [18][  160/  193]    Overall Loss 0.000313    Objective Loss 0.000313                                        LR 0.001000    Time 1.633725    
2024-06-06 16:18:17,831 - Epoch: [18][  170/  193]    Overall Loss 0.000311    Objective Loss 0.000311                                        LR 0.001000    Time 1.627406    
2024-06-06 16:18:33,252 - Epoch: [18][  180/  193]    Overall Loss 0.000319    Objective Loss 0.000319                                        LR 0.001000    Time 1.622228    
2024-06-06 16:18:48,962 - Epoch: [18][  190/  193]    Overall Loss 0.000309    Objective Loss 0.000309                                        LR 0.001000    Time 1.619062    
2024-06-06 16:18:52,788 - Epoch: [18][  193/  193]    Overall Loss 0.000309    Objective Loss 0.000309    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 1.613333    
2024-06-06 16:18:53,812 - --- validate (epoch=18)-----------
2024-06-06 16:18:53,812 - 2172 samples (64 per mini-batch)
2024-06-06 16:19:25,600 - Epoch: [18][   10/   34]    Loss 0.000450    Top1 100.000000    Top5 100.000000    
2024-06-06 16:19:31,887 - Epoch: [18][   20/   34]    Loss 0.000448    Top1 100.000000    Top5 100.000000    
2024-06-06 16:19:38,035 - Epoch: [18][   30/   34]    Loss 0.000390    Top1 100.000000    Top5 100.000000    
2024-06-06 16:19:40,634 - Epoch: [18][   34/   34]    Loss 0.000372    Top1 100.000000    Top5 100.000000    
2024-06-06 16:19:42,307 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 16:19:42,307 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 16:19:42,810 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 18]
2024-06-06 16:19:42,811 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 16:19:42,852 - 

2024-06-06 16:19:42,852 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 16:20:22,751 - Epoch: [19][   10/  193]    Overall Loss 0.000101    Objective Loss 0.000101                                        LR 0.001000    Time 3.989504    
2024-06-06 16:20:37,059 - Epoch: [19][   20/  193]    Overall Loss 0.000164    Objective Loss 0.000164                                        LR 0.001000    Time 2.705878    
2024-06-06 16:20:52,307 - Epoch: [19][   30/  193]    Overall Loss 0.000137    Objective Loss 0.000137                                        LR 0.001000    Time 2.310397    
2024-06-06 16:21:07,434 - Epoch: [19][   40/  193]    Overall Loss 0.000135    Objective Loss 0.000135                                        LR 0.001000    Time 2.109233    
2024-06-06 16:21:22,717 - Epoch: [19][   50/  193]    Overall Loss 0.000127    Objective Loss 0.000127                                        LR 0.001000    Time 1.991586    
2024-06-06 16:21:38,002 - Epoch: [19][   60/  193]    Overall Loss 0.000131    Objective Loss 0.000131                                        LR 0.001000    Time 1.913005    
2024-06-06 16:21:53,325 - Epoch: [19][   70/  193]    Overall Loss 0.000132    Objective Loss 0.000132                                        LR 0.001000    Time 1.857476    
2024-06-06 16:22:05,957 - Epoch: [19][   80/  193]    Overall Loss 0.000127    Objective Loss 0.000127                                        LR 0.001000    Time 1.782350    
2024-06-06 16:22:21,351 - Epoch: [19][   90/  193]    Overall Loss 0.000152    Objective Loss 0.000152                                        LR 0.001000    Time 1.754209    
2024-06-06 16:22:35,843 - Epoch: [19][  100/  193]    Overall Loss 0.000146    Objective Loss 0.000146                                        LR 0.001000    Time 1.722966    
2024-06-06 16:22:50,806 - Epoch: [19][  110/  193]    Overall Loss 0.000148    Objective Loss 0.000148                                        LR 0.001000    Time 1.701589    
2024-06-06 16:23:05,973 - Epoch: [19][  120/  193]    Overall Loss 0.000147    Objective Loss 0.000147                                        LR 0.001000    Time 1.685528    
2024-06-06 16:23:21,265 - Epoch: [19][  130/  193]    Overall Loss 0.000154    Objective Loss 0.000154                                        LR 0.001000    Time 1.672883    
2024-06-06 16:23:36,620 - Epoch: [19][  140/  193]    Overall Loss 0.000155    Objective Loss 0.000155                                        LR 0.001000    Time 1.662547    
2024-06-06 16:23:51,208 - Epoch: [19][  150/  193]    Overall Loss 0.000158    Objective Loss 0.000158                                        LR 0.001000    Time 1.648402    
2024-06-06 16:24:05,890 - Epoch: [19][  160/  193]    Overall Loss 0.000238    Objective Loss 0.000238                                        LR 0.001000    Time 1.636627    
2024-06-06 16:24:21,037 - Epoch: [19][  170/  193]    Overall Loss 0.000256    Objective Loss 0.000256                                        LR 0.001000    Time 1.628954    
2024-06-06 16:24:36,504 - Epoch: [19][  180/  193]    Overall Loss 0.000276    Objective Loss 0.000276                                        LR 0.001000    Time 1.623868    
2024-06-06 16:24:52,058 - Epoch: [19][  190/  193]    Overall Loss 0.000277    Objective Loss 0.000277                                        LR 0.001000    Time 1.619874    
2024-06-06 16:24:55,853 - Epoch: [19][  193/  193]    Overall Loss 0.000279    Objective Loss 0.000279    Top1 100.000000    Top5 100.000000    LR 0.001000    Time 1.614010    
2024-06-06 16:24:56,848 - --- validate (epoch=19)-----------
2024-06-06 16:24:56,849 - 2172 samples (64 per mini-batch)
2024-06-06 16:25:27,270 - Epoch: [19][   10/   34]    Loss 0.000401    Top1 100.000000    Top5 100.000000    
2024-06-06 16:25:33,479 - Epoch: [19][   20/   34]    Loss 0.000368    Top1 100.000000    Top5 100.000000    
2024-06-06 16:25:39,833 - Epoch: [19][   30/   34]    Loss 0.000302    Top1 100.000000    Top5 100.000000    
2024-06-06 16:25:42,439 - Epoch: [19][   34/   34]    Loss 0.000291    Top1 100.000000    Top5 100.000000    
2024-06-06 16:25:44,100 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 16:25:44,101 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 16:25:44,594 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 19]
2024-06-06 16:25:44,594 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 16:25:44,634 - 

2024-06-06 16:25:44,635 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 16:26:51,153 - Epoch: [20][   10/  193]    Overall Loss 0.000355    Objective Loss 0.000355                                        LR 0.000500    Time 6.651631    
2024-06-06 16:27:05,521 - Epoch: [20][   20/  193]    Overall Loss 0.000251    Objective Loss 0.000251                                        LR 0.000500    Time 4.040448    
2024-06-06 16:27:20,234 - Epoch: [20][   30/  193]    Overall Loss 0.000221    Objective Loss 0.000221                                        LR 0.000500    Time 3.181237    
2024-06-06 16:45:54,762 - Epoch: [20][   40/  193]    Overall Loss 0.000213    Objective Loss 0.000213                                        LR 0.000500    Time 30.247236    
2024-06-06 16:46:13,690 - Epoch: [20][   50/  193]    Overall Loss 0.000205    Objective Loss 0.000205                                        LR 0.000500    Time 24.575137    
2024-06-06 16:46:22,205 - Epoch: [20][   60/  193]    Overall Loss 0.000205    Objective Loss 0.000205                                        LR 0.000500    Time 20.620523    
2024-06-06 16:46:30,672 - Epoch: [20][   70/  193]    Overall Loss 0.000330    Objective Loss 0.000330                                        LR 0.000500    Time 17.795268    
2024-06-06 16:46:39,262 - Epoch: [20][   80/  193]    Overall Loss 0.000306    Objective Loss 0.000306                                        LR 0.000500    Time 15.677636    
2024-06-06 16:46:47,283 - Epoch: [20][   90/  193]    Overall Loss 0.000299    Objective Loss 0.000299                                        LR 0.000500    Time 14.024447    
2024-06-06 16:46:55,415 - Epoch: [20][  100/  193]    Overall Loss 0.000281    Objective Loss 0.000281                                        LR 0.000500    Time 12.703074    
2024-06-06 16:47:03,692 - Epoch: [20][  110/  193]    Overall Loss 0.000266    Objective Loss 0.000266                                        LR 0.000500    Time 11.623020    
2024-06-06 16:47:11,944 - Epoch: [20][  120/  193]    Overall Loss 0.000253    Objective Loss 0.000253                                        LR 0.000500    Time 10.722980    
2024-06-06 16:47:19,945 - Epoch: [20][  130/  193]    Overall Loss 0.000244    Objective Loss 0.000244                                        LR 0.000500    Time 9.959395    
2024-06-06 16:47:27,822 - Epoch: [20][  140/  193]    Overall Loss 0.000233    Objective Loss 0.000233                                        LR 0.000500    Time 9.304013    
2024-06-06 16:47:36,009 - Epoch: [20][  150/  193]    Overall Loss 0.000222    Objective Loss 0.000222                                        LR 0.000500    Time 8.738148    
2024-06-06 16:47:43,966 - Epoch: [20][  160/  193]    Overall Loss 0.000227    Objective Loss 0.000227                                        LR 0.000500    Time 8.241608    
2024-06-06 16:47:51,941 - Epoch: [20][  170/  193]    Overall Loss 0.000221    Objective Loss 0.000221                                        LR 0.000500    Time 7.803575    
2024-06-06 16:47:59,943 - Epoch: [20][  180/  193]    Overall Loss 0.000220    Objective Loss 0.000220                                        LR 0.000500    Time 7.414331    
2024-06-06 16:48:07,948 - Epoch: [20][  190/  193]    Overall Loss 0.000219    Objective Loss 0.000219                                        LR 0.000500    Time 7.066110    
2024-06-06 16:48:09,799 - Epoch: [20][  193/  193]    Overall Loss 0.000216    Objective Loss 0.000216    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 6.965718    
2024-06-06 16:48:10,463 - --- validate (epoch=20)-----------
2024-06-06 16:48:10,463 - 2172 samples (64 per mini-batch)
2024-06-06 16:48:33,706 - Epoch: [20][   10/   34]    Loss 0.000307    Top1 100.000000    Top5 100.000000    
2024-06-06 16:48:37,042 - Epoch: [20][   20/   34]    Loss 0.000251    Top1 100.000000    Top5 100.000000    
2024-06-06 16:48:40,809 - Epoch: [20][   30/   34]    Loss 0.000204    Top1 100.000000    Top5 100.000000    
2024-06-06 16:48:42,198 - Epoch: [20][   34/   34]    Loss 0.000199    Top1 100.000000    Top5 100.000000    
2024-06-06 16:48:43,166 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 16:48:43,167 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 16:48:43,374 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 20]
2024-06-06 16:48:43,375 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 16:48:43,393 - 

2024-06-06 16:48:43,394 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 16:49:12,966 - Epoch: [21][   10/  193]    Overall Loss 0.000076    Objective Loss 0.000076                                        LR 0.000500    Time 2.957037    
2024-06-06 16:49:21,556 - Epoch: [21][   20/  193]    Overall Loss 0.000102    Objective Loss 0.000102                                        LR 0.000500    Time 1.906593    
2024-06-06 16:49:30,063 - Epoch: [21][   30/  193]    Overall Loss 0.000092    Objective Loss 0.000092                                        LR 0.000500    Time 1.553736    
2024-06-06 16:49:38,897 - Epoch: [21][   40/  193]    Overall Loss 0.000104    Objective Loss 0.000104                                        LR 0.000500    Time 1.385423    
2024-06-06 16:49:47,783 - Epoch: [21][   50/  193]    Overall Loss 0.000092    Objective Loss 0.000092                                        LR 0.000500    Time 1.285298    
2024-06-06 16:49:56,692 - Epoch: [21][   60/  193]    Overall Loss 0.000089    Objective Loss 0.000089                                        LR 0.000500    Time 1.218618    
2024-06-06 16:50:05,568 - Epoch: [21][   70/  193]    Overall Loss 0.000123    Objective Loss 0.000123                                        LR 0.000500    Time 1.170934    
2024-06-06 16:50:14,185 - Epoch: [21][   80/  193]    Overall Loss 0.000116    Objective Loss 0.000116                                        LR 0.000500    Time 1.131930    
2024-06-06 16:50:23,005 - Epoch: [21][   90/  193]    Overall Loss 0.000121    Objective Loss 0.000121                                        LR 0.000500    Time 1.103904    
2024-06-06 16:50:31,795 - Epoch: [21][  100/  193]    Overall Loss 0.000122    Objective Loss 0.000122                                        LR 0.000500    Time 1.081054    
2024-06-06 16:50:40,888 - Epoch: [21][  110/  193]    Overall Loss 0.000118    Objective Loss 0.000118                                        LR 0.000500    Time 1.065118    
2024-06-06 16:50:49,820 - Epoch: [21][  120/  193]    Overall Loss 0.000113    Objective Loss 0.000113                                        LR 0.000500    Time 1.050535    
2024-06-06 16:51:00,010 - Epoch: [21][  130/  193]    Overall Loss 0.000113    Objective Loss 0.000113                                        LR 0.000500    Time 1.047831    
2024-06-06 16:51:09,891 - Epoch: [21][  140/  193]    Overall Loss 0.000109    Objective Loss 0.000109                                        LR 0.000500    Time 1.043148    
2024-06-06 16:51:19,995 - Epoch: [21][  150/  193]    Overall Loss 0.000118    Objective Loss 0.000118                                        LR 0.000500    Time 1.040682    
2024-06-06 16:51:29,823 - Epoch: [21][  160/  193]    Overall Loss 0.000117    Objective Loss 0.000117                                        LR 0.000500    Time 1.036740    
2024-06-06 16:51:40,019 - Epoch: [21][  170/  193]    Overall Loss 0.000113    Objective Loss 0.000113                                        LR 0.000500    Time 1.035418    
2024-06-06 16:51:49,782 - Epoch: [21][  180/  193]    Overall Loss 0.000122    Objective Loss 0.000122                                        LR 0.000500    Time 1.031833    
2024-06-06 16:51:59,232 - Epoch: [21][  190/  193]    Overall Loss 0.000119    Objective Loss 0.000119                                        LR 0.000500    Time 1.027075    
2024-06-06 16:52:01,496 - Epoch: [21][  193/  193]    Overall Loss 0.000119    Objective Loss 0.000119    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 1.022528    
2024-06-06 16:52:02,209 - --- validate (epoch=21)-----------
2024-06-06 16:52:02,209 - 2172 samples (64 per mini-batch)
2024-06-06 16:52:37,302 - Epoch: [21][   10/   34]    Loss 0.000190    Top1 100.000000    Top5 100.000000    
2024-06-06 16:52:41,658 - Epoch: [21][   20/   34]    Loss 0.000145    Top1 100.000000    Top5 100.000000    
2024-06-06 16:52:45,789 - Epoch: [21][   30/   34]    Loss 0.000223    Top1 100.000000    Top5 100.000000    
2024-06-06 16:52:47,237 - Epoch: [21][   34/   34]    Loss 0.000204    Top1 100.000000    Top5 100.000000    
2024-06-06 16:52:48,270 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 16:52:48,270 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 16:52:48,524 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 21]
2024-06-06 16:52:48,525 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 16:52:48,543 - 

2024-06-06 16:52:48,544 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 16:53:20,513 - Epoch: [22][   10/  193]    Overall Loss 0.000210    Objective Loss 0.000210                                        LR 0.000500    Time 3.196782    
2024-06-06 16:53:30,162 - Epoch: [22][   20/  193]    Overall Loss 0.000213    Objective Loss 0.000213                                        LR 0.000500    Time 2.078740    
2024-06-06 16:53:39,627 - Epoch: [22][   30/  193]    Overall Loss 0.000161    Objective Loss 0.000161                                        LR 0.000500    Time 1.700130    
2024-06-06 16:53:49,297 - Epoch: [22][   40/  193]    Overall Loss 0.000135    Objective Loss 0.000135                                        LR 0.000500    Time 1.515654    
2024-06-06 16:53:58,867 - Epoch: [22][   50/  193]    Overall Loss 0.000122    Objective Loss 0.000122                                        LR 0.000500    Time 1.403210    
2024-06-06 16:54:08,964 - Epoch: [22][   60/  193]    Overall Loss 0.000118    Objective Loss 0.000118                                        LR 0.000500    Time 1.337117    
2024-06-06 16:54:18,616 - Epoch: [22][   70/  193]    Overall Loss 0.000152    Objective Loss 0.000152                                        LR 0.000500    Time 1.283579    
2024-06-06 16:54:28,218 - Epoch: [22][   80/  193]    Overall Loss 0.000154    Objective Loss 0.000154                                        LR 0.000500    Time 1.242646    
2024-06-06 16:54:37,723 - Epoch: [22][   90/  193]    Overall Loss 0.000154    Objective Loss 0.000154                                        LR 0.000500    Time 1.209729    
2024-06-06 16:54:47,199 - Epoch: [22][  100/  193]    Overall Loss 0.000237    Objective Loss 0.000237                                        LR 0.000500    Time 1.183176    
2024-06-06 16:54:56,713 - Epoch: [22][  110/  193]    Overall Loss 0.000228    Objective Loss 0.000228                                        LR 0.000500    Time 1.161763    
2024-06-06 16:55:06,515 - Epoch: [22][  120/  193]    Overall Loss 0.000218    Objective Loss 0.000218                                        LR 0.000500    Time 1.146366    
2024-06-06 16:55:16,094 - Epoch: [22][  130/  193]    Overall Loss 0.000212    Objective Loss 0.000212                                        LR 0.000500    Time 1.131661    
2024-06-06 16:55:25,635 - Epoch: [22][  140/  193]    Overall Loss 0.000205    Objective Loss 0.000205                                        LR 0.000500    Time 1.118658    
2024-06-06 16:55:35,181 - Epoch: [22][  150/  193]    Overall Loss 0.000201    Objective Loss 0.000201                                        LR 0.000500    Time 1.107509    
2024-06-06 16:55:44,773 - Epoch: [22][  160/  193]    Overall Loss 0.000197    Objective Loss 0.000197                                        LR 0.000500    Time 1.098011    
2024-06-06 16:55:54,513 - Epoch: [22][  170/  193]    Overall Loss 0.000189    Objective Loss 0.000189                                        LR 0.000500    Time 1.090477    
2024-06-06 16:56:04,356 - Epoch: [22][  180/  193]    Overall Loss 0.000218    Objective Loss 0.000218                                        LR 0.000500    Time 1.084390    
2024-06-06 16:56:13,761 - Epoch: [22][  190/  193]    Overall Loss 0.000210    Objective Loss 0.000210                                        LR 0.000500    Time 1.076622    
2024-06-06 16:56:16,029 - Epoch: [22][  193/  193]    Overall Loss 0.000209    Objective Loss 0.000209    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 1.071437    
2024-06-06 16:56:16,750 - --- validate (epoch=22)-----------
2024-06-06 16:56:16,750 - 2172 samples (64 per mini-batch)
2024-06-06 16:56:44,210 - Epoch: [22][   10/   34]    Loss 0.000163    Top1 100.000000    Top5 100.000000    
2024-06-06 16:56:48,599 - Epoch: [22][   20/   34]    Loss 0.000320    Top1 100.000000    Top5 100.000000    
2024-06-06 16:56:52,673 - Epoch: [22][   30/   34]    Loss 0.000279    Top1 100.000000    Top5 100.000000    
2024-06-06 16:56:54,129 - Epoch: [22][   34/   34]    Loss 0.000259    Top1 100.000000    Top5 100.000000    
2024-06-06 16:56:55,141 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 16:56:55,142 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 16:56:55,382 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 22]
2024-06-06 16:56:55,382 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 16:56:55,404 - 

2024-06-06 16:56:55,404 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 16:57:30,392 - Epoch: [23][   10/  193]    Overall Loss 0.000163    Objective Loss 0.000163                                        LR 0.000500    Time 3.498548    
2024-06-06 16:57:40,077 - Epoch: [23][   20/  193]    Overall Loss 0.000264    Objective Loss 0.000264                                        LR 0.000500    Time 2.232309    
2024-06-06 16:57:49,627 - Epoch: [23][   30/  193]    Overall Loss 0.000204    Objective Loss 0.000204                                        LR 0.000500    Time 1.805413    
2024-06-06 16:57:59,241 - Epoch: [23][   40/  193]    Overall Loss 0.000193    Objective Loss 0.000193                                        LR 0.000500    Time 1.593700    
2024-06-06 16:58:08,839 - Epoch: [23][   50/  193]    Overall Loss 0.000213    Objective Loss 0.000213                                        LR 0.000500    Time 1.466064    
2024-06-06 16:58:18,351 - Epoch: [23][   60/  193]    Overall Loss 0.000200    Objective Loss 0.000200                                        LR 0.000500    Time 1.379554    
2024-06-06 16:58:28,363 - Epoch: [23][   70/  193]    Overall Loss 0.000190    Objective Loss 0.000190                                        LR 0.000500    Time 1.325075    
2024-06-06 16:58:37,914 - Epoch: [23][   80/  193]    Overall Loss 0.000196    Objective Loss 0.000196                                        LR 0.000500    Time 1.278390    
2024-06-06 16:58:47,937 - Epoch: [23][   90/  193]    Overall Loss 0.000187    Objective Loss 0.000187                                        LR 0.000500    Time 1.247147    
2024-06-06 16:58:57,491 - Epoch: [23][  100/  193]    Overall Loss 0.000175    Objective Loss 0.000175                                        LR 0.000500    Time 1.217507    
2024-06-06 16:59:07,028 - Epoch: [23][  110/  193]    Overall Loss 0.000165    Objective Loss 0.000165                                        LR 0.000500    Time 1.193146    
2024-06-06 16:59:16,841 - Epoch: [23][  120/  193]    Overall Loss 0.000157    Objective Loss 0.000157                                        LR 0.000500    Time 1.175128    
2024-06-06 16:59:26,429 - Epoch: [23][  130/  193]    Overall Loss 0.000152    Objective Loss 0.000152                                        LR 0.000500    Time 1.158114    
2024-06-06 16:59:35,970 - Epoch: [23][  140/  193]    Overall Loss 0.000145    Objective Loss 0.000145                                        LR 0.000500    Time 1.143255    
2024-06-06 16:59:45,495 - Epoch: [23][  150/  193]    Overall Loss 0.000179    Objective Loss 0.000179                                        LR 0.000500    Time 1.130351    
2024-06-06 16:59:55,083 - Epoch: [23][  160/  193]    Overall Loss 0.000184    Objective Loss 0.000184                                        LR 0.000500    Time 1.119404    
2024-06-06 17:00:05,140 - Epoch: [23][  170/  193]    Overall Loss 0.000181    Objective Loss 0.000181                                        LR 0.000500    Time 1.112510    
2024-06-06 17:00:15,057 - Epoch: [23][  180/  193]    Overall Loss 0.000216    Objective Loss 0.000216                                        LR 0.000500    Time 1.105402    
2024-06-06 17:00:24,317 - Epoch: [23][  190/  193]    Overall Loss 0.000213    Objective Loss 0.000213                                        LR 0.000500    Time 1.095771    
2024-06-06 17:00:26,522 - Epoch: [23][  193/  193]    Overall Loss 0.000211    Objective Loss 0.000211    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 1.089995    
2024-06-06 17:00:27,232 - --- validate (epoch=23)-----------
2024-06-06 17:00:27,232 - 2172 samples (64 per mini-batch)
2024-06-06 17:00:54,129 - Epoch: [23][   10/   34]    Loss 0.000732    Top1 100.000000    Top5 100.000000    
2024-06-06 17:00:58,479 - Epoch: [23][   20/   34]    Loss 0.000784    Top1 100.000000    Top5 100.000000    
2024-06-06 17:01:02,723 - Epoch: [23][   30/   34]    Loss 0.000797    Top1 100.000000    Top5 100.000000    
2024-06-06 17:01:04,282 - Epoch: [23][   34/   34]    Loss 0.000743    Top1 100.000000    Top5 100.000000    
2024-06-06 17:01:05,285 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.001

2024-06-06 17:01:05,285 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 17:01:05,530 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 23]
2024-06-06 17:01:05,531 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 17:01:05,553 - 

2024-06-06 17:01:05,554 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 17:01:39,286 - Epoch: [24][   10/  193]    Overall Loss 0.000238    Objective Loss 0.000238                                        LR 0.000500    Time 3.373107    
2024-06-06 17:01:49,624 - Epoch: [24][   20/  193]    Overall Loss 0.000692    Objective Loss 0.000692                                        LR 0.000500    Time 2.199747    
2024-06-06 17:01:59,863 - Epoch: [24][   30/  193]    Overall Loss 0.000536    Objective Loss 0.000536                                        LR 0.000500    Time 1.806437    
2024-06-06 17:02:09,687 - Epoch: [24][   40/  193]    Overall Loss 0.000452    Objective Loss 0.000452                                        LR 0.000500    Time 1.599315    
2024-06-06 17:02:19,210 - Epoch: [24][   50/  193]    Overall Loss 0.000501    Objective Loss 0.000501                                        LR 0.000500    Time 1.469262    
2024-06-06 17:02:28,772 - Epoch: [24][   60/  193]    Overall Loss 0.000489    Objective Loss 0.000489                                        LR 0.000500    Time 1.382975    
2024-06-06 17:02:38,232 - Epoch: [24][   70/  193]    Overall Loss 0.000539    Objective Loss 0.000539                                        LR 0.000500    Time 1.320128    
2024-06-06 17:02:47,640 - Epoch: [24][   80/  193]    Overall Loss 0.000497    Objective Loss 0.000497                                        LR 0.000500    Time 1.272198    
2024-06-06 17:02:57,021 - Epoch: [24][   90/  193]    Overall Loss 0.000461    Objective Loss 0.000461                                        LR 0.000500    Time 1.234675    
2024-06-06 17:03:06,513 - Epoch: [24][  100/  193]    Overall Loss 0.000423    Objective Loss 0.000423                                        LR 0.000500    Time 1.205770    
2024-06-06 17:03:16,162 - Epoch: [24][  110/  193]    Overall Loss 0.000399    Objective Loss 0.000399                                        LR 0.000500    Time 1.183563    
2024-06-06 17:03:25,847 - Epoch: [24][  120/  193]    Overall Loss 0.000373    Objective Loss 0.000373                                        LR 0.000500    Time 1.165407    
2024-06-06 17:03:35,390 - Epoch: [24][  130/  193]    Overall Loss 0.000356    Objective Loss 0.000356                                        LR 0.000500    Time 1.148938    
2024-06-06 17:03:44,804 - Epoch: [24][  140/  193]    Overall Loss 0.000335    Objective Loss 0.000335                                        LR 0.000500    Time 1.133880    
2024-06-06 17:03:54,221 - Epoch: [24][  150/  193]    Overall Loss 0.000320    Objective Loss 0.000320                                        LR 0.000500    Time 1.120779    
2024-06-06 17:04:03,792 - Epoch: [24][  160/  193]    Overall Loss 0.000309    Objective Loss 0.000309                                        LR 0.000500    Time 1.110345    
2024-06-06 17:04:13,377 - Epoch: [24][  170/  193]    Overall Loss 0.000294    Objective Loss 0.000294                                        LR 0.000500    Time 1.101238    
2024-06-06 17:04:22,872 - Epoch: [24][  180/  193]    Overall Loss 0.000293    Objective Loss 0.000293                                        LR 0.000500    Time 1.092652    
2024-06-06 17:04:32,032 - Epoch: [24][  190/  193]    Overall Loss 0.000281    Objective Loss 0.000281                                        LR 0.000500    Time 1.083166    
2024-06-06 17:04:34,155 - Epoch: [24][  193/  193]    Overall Loss 0.000278    Objective Loss 0.000278    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 1.077154    
2024-06-06 17:04:34,869 - --- validate (epoch=24)-----------
2024-06-06 17:04:34,869 - 2172 samples (64 per mini-batch)
2024-06-06 17:05:01,544 - Epoch: [24][   10/   34]    Loss 0.000094    Top1 100.000000    Top5 100.000000    
2024-06-06 17:05:05,874 - Epoch: [24][   20/   34]    Loss 0.000104    Top1 100.000000    Top5 100.000000    
2024-06-06 17:05:09,897 - Epoch: [24][   30/   34]    Loss 0.000091    Top1 100.000000    Top5 100.000000    
2024-06-06 17:05:11,337 - Epoch: [24][   34/   34]    Loss 0.000095    Top1 100.000000    Top5 100.000000    
2024-06-06 17:05:12,431 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 17:05:12,431 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 17:05:12,739 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 24]
2024-06-06 17:05:12,740 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 17:05:12,770 - 

2024-06-06 17:05:12,770 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 17:05:44,689 - Epoch: [25][   10/  193]    Overall Loss 0.000092    Objective Loss 0.000092                                        LR 0.000500    Time 3.191685    
2024-06-06 17:05:54,655 - Epoch: [25][   20/  193]    Overall Loss 0.000090    Objective Loss 0.000090                                        LR 0.000500    Time 2.092536    
2024-06-06 17:06:04,488 - Epoch: [25][   30/  193]    Overall Loss 0.000076    Objective Loss 0.000076                                        LR 0.000500    Time 1.721684    
2024-06-06 17:06:14,292 - Epoch: [25][   40/  193]    Overall Loss 0.000069    Objective Loss 0.000069                                        LR 0.000500    Time 1.535334    
2024-06-06 17:06:23,874 - Epoch: [25][   50/  193]    Overall Loss 0.000085    Objective Loss 0.000085                                        LR 0.000500    Time 1.419368    
2024-06-06 17:06:33,496 - Epoch: [25][   60/  193]    Overall Loss 0.000128    Objective Loss 0.000128                                        LR 0.000500    Time 1.342604    
2024-06-06 17:06:43,655 - Epoch: [25][   70/  193]    Overall Loss 0.000176    Objective Loss 0.000176                                        LR 0.000500    Time 1.295214    
2024-06-06 17:06:54,433 - Epoch: [25][   80/  193]    Overall Loss 0.009823    Objective Loss 0.009823                                        LR 0.000500    Time 1.267059    
2024-06-06 17:07:04,845 - Epoch: [25][   90/  193]    Overall Loss 0.015610    Objective Loss 0.015610                                        LR 0.000500    Time 1.241542    
2024-06-06 17:07:15,383 - Epoch: [25][  100/  193]    Overall Loss 0.014922    Objective Loss 0.014922                                        LR 0.000500    Time 1.222406    
2024-06-06 17:07:24,812 - Epoch: [25][  110/  193]    Overall Loss 0.014279    Objective Loss 0.014279                                        LR 0.000500    Time 1.196752    
2024-06-06 17:07:34,189 - Epoch: [25][  120/  193]    Overall Loss 0.013409    Objective Loss 0.013409                                        LR 0.000500    Time 1.174905    
2024-06-06 17:07:43,489 - Epoch: [25][  130/  193]    Overall Loss 0.012468    Objective Loss 0.012468                                        LR 0.000500    Time 1.155838    
2024-06-06 17:07:52,662 - Epoch: [25][  140/  193]    Overall Loss 0.011640    Objective Loss 0.011640                                        LR 0.000500    Time 1.138598    
2024-06-06 17:08:01,830 - Epoch: [25][  150/  193]    Overall Loss 0.010931    Objective Loss 0.010931                                        LR 0.000500    Time 1.123602    
2024-06-06 17:08:11,038 - Epoch: [25][  160/  193]    Overall Loss 0.010335    Objective Loss 0.010335                                        LR 0.000500    Time 1.110671    
2024-06-06 17:08:20,218 - Epoch: [25][  170/  193]    Overall Loss 0.009768    Objective Loss 0.009768                                        LR 0.000500    Time 1.099112    
2024-06-06 17:08:29,321 - Epoch: [25][  180/  193]    Overall Loss 0.009253    Objective Loss 0.009253                                        LR 0.000500    Time 1.088427    
2024-06-06 17:08:38,134 - Epoch: [25][  190/  193]    Overall Loss 0.008787    Objective Loss 0.008787                                        LR 0.000500    Time 1.077357    
2024-06-06 17:08:40,510 - Epoch: [25][  193/  193]    Overall Loss 0.008652    Objective Loss 0.008652    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 1.072705    
2024-06-06 17:08:41,273 - --- validate (epoch=25)-----------
2024-06-06 17:08:41,273 - 2172 samples (64 per mini-batch)
2024-06-06 17:09:19,410 - Epoch: [25][   10/   34]    Loss 0.000358    Top1 100.000000    Top5 100.000000    
2024-06-06 17:09:23,845 - Epoch: [25][   20/   34]    Loss 0.000641    Top1 100.000000    Top5 100.000000    
2024-06-06 17:09:28,263 - Epoch: [25][   30/   34]    Loss 0.000652    Top1 100.000000    Top5 100.000000    
2024-06-06 17:09:29,631 - Epoch: [25][   34/   34]    Loss 0.000616    Top1 100.000000    Top5 100.000000    
2024-06-06 17:09:30,645 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.001

2024-06-06 17:09:30,646 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 17:09:30,859 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 25]
2024-06-06 17:09:30,859 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 17:09:30,879 - 

2024-06-06 17:09:30,879 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 17:10:01,993 - Epoch: [26][   10/  193]    Overall Loss 0.001180    Objective Loss 0.001180                                        LR 0.000500    Time 3.111383    
2024-06-06 17:10:11,382 - Epoch: [26][   20/  193]    Overall Loss 0.001852    Objective Loss 0.001852                                        LR 0.000500    Time 2.023461    
2024-06-06 17:10:20,687 - Epoch: [26][   30/  193]    Overall Loss 0.001311    Objective Loss 0.001311                                        LR 0.000500    Time 1.658087    
2024-06-06 17:10:29,836 - Epoch: [26][   40/  193]    Overall Loss 0.001115    Objective Loss 0.001115                                        LR 0.000500    Time 1.471409    
2024-06-06 17:10:38,933 - Epoch: [26][   50/  193]    Overall Loss 0.001035    Objective Loss 0.001035                                        LR 0.000500    Time 1.358404    
2024-06-06 17:10:48,259 - Epoch: [26][   60/  193]    Overall Loss 0.000917    Objective Loss 0.000917                                        LR 0.000500    Time 1.287010    
2024-06-06 17:10:58,517 - Epoch: [26][   70/  193]    Overall Loss 0.000859    Objective Loss 0.000859                                        LR 0.000500    Time 1.249076    
2024-06-06 17:11:08,000 - Epoch: [26][   80/  193]    Overall Loss 0.000847    Objective Loss 0.000847                                        LR 0.000500    Time 1.210978    
2024-06-06 17:11:17,360 - Epoch: [26][   90/  193]    Overall Loss 0.000806    Objective Loss 0.000806                                        LR 0.000500    Time 1.179994    
2024-06-06 17:11:26,698 - Epoch: [26][  100/  193]    Overall Loss 0.000746    Objective Loss 0.000746                                        LR 0.000500    Time 1.154769    
2024-06-06 17:11:35,900 - Epoch: [26][  110/  193]    Overall Loss 0.000701    Objective Loss 0.000701                                        LR 0.000500    Time 1.133181    
2024-06-06 17:11:45,236 - Epoch: [26][  120/  193]    Overall Loss 0.000654    Objective Loss 0.000654                                        LR 0.000500    Time 1.116261    
2024-06-06 17:11:54,615 - Epoch: [26][  130/  193]    Overall Loss 0.000639    Objective Loss 0.000639                                        LR 0.000500    Time 1.102344    
2024-06-06 17:12:03,894 - Epoch: [26][  140/  193]    Overall Loss 0.000621    Objective Loss 0.000621                                        LR 0.000500    Time 1.089617    
2024-06-06 17:12:13,381 - Epoch: [26][  150/  193]    Overall Loss 0.000596    Objective Loss 0.000596                                        LR 0.000500    Time 1.079988    
2024-06-06 17:12:23,091 - Epoch: [26][  160/  193]    Overall Loss 0.000606    Objective Loss 0.000606                                        LR 0.000500    Time 1.072907    
2024-06-06 17:12:32,482 - Epoch: [26][  170/  193]    Overall Loss 0.000580    Objective Loss 0.000580                                        LR 0.000500    Time 1.064785    
2024-06-06 17:12:41,773 - Epoch: [26][  180/  193]    Overall Loss 0.000568    Objective Loss 0.000568                                        LR 0.000500    Time 1.057029    
2024-06-06 17:12:50,650 - Epoch: [26][  190/  193]    Overall Loss 0.000551    Objective Loss 0.000551                                        LR 0.000500    Time 1.047942    
2024-06-06 17:12:52,754 - Epoch: [26][  193/  193]    Overall Loss 0.000544    Objective Loss 0.000544    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 1.042388    
2024-06-06 17:12:53,454 - --- validate (epoch=26)-----------
2024-06-06 17:12:53,455 - 2172 samples (64 per mini-batch)
2024-06-06 17:13:19,968 - Epoch: [26][   10/   34]    Loss 0.000237    Top1 100.000000    Top5 100.000000    
2024-06-06 17:13:24,300 - Epoch: [26][   20/   34]    Loss 0.000313    Top1 100.000000    Top5 100.000000    
2024-06-06 17:13:28,386 - Epoch: [26][   30/   34]    Loss 0.000289    Top1 100.000000    Top5 100.000000    
2024-06-06 17:13:29,802 - Epoch: [26][   34/   34]    Loss 0.000284    Top1 100.000000    Top5 100.000000    
2024-06-06 17:13:30,836 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 17:13:30,836 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 17:13:31,090 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 26]
2024-06-06 17:13:31,091 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 17:13:31,112 - 

2024-06-06 17:13:31,112 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 17:14:02,573 - Epoch: [27][   10/  193]    Overall Loss 0.000121    Objective Loss 0.000121                                        LR 0.000500    Time 3.145955    
2024-06-06 17:14:12,809 - Epoch: [27][   20/  193]    Overall Loss 0.000233    Objective Loss 0.000233                                        LR 0.000500    Time 2.082393    
2024-06-06 17:14:22,749 - Epoch: [27][   30/  193]    Overall Loss 0.000215    Objective Loss 0.000215                                        LR 0.000500    Time 1.717407    
2024-06-06 17:14:32,026 - Epoch: [27][   40/  193]    Overall Loss 0.000408    Objective Loss 0.000408                                        LR 0.000500    Time 1.519046    
2024-06-06 17:14:41,170 - Epoch: [27][   50/  193]    Overall Loss 0.000471    Objective Loss 0.000471                                        LR 0.000500    Time 1.397317    
2024-06-06 17:14:50,326 - Epoch: [27][   60/  193]    Overall Loss 0.000462    Objective Loss 0.000462                                        LR 0.000500    Time 1.316492    
2024-06-06 17:14:59,563 - Epoch: [27][   70/  193]    Overall Loss 0.000427    Objective Loss 0.000427                                        LR 0.000500    Time 1.259880    
2024-06-06 17:15:08,708 - Epoch: [27][   80/  193]    Overall Loss 0.000411    Objective Loss 0.000411                                        LR 0.000500    Time 1.216356    
2024-06-06 17:15:17,712 - Epoch: [27][   90/  193]    Overall Loss 0.000389    Objective Loss 0.000389                                        LR 0.000500    Time 1.180932    
2024-06-06 17:15:27,311 - Epoch: [27][  100/  193]    Overall Loss 0.000361    Objective Loss 0.000361                                        LR 0.000500    Time 1.158509    
2024-06-06 17:15:36,897 - Epoch: [27][  110/  193]    Overall Loss 0.000337    Objective Loss 0.000337                                        LR 0.000500    Time 1.140051    
2024-06-06 17:15:46,303 - Epoch: [27][  120/  193]    Overall Loss 0.000315    Objective Loss 0.000315                                        LR 0.000500    Time 1.123140    
2024-06-06 17:15:56,284 - Epoch: [27][  130/  193]    Overall Loss 0.000309    Objective Loss 0.000309                                        LR 0.000500    Time 1.113273    
2024-06-06 17:16:07,243 - Epoch: [27][  140/  193]    Overall Loss 0.000317    Objective Loss 0.000317                                        LR 0.000500    Time 1.111621    
2024-06-06 17:16:17,067 - Epoch: [27][  150/  193]    Overall Loss 0.000319    Objective Loss 0.000319                                        LR 0.000500    Time 1.102684    
2024-06-06 17:16:26,713 - Epoch: [27][  160/  193]    Overall Loss 0.000353    Objective Loss 0.000353                                        LR 0.000500    Time 1.093815    
2024-06-06 17:16:36,330 - Epoch: [27][  170/  193]    Overall Loss 0.000360    Objective Loss 0.000360                                        LR 0.000500    Time 1.085802    
2024-06-06 17:16:45,891 - Epoch: [27][  180/  193]    Overall Loss 0.000350    Objective Loss 0.000350                                        LR 0.000500    Time 1.078366    
2024-06-06 17:16:54,898 - Epoch: [27][  190/  193]    Overall Loss 0.000345    Objective Loss 0.000345                                        LR 0.000500    Time 1.068778    
2024-06-06 17:16:57,026 - Epoch: [27][  193/  193]    Overall Loss 0.000341    Objective Loss 0.000341    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 1.062950    
2024-06-06 17:16:57,713 - --- validate (epoch=27)-----------
2024-06-06 17:16:57,714 - 2172 samples (64 per mini-batch)
2024-06-06 17:17:24,136 - Epoch: [27][   10/   34]    Loss 0.000162    Top1 100.000000    Top5 100.000000    
2024-06-06 17:17:28,270 - Epoch: [27][   20/   34]    Loss 0.000145    Top1 100.000000    Top5 100.000000    
2024-06-06 17:17:32,187 - Epoch: [27][   30/   34]    Loss 0.000155    Top1 100.000000    Top5 100.000000    
2024-06-06 17:17:33,547 - Epoch: [27][   34/   34]    Loss 0.000148    Top1 100.000000    Top5 100.000000    
2024-06-06 17:17:34,562 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 17:17:34,562 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 17:17:34,779 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 27]
2024-06-06 17:17:34,779 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 17:17:34,797 - 

2024-06-06 17:17:34,797 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 18:12:40,683 - Epoch: [28][   10/  193]    Overall Loss 0.000672    Objective Loss 0.000672                                        LR 0.000500    Time 330.588333    
2024-06-06 18:12:48,952 - Epoch: [28][   20/  193]    Overall Loss 0.000456    Objective Loss 0.000456                                        LR 0.000500    Time 165.706083    
2024-06-06 18:12:57,093 - Epoch: [28][   30/  193]    Overall Loss 0.000350    Objective Loss 0.000350                                        LR 0.000500    Time 110.741097    
2024-06-06 18:13:05,345 - Epoch: [28][   40/  193]    Overall Loss 0.000287    Objective Loss 0.000287                                        LR 0.000500    Time 83.261082    
2024-06-06 18:13:13,412 - Epoch: [28][   50/  193]    Overall Loss 0.000249    Objective Loss 0.000249                                        LR 0.000500    Time 66.769634    
2024-06-06 18:13:21,752 - Epoch: [28][   60/  193]    Overall Loss 0.000276    Objective Loss 0.000276                                        LR 0.000500    Time 55.779858    
2024-06-06 18:13:29,911 - Epoch: [28][   70/  193]    Overall Loss 0.000365    Objective Loss 0.000365                                        LR 0.000500    Time 47.927022    
2024-06-06 18:13:37,685 - Epoch: [28][   80/  193]    Overall Loss 0.000340    Objective Loss 0.000340                                        LR 0.000500    Time 42.032898    
2024-06-06 18:13:45,586 - Epoch: [28][   90/  193]    Overall Loss 0.000315    Objective Loss 0.000315                                        LR 0.000500    Time 37.450067    
2024-06-06 18:13:53,291 - Epoch: [28][  100/  193]    Overall Loss 0.000302    Objective Loss 0.000302                                        LR 0.000500    Time 33.781850    
2024-06-06 18:14:01,072 - Epoch: [28][  110/  193]    Overall Loss 0.000298    Objective Loss 0.000298                                        LR 0.000500    Time 30.781245    
2024-06-06 18:14:08,877 - Epoch: [28][  120/  193]    Overall Loss 0.000279    Objective Loss 0.000279                                        LR 0.000500    Time 28.281012    
2024-06-06 18:14:16,670 - Epoch: [28][  130/  193]    Overall Loss 0.000272    Objective Loss 0.000272                                        LR 0.000500    Time 26.165274    
2024-06-06 18:14:24,534 - Epoch: [28][  140/  193]    Overall Loss 0.000263    Objective Loss 0.000263                                        LR 0.000500    Time 24.352339    
2024-06-06 18:14:32,296 - Epoch: [28][  150/  193]    Overall Loss 0.000259    Objective Loss 0.000259                                        LR 0.000500    Time 22.780432    
2024-06-06 18:14:40,224 - Epoch: [28][  160/  193]    Overall Loss 0.000254    Objective Loss 0.000254                                        LR 0.000500    Time 21.406049    
2024-06-06 18:14:47,969 - Epoch: [28][  170/  193]    Overall Loss 0.000243    Objective Loss 0.000243                                        LR 0.000500    Time 20.192270    
2024-06-06 18:14:55,697 - Epoch: [28][  180/  193]    Overall Loss 0.000250    Objective Loss 0.000250                                        LR 0.000500    Time 19.113248    
2024-06-06 18:15:03,222 - Epoch: [28][  190/  193]    Overall Loss 0.000242    Objective Loss 0.000242                                        LR 0.000500    Time 18.146721    
2024-06-06 18:15:05,001 - Epoch: [28][  193/  193]    Overall Loss 0.000238    Objective Loss 0.000238    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 17.873710    
2024-06-06 18:15:05,630 - --- validate (epoch=28)-----------
2024-06-06 18:15:05,630 - 2172 samples (64 per mini-batch)
2024-06-06 18:15:29,486 - Epoch: [28][   10/   34]    Loss 0.000181    Top1 100.000000    Top5 100.000000    
2024-06-06 18:15:32,750 - Epoch: [28][   20/   34]    Loss 0.000350    Top1 100.000000    Top5 100.000000    
2024-06-06 18:15:35,889 - Epoch: [28][   30/   34]    Loss 0.000264    Top1 100.000000    Top5 100.000000    
2024-06-06 18:15:37,022 - Epoch: [28][   34/   34]    Loss 0.000242    Top1 100.000000    Top5 100.000000    
2024-06-06 18:15:37,914 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 18:15:37,915 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 18:15:38,084 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 28]
2024-06-06 18:15:38,085 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 18:15:38,100 - 

2024-06-06 18:15:38,100 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 18:16:06,120 - Epoch: [29][   10/  193]    Overall Loss 0.000085    Objective Loss 0.000085                                        LR 0.000500    Time 2.801968    
2024-06-06 18:16:13,911 - Epoch: [29][   20/  193]    Overall Loss 0.000277    Objective Loss 0.000277                                        LR 0.000500    Time 1.789050    
2024-06-06 18:16:21,824 - Epoch: [29][   30/  193]    Overall Loss 0.000212    Objective Loss 0.000212                                        LR 0.000500    Time 1.455664    
2024-06-06 18:16:29,796 - Epoch: [29][   40/  193]    Overall Loss 0.000340    Objective Loss 0.000340                                        LR 0.000500    Time 1.289884    
2024-06-06 18:16:37,679 - Epoch: [29][   50/  193]    Overall Loss 0.000295    Objective Loss 0.000295                                        LR 0.000500    Time 1.188752    
2024-06-06 18:16:45,722 - Epoch: [29][   60/  193]    Overall Loss 0.000275    Objective Loss 0.000275                                        LR 0.000500    Time 1.124117    
2024-06-06 18:16:53,535 - Epoch: [29][   70/  193]    Overall Loss 0.000267    Objective Loss 0.000267                                        LR 0.000500    Time 1.074650    
2024-06-06 18:17:01,232 - Epoch: [29][   80/  193]    Overall Loss 0.000247    Objective Loss 0.000247                                        LR 0.000500    Time 1.036211    
2024-06-06 18:17:09,048 - Epoch: [29][   90/  193]    Overall Loss 0.000230    Objective Loss 0.000230                                        LR 0.000500    Time 1.007563    
2024-06-06 18:17:17,886 - Epoch: [29][  100/  193]    Overall Loss 0.000223    Objective Loss 0.000223                                        LR 0.000500    Time 0.994888    
2024-06-06 18:17:27,210 - Epoch: [29][  110/  193]    Overall Loss 0.000218    Objective Loss 0.000218                                        LR 0.000500    Time 0.988812    
2024-06-06 18:17:36,669 - Epoch: [29][  120/  193]    Overall Loss 0.000212    Objective Loss 0.000212                                        LR 0.000500    Time 0.984872    
2024-06-06 18:17:45,865 - Epoch: [29][  130/  193]    Overall Loss 0.000202    Objective Loss 0.000202                                        LR 0.000500    Time 0.979464    
2024-06-06 18:17:55,229 - Epoch: [29][  140/  193]    Overall Loss 0.000195    Objective Loss 0.000195                                        LR 0.000500    Time 0.976126    
2024-06-06 18:18:04,019 - Epoch: [29][  150/  193]    Overall Loss 0.000187    Objective Loss 0.000187                                        LR 0.000500    Time 0.969415    
2024-06-06 18:18:12,416 - Epoch: [29][  160/  193]    Overall Loss 0.000186    Objective Loss 0.000186                                        LR 0.000500    Time 0.961126    
2024-06-06 18:18:20,725 - Epoch: [29][  170/  193]    Overall Loss 0.000178    Objective Loss 0.000178                                        LR 0.000500    Time 0.953312    
2024-06-06 18:18:29,225 - Epoch: [29][  180/  193]    Overall Loss 0.000184    Objective Loss 0.000184                                        LR 0.000500    Time 0.947358    
2024-06-06 18:18:37,328 - Epoch: [29][  190/  193]    Overall Loss 0.000181    Objective Loss 0.000181                                        LR 0.000500    Time 0.940011    
2024-06-06 18:18:39,251 - Epoch: [29][  193/  193]    Overall Loss 0.000179    Objective Loss 0.000179    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 0.935178    
2024-06-06 18:18:39,932 - --- validate (epoch=29)-----------
2024-06-06 18:18:39,932 - 2172 samples (64 per mini-batch)
2024-06-06 18:19:03,632 - Epoch: [29][   10/   34]    Loss 0.000058    Top1 100.000000    Top5 100.000000    
2024-06-06 18:19:07,278 - Epoch: [29][   20/   34]    Loss 0.000090    Top1 100.000000    Top5 100.000000    
2024-06-06 18:19:10,687 - Epoch: [29][   30/   34]    Loss 0.000095    Top1 100.000000    Top5 100.000000    
2024-06-06 18:19:11,932 - Epoch: [29][   34/   34]    Loss 0.000093    Top1 100.000000    Top5 100.000000    
2024-06-06 18:19:12,904 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 18:19:12,904 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 18:19:13,105 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 29]
2024-06-06 18:19:13,106 - Saving checkpoint to: logs\2024.06.06-144451\checkpoint.pth.tar
2024-06-06 18:19:13,126 - 

2024-06-06 18:19:13,126 - Initiating quantization aware training (QAT)...
2024-06-06 18:19:13,138 - 

2024-06-06 18:19:13,138 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 18:19:42,940 - Epoch: [30][   10/  193]    Overall Loss 0.000106    Objective Loss 0.000106                                        LR 0.000500    Time 2.980280    
2024-06-06 18:19:53,015 - Epoch: [30][   20/  193]    Overall Loss 0.000115    Objective Loss 0.000115                                        LR 0.000500    Time 1.992289    
2024-06-06 18:20:02,869 - Epoch: [30][   30/  193]    Overall Loss 0.000105    Objective Loss 0.000105                                        LR 0.000500    Time 1.655349    
2024-06-06 18:20:13,028 - Epoch: [30][   40/  193]    Overall Loss 0.000108    Objective Loss 0.000108                                        LR 0.000500    Time 1.494782    
2024-06-06 18:20:23,109 - Epoch: [30][   50/  193]    Overall Loss 0.000100    Objective Loss 0.000100                                        LR 0.000500    Time 1.396820    
2024-06-06 18:20:34,344 - Epoch: [30][   60/  193]    Overall Loss 0.000098    Objective Loss 0.000098                                        LR 0.000500    Time 1.350576    
2024-06-06 18:20:44,867 - Epoch: [30][   70/  193]    Overall Loss 0.000195    Objective Loss 0.000195                                        LR 0.000500    Time 1.307517    
2024-06-06 18:20:54,809 - Epoch: [30][   80/  193]    Overall Loss 0.000196    Objective Loss 0.000196                                        LR 0.000500    Time 1.268020    
2024-06-06 18:21:05,194 - Epoch: [30][   90/  193]    Overall Loss 0.000263    Objective Loss 0.000263                                        LR 0.000500    Time 1.242206    
2024-06-06 18:21:15,740 - Epoch: [30][  100/  193]    Overall Loss 0.000304    Objective Loss 0.000304                                        LR 0.000500    Time 1.223076    
2024-06-06 18:21:25,666 - Epoch: [30][  110/  193]    Overall Loss 0.000299    Objective Loss 0.000299                                        LR 0.000500    Time 1.201889    
2024-06-06 18:21:35,702 - Epoch: [30][  120/  193]    Overall Loss 0.000404    Objective Loss 0.000404                                        LR 0.000500    Time 1.185124    
2024-06-06 18:21:45,826 - Epoch: [30][  130/  193]    Overall Loss 0.000412    Objective Loss 0.000412                                        LR 0.000500    Time 1.171660    
2024-06-06 18:21:55,859 - Epoch: [30][  140/  193]    Overall Loss 0.000401    Objective Loss 0.000401                                        LR 0.000500    Time 1.159425    
2024-06-06 18:22:06,032 - Epoch: [30][  150/  193]    Overall Loss 0.000394    Objective Loss 0.000394                                        LR 0.000500    Time 1.149770    
2024-06-06 18:22:16,301 - Epoch: [30][  160/  193]    Overall Loss 0.000388    Objective Loss 0.000388                                        LR 0.000500    Time 1.141854    
2024-06-06 18:22:26,619 - Epoch: [30][  170/  193]    Overall Loss 0.000372    Objective Loss 0.000372                                        LR 0.000500    Time 1.135204    
2024-06-06 18:22:36,868 - Epoch: [30][  180/  193]    Overall Loss 0.000357    Objective Loss 0.000357                                        LR 0.000500    Time 1.128927    
2024-06-06 18:22:46,933 - Epoch: [30][  190/  193]    Overall Loss 0.000346    Objective Loss 0.000346                                        LR 0.000500    Time 1.122311    
2024-06-06 18:22:49,377 - Epoch: [30][  193/  193]    Overall Loss 0.000342    Objective Loss 0.000342    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 1.117349    
2024-06-06 18:22:50,020 - --- validate (epoch=30)-----------
2024-06-06 18:22:50,021 - 2172 samples (64 per mini-batch)
2024-06-06 18:23:16,459 - Epoch: [30][   10/   34]    Loss 0.000059    Top1 100.000000    Top5 100.000000    
2024-06-06 18:23:22,106 - Epoch: [30][   20/   34]    Loss 0.000221    Top1 100.000000    Top5 100.000000    
2024-06-06 18:23:27,471 - Epoch: [30][   30/   34]    Loss 0.000343    Top1 100.000000    Top5 100.000000    
2024-06-06 18:23:29,434 - Epoch: [30][   34/   34]    Loss 0.000312    Top1 100.000000    Top5 100.000000    
2024-06-06 18:23:30,407 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 18:23:30,407 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 18:23:30,613 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 30]
2024-06-06 18:23:30,613 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 18:23:30,644 - 

2024-06-06 18:23:30,644 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 18:24:01,034 - Epoch: [31][   10/  193]    Overall Loss 0.000124    Objective Loss 0.000124                                        LR 0.000500    Time 3.038957    
2024-06-06 18:24:11,261 - Epoch: [31][   20/  193]    Overall Loss 0.000242    Objective Loss 0.000242                                        LR 0.000500    Time 2.029165    
2024-06-06 18:24:21,421 - Epoch: [31][   30/  193]    Overall Loss 0.000206    Objective Loss 0.000206                                        LR 0.000500    Time 1.690589    
2024-06-06 18:24:31,603 - Epoch: [31][   40/  193]    Overall Loss 0.000169    Objective Loss 0.000169                                        LR 0.000500    Time 1.521678    
2024-06-06 18:24:41,627 - Epoch: [31][   50/  193]    Overall Loss 0.000147    Objective Loss 0.000147                                        LR 0.000500    Time 1.417291    
2024-06-06 18:24:51,838 - Epoch: [31][   60/  193]    Overall Loss 0.000138    Objective Loss 0.000138                                        LR 0.000500    Time 1.350794    
2024-06-06 18:25:01,903 - Epoch: [31][   70/  193]    Overall Loss 0.000136    Objective Loss 0.000136                                        LR 0.000500    Time 1.300994    
2024-06-06 18:25:12,002 - Epoch: [31][   80/  193]    Overall Loss 0.000129    Objective Loss 0.000129                                        LR 0.000500    Time 1.264241    
2024-06-06 18:25:22,137 - Epoch: [31][   90/  193]    Overall Loss 0.000151    Objective Loss 0.000151                                        LR 0.000500    Time 1.236019    
2024-06-06 18:25:32,188 - Epoch: [31][  100/  193]    Overall Loss 0.000142    Objective Loss 0.000142                                        LR 0.000500    Time 1.212668    
2024-06-06 18:25:42,484 - Epoch: [31][  110/  193]    Overall Loss 0.000137    Objective Loss 0.000137                                        LR 0.000500    Time 1.195828    
2024-06-06 18:25:52,741 - Epoch: [31][  120/  193]    Overall Loss 0.000130    Objective Loss 0.000130                                        LR 0.000500    Time 1.181365    
2024-06-06 18:26:02,945 - Epoch: [31][  130/  193]    Overall Loss 0.000124    Objective Loss 0.000124                                        LR 0.000500    Time 1.168759    
2024-06-06 18:26:13,026 - Epoch: [31][  140/  193]    Overall Loss 0.000128    Objective Loss 0.000128                                        LR 0.000500    Time 1.157112    
2024-06-06 18:26:23,051 - Epoch: [31][  150/  193]    Overall Loss 0.000128    Objective Loss 0.000128                                        LR 0.000500    Time 1.146572    
2024-06-06 18:26:33,110 - Epoch: [31][  160/  193]    Overall Loss 0.000130    Objective Loss 0.000130                                        LR 0.000500    Time 1.137610    
2024-06-06 18:26:43,230 - Epoch: [31][  170/  193]    Overall Loss 0.000127    Objective Loss 0.000127                                        LR 0.000500    Time 1.130050    
2024-06-06 18:26:53,377 - Epoch: [31][  180/  193]    Overall Loss 0.000140    Objective Loss 0.000140                                        LR 0.000500    Time 1.123452    
2024-06-06 18:27:03,091 - Epoch: [31][  190/  193]    Overall Loss 0.000152    Objective Loss 0.000152                                        LR 0.000500    Time 1.115290    
2024-06-06 18:27:05,421 - Epoch: [31][  193/  193]    Overall Loss 0.000152    Objective Loss 0.000152    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 1.109865    
2024-06-06 18:27:06,081 - --- validate (epoch=31)-----------
2024-06-06 18:27:06,081 - 2172 samples (64 per mini-batch)
2024-06-06 18:27:33,169 - Epoch: [31][   10/   34]    Loss 0.000067    Top1 100.000000    Top5 100.000000    
2024-06-06 18:27:38,881 - Epoch: [31][   20/   34]    Loss 0.000096    Top1 100.000000    Top5 100.000000    
2024-06-06 18:27:44,261 - Epoch: [31][   30/   34]    Loss 0.000122    Top1 100.000000    Top5 100.000000    
2024-06-06 18:27:46,237 - Epoch: [31][   34/   34]    Loss 0.000122    Top1 100.000000    Top5 100.000000    
2024-06-06 18:27:47,293 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 18:27:47,293 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 18:27:47,502 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 31]
2024-06-06 18:27:47,502 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 18:27:47,521 - 

2024-06-06 18:27:47,521 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 18:28:18,235 - Epoch: [32][   10/  193]    Overall Loss 0.001631    Objective Loss 0.001631                                        LR 0.000500    Time 3.071350    
2024-06-06 18:28:28,548 - Epoch: [32][   20/  193]    Overall Loss 0.001278    Objective Loss 0.001278                                        LR 0.000500    Time 2.049846    
2024-06-06 18:28:38,848 - Epoch: [32][   30/  193]    Overall Loss 0.000885    Objective Loss 0.000885                                        LR 0.000500    Time 1.708975    
2024-06-06 18:28:49,357 - Epoch: [32][   40/  193]    Overall Loss 0.000766    Objective Loss 0.000766                                        LR 0.000500    Time 1.543579    
2024-06-06 18:28:59,674 - Epoch: [32][   50/  193]    Overall Loss 0.000628    Objective Loss 0.000628                                        LR 0.000500    Time 1.440539    
2024-06-06 18:29:10,172 - Epoch: [32][   60/  193]    Overall Loss 0.000554    Objective Loss 0.000554                                        LR 0.000500    Time 1.374923    
2024-06-06 18:29:20,686 - Epoch: [32][   70/  193]    Overall Loss 0.000592    Objective Loss 0.000592                                        LR 0.000500    Time 1.328318    
2024-06-06 18:29:31,030 - Epoch: [32][   80/  193]    Overall Loss 0.000533    Objective Loss 0.000533                                        LR 0.000500    Time 1.291110    
2024-06-06 18:29:41,359 - Epoch: [32][   90/  193]    Overall Loss 0.000518    Objective Loss 0.000518                                        LR 0.000500    Time 1.262032    
2024-06-06 18:29:51,707 - Epoch: [32][  100/  193]    Overall Loss 0.000473    Objective Loss 0.000473                                        LR 0.000500    Time 1.239066    
2024-06-06 18:30:02,027 - Epoch: [32][  110/  193]    Overall Loss 0.000441    Objective Loss 0.000441                                        LR 0.000500    Time 1.219960    
2024-06-06 18:30:12,291 - Epoch: [32][  120/  193]    Overall Loss 0.000407    Objective Loss 0.000407                                        LR 0.000500    Time 1.203592    
2024-06-06 18:30:22,708 - Epoch: [32][  130/  193]    Overall Loss 0.000380    Objective Loss 0.000380                                        LR 0.000500    Time 1.190973    
2024-06-06 18:30:32,869 - Epoch: [32][  140/  193]    Overall Loss 0.000357    Objective Loss 0.000357                                        LR 0.000500    Time 1.178282    
2024-06-06 18:30:42,965 - Epoch: [32][  150/  193]    Overall Loss 0.000388    Objective Loss 0.000388                                        LR 0.000500    Time 1.166831    
2024-06-06 18:30:53,161 - Epoch: [32][  160/  193]    Overall Loss 0.000420    Objective Loss 0.000420                                        LR 0.000500    Time 1.157480    
2024-06-06 18:31:03,164 - Epoch: [32][  170/  193]    Overall Loss 0.000414    Objective Loss 0.000414                                        LR 0.000500    Time 1.148054    
2024-06-06 18:31:13,208 - Epoch: [32][  180/  193]    Overall Loss 0.000430    Objective Loss 0.000430                                        LR 0.000500    Time 1.139924    
2024-06-06 18:31:22,954 - Epoch: [32][  190/  193]    Overall Loss 0.000412    Objective Loss 0.000412                                        LR 0.000500    Time 1.131075    
2024-06-06 18:31:25,236 - Epoch: [32][  193/  193]    Overall Loss 0.000406    Objective Loss 0.000406    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 1.125143    
2024-06-06 18:31:25,856 - --- validate (epoch=32)-----------
2024-06-06 18:31:25,856 - 2172 samples (64 per mini-batch)
2024-06-06 18:31:51,156 - Epoch: [32][   10/   34]    Loss 0.000166    Top1 100.000000    Top5 100.000000    
2024-06-06 18:31:56,642 - Epoch: [32][   20/   34]    Loss 0.000126    Top1 100.000000    Top5 100.000000    
2024-06-06 18:32:01,883 - Epoch: [32][   30/   34]    Loss 0.000158    Top1 100.000000    Top5 100.000000    
2024-06-06 18:32:03,807 - Epoch: [32][   34/   34]    Loss 0.000144    Top1 100.000000    Top5 100.000000    
2024-06-06 18:32:04,730 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 18:32:04,730 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 18:32:04,925 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 32]
2024-06-06 18:32:04,925 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 18:32:04,942 - 

2024-06-06 18:32:04,942 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 18:32:42,986 - Epoch: [33][   10/  193]    Overall Loss 0.000149    Objective Loss 0.000149                                        LR 0.000500    Time 3.804200    
2024-06-06 18:32:52,997 - Epoch: [33][   20/  193]    Overall Loss 0.000276    Objective Loss 0.000276                                        LR 0.000500    Time 2.401098    
2024-06-06 18:33:03,262 - Epoch: [33][   30/  193]    Overall Loss 0.000229    Objective Loss 0.000229                                        LR 0.000500    Time 1.941868    
2024-06-06 18:33:13,326 - Epoch: [33][   40/  193]    Overall Loss 0.000184    Objective Loss 0.000184                                        LR 0.000500    Time 1.707259    
2024-06-06 18:33:23,322 - Epoch: [33][   50/  193]    Overall Loss 0.000153    Objective Loss 0.000153                                        LR 0.000500    Time 1.565119    
2024-06-06 18:33:33,592 - Epoch: [33][   60/  193]    Overall Loss 0.000145    Objective Loss 0.000145                                        LR 0.000500    Time 1.474724    
2024-06-06 18:33:43,597 - Epoch: [33][   70/  193]    Overall Loss 0.000157    Objective Loss 0.000157                                        LR 0.000500    Time 1.406333    
2024-06-06 18:33:53,632 - Epoch: [33][   80/  193]    Overall Loss 0.000147    Objective Loss 0.000147                                        LR 0.000500    Time 1.355665    
2024-06-06 18:34:03,535 - Epoch: [33][   90/  193]    Overall Loss 0.000143    Objective Loss 0.000143                                        LR 0.000500    Time 1.314729    
2024-06-06 18:34:13,704 - Epoch: [33][  100/  193]    Overall Loss 0.000134    Objective Loss 0.000134                                        LR 0.000500    Time 1.284683    
2024-06-06 18:34:23,824 - Epoch: [33][  110/  193]    Overall Loss 0.000126    Objective Loss 0.000126                                        LR 0.000500    Time 1.259635    
2024-06-06 18:34:33,965 - Epoch: [33][  120/  193]    Overall Loss 0.000126    Objective Loss 0.000126                                        LR 0.000500    Time 1.238996    
2024-06-06 18:34:44,206 - Epoch: [33][  130/  193]    Overall Loss 0.000121    Objective Loss 0.000121                                        LR 0.000500    Time 1.222270    
2024-06-06 18:34:54,334 - Epoch: [33][  140/  193]    Overall Loss 0.000120    Objective Loss 0.000120                                        LR 0.000500    Time 1.207092    
2024-06-06 18:35:04,507 - Epoch: [33][  150/  193]    Overall Loss 0.000116    Objective Loss 0.000116                                        LR 0.000500    Time 1.194186    
2024-06-06 18:35:14,618 - Epoch: [33][  160/  193]    Overall Loss 0.000115    Objective Loss 0.000115                                        LR 0.000500    Time 1.182602    
2024-06-06 18:35:24,802 - Epoch: [33][  170/  193]    Overall Loss 0.000114    Objective Loss 0.000114                                        LR 0.000500    Time 1.172762    
2024-06-06 18:35:34,921 - Epoch: [33][  180/  193]    Overall Loss 0.000113    Objective Loss 0.000113                                        LR 0.000500    Time 1.163653    
2024-06-06 18:35:44,781 - Epoch: [33][  190/  193]    Overall Loss 0.000121    Objective Loss 0.000121                                        LR 0.000500    Time 1.154159    
2024-06-06 18:35:47,119 - Epoch: [33][  193/  193]    Overall Loss 0.000121    Objective Loss 0.000121    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 1.148186    
2024-06-06 18:35:47,792 - --- validate (epoch=33)-----------
2024-06-06 18:35:47,792 - 2172 samples (64 per mini-batch)
2024-06-06 18:36:13,639 - Epoch: [33][   10/   34]    Loss 0.000077    Top1 100.000000    Top5 100.000000    
2024-06-06 18:36:19,261 - Epoch: [33][   20/   34]    Loss 0.000322    Top1 100.000000    Top5 100.000000    
2024-06-06 18:36:24,646 - Epoch: [33][   30/   34]    Loss 0.000250    Top1 100.000000    Top5 100.000000    
2024-06-06 18:36:26,678 - Epoch: [33][   34/   34]    Loss 0.000229    Top1 100.000000    Top5 100.000000    
2024-06-06 18:36:27,619 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 18:36:27,619 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 18:36:27,820 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 33]
2024-06-06 18:36:27,820 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 18:36:27,837 - 

2024-06-06 18:36:27,837 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 18:36:57,785 - Epoch: [34][   10/  193]    Overall Loss 0.000275    Objective Loss 0.000275                                        LR 0.000500    Time 2.994746    
2024-06-06 18:37:07,960 - Epoch: [34][   20/  193]    Overall Loss 0.000226    Objective Loss 0.000226                                        LR 0.000500    Time 2.004491    
2024-06-06 18:37:17,982 - Epoch: [34][   30/  193]    Overall Loss 0.000193    Objective Loss 0.000193                                        LR 0.000500    Time 1.669282    
2024-06-06 18:37:28,176 - Epoch: [34][   40/  193]    Overall Loss 0.000170    Objective Loss 0.000170                                        LR 0.000500    Time 1.506241    
2024-06-06 18:37:38,189 - Epoch: [34][   50/  193]    Overall Loss 0.000149    Objective Loss 0.000149                                        LR 0.000500    Time 1.404780    
2024-06-06 18:37:48,649 - Epoch: [34][   60/  193]    Overall Loss 0.000131    Objective Loss 0.000131                                        LR 0.000500    Time 1.344375    
2024-06-06 18:37:58,937 - Epoch: [34][   70/  193]    Overall Loss 0.000131    Objective Loss 0.000131                                        LR 0.000500    Time 1.298851    
2024-06-06 18:38:08,942 - Epoch: [34][   80/  193]    Overall Loss 0.000142    Objective Loss 0.000142                                        LR 0.000500    Time 1.261153    
2024-06-06 18:38:18,902 - Epoch: [34][   90/  193]    Overall Loss 0.000134    Objective Loss 0.000134                                        LR 0.000500    Time 1.231349    
2024-06-06 18:38:28,969 - Epoch: [34][  100/  193]    Overall Loss 0.000127    Objective Loss 0.000127                                        LR 0.000500    Time 1.208575    
2024-06-06 18:38:39,008 - Epoch: [34][  110/  193]    Overall Loss 0.000122    Objective Loss 0.000122                                        LR 0.000500    Time 1.189697    
2024-06-06 18:38:49,057 - Epoch: [34][  120/  193]    Overall Loss 0.000118    Objective Loss 0.000118                                        LR 0.000500    Time 1.174131    
2024-06-06 18:38:59,112 - Epoch: [34][  130/  193]    Overall Loss 0.000116    Objective Loss 0.000116                                        LR 0.000500    Time 1.160983    
2024-06-06 18:39:09,059 - Epoch: [34][  140/  193]    Overall Loss 0.000116    Objective Loss 0.000116                                        LR 0.000500    Time 1.148923    
2024-06-06 18:39:19,080 - Epoch: [34][  150/  193]    Overall Loss 0.000117    Objective Loss 0.000117                                        LR 0.000500    Time 1.138992    
2024-06-06 18:39:29,224 - Epoch: [34][  160/  193]    Overall Loss 0.000115    Objective Loss 0.000115                                        LR 0.000500    Time 1.131037    
2024-06-06 18:39:39,275 - Epoch: [34][  170/  193]    Overall Loss 0.000111    Objective Loss 0.000111                                        LR 0.000500    Time 1.123447    
2024-06-06 18:39:49,578 - Epoch: [34][  180/  193]    Overall Loss 0.000144    Objective Loss 0.000144                                        LR 0.000500    Time 1.118082    
2024-06-06 18:39:59,382 - Epoch: [34][  190/  193]    Overall Loss 0.000141    Objective Loss 0.000141                                        LR 0.000500    Time 1.110704    
2024-06-06 18:40:01,700 - Epoch: [34][  193/  193]    Overall Loss 0.000139    Objective Loss 0.000139    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 1.105332    
2024-06-06 18:40:02,344 - --- validate (epoch=34)-----------
2024-06-06 18:40:02,345 - 2172 samples (64 per mini-batch)
2024-06-06 18:40:27,851 - Epoch: [34][   10/   34]    Loss 0.000043    Top1 100.000000    Top5 100.000000    
2024-06-06 18:40:33,443 - Epoch: [34][   20/   34]    Loss 0.000149    Top1 100.000000    Top5 100.000000    
2024-06-06 18:40:38,922 - Epoch: [34][   30/   34]    Loss 0.000124    Top1 100.000000    Top5 100.000000    
2024-06-06 18:40:40,951 - Epoch: [34][   34/   34]    Loss 0.000116    Top1 100.000000    Top5 100.000000    
2024-06-06 18:40:41,908 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 18:40:41,908 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 18:40:42,131 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 34]
2024-06-06 18:40:42,132 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 18:40:42,148 - 

2024-06-06 18:40:42,149 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 18:41:12,347 - Epoch: [35][   10/  193]    Overall Loss 0.000043    Objective Loss 0.000043                                        LR 0.000500    Time 3.019794    
2024-06-06 18:41:22,545 - Epoch: [35][   20/  193]    Overall Loss 0.000205    Objective Loss 0.000205                                        LR 0.000500    Time 2.018641    
2024-06-06 18:41:32,575 - Epoch: [35][   30/  193]    Overall Loss 0.000168    Objective Loss 0.000168                                        LR 0.000500    Time 1.679176    
2024-06-06 18:41:42,767 - Epoch: [35][   40/  193]    Overall Loss 0.000170    Objective Loss 0.000170                                        LR 0.000500    Time 1.513439    
2024-06-06 18:41:52,851 - Epoch: [35][   50/  193]    Overall Loss 0.000161    Objective Loss 0.000161                                        LR 0.000500    Time 1.411825    
2024-06-06 18:42:03,079 - Epoch: [35][   60/  193]    Overall Loss 0.000145    Objective Loss 0.000145                                        LR 0.000500    Time 1.346389    
2024-06-06 18:42:13,301 - Epoch: [35][   70/  193]    Overall Loss 0.000150    Objective Loss 0.000150                                        LR 0.000500    Time 1.299657    
2024-06-06 18:42:23,823 - Epoch: [35][   80/  193]    Overall Loss 0.000145    Objective Loss 0.000145                                        LR 0.000500    Time 1.268417    
2024-06-06 18:42:33,950 - Epoch: [35][   90/  193]    Overall Loss 0.000136    Objective Loss 0.000136                                        LR 0.000500    Time 1.239636    
2024-06-06 18:42:44,120 - Epoch: [35][  100/  193]    Overall Loss 0.000127    Objective Loss 0.000127                                        LR 0.000500    Time 1.217040    
2024-06-06 18:42:54,518 - Epoch: [35][  110/  193]    Overall Loss 0.000121    Objective Loss 0.000121                                        LR 0.000500    Time 1.200676    
2024-06-06 18:43:05,082 - Epoch: [35][  120/  193]    Overall Loss 0.000128    Objective Loss 0.000128                                        LR 0.000500    Time 1.188324    
2024-06-06 18:43:15,259 - Epoch: [35][  130/  193]    Overall Loss 0.000122    Objective Loss 0.000122                                        LR 0.000500    Time 1.174972    
2024-06-06 18:43:25,301 - Epoch: [35][  140/  193]    Overall Loss 0.000117    Objective Loss 0.000117                                        LR 0.000500    Time 1.162606    
2024-06-06 18:43:35,456 - Epoch: [35][  150/  193]    Overall Loss 0.000116    Objective Loss 0.000116                                        LR 0.000500    Time 1.152568    
2024-06-06 18:43:45,492 - Epoch: [35][  160/  193]    Overall Loss 0.000115    Objective Loss 0.000115                                        LR 0.000500    Time 1.143046    
2024-06-06 18:43:55,529 - Epoch: [35][  170/  193]    Overall Loss 0.000111    Objective Loss 0.000111                                        LR 0.000500    Time 1.134624    
2024-06-06 19:18:23,578 - Epoch: [35][  180/  193]    Overall Loss 0.000109    Objective Loss 0.000109                                        LR 0.000500    Time 12.560606    
2024-06-06 19:18:50,290 - Epoch: [35][  190/  193]    Overall Loss 0.000108    Objective Loss 0.000108                                        LR 0.000500    Time 12.039910    
2024-06-06 19:18:53,010 - Epoch: [35][  193/  193]    Overall Loss 0.000107    Objective Loss 0.000107    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 11.866539    
2024-06-06 19:18:53,723 - --- validate (epoch=35)-----------
2024-06-06 19:18:53,723 - 2172 samples (64 per mini-batch)
2024-06-06 19:19:19,587 - Epoch: [35][   10/   34]    Loss 0.000034    Top1 100.000000    Top5 100.000000    
2024-06-06 19:19:25,124 - Epoch: [35][   20/   34]    Loss 0.000065    Top1 100.000000    Top5 100.000000    
2024-06-06 19:19:30,567 - Epoch: [35][   30/   34]    Loss 0.000065    Top1 100.000000    Top5 100.000000    
2024-06-06 19:19:32,554 - Epoch: [35][   34/   34]    Loss 0.000072    Top1 100.000000    Top5 100.000000    
2024-06-06 19:19:33,536 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 19:19:33,538 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 19:19:33,722 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 35]
2024-06-06 19:19:33,723 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 19:19:33,739 - 

2024-06-06 19:19:33,739 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 19:20:03,894 - Epoch: [36][   10/  193]    Overall Loss 0.000026    Objective Loss 0.000026                                        LR 0.000500    Time 3.015376    
2024-06-06 19:20:13,728 - Epoch: [36][   20/  193]    Overall Loss 0.000057    Objective Loss 0.000057                                        LR 0.000500    Time 1.998229    
2024-06-06 19:20:23,587 - Epoch: [36][   30/  193]    Overall Loss 0.000054    Objective Loss 0.000054                                        LR 0.000500    Time 1.659835    
2024-06-06 19:20:33,450 - Epoch: [36][   40/  193]    Overall Loss 0.000073    Objective Loss 0.000073                                        LR 0.000500    Time 1.490953    
2024-06-06 19:20:44,163 - Epoch: [36][   50/  193]    Overall Loss 0.000065    Objective Loss 0.000065                                        LR 0.000500    Time 1.406385    
2024-06-06 19:20:53,760 - Epoch: [36][   60/  193]    Overall Loss 0.000059    Objective Loss 0.000059                                        LR 0.000500    Time 1.331493    
2024-06-06 19:21:03,339 - Epoch: [36][   70/  193]    Overall Loss 0.000061    Objective Loss 0.000061                                        LR 0.000500    Time 1.277695    
2024-06-06 19:21:12,834 - Epoch: [36][   80/  193]    Overall Loss 0.000063    Objective Loss 0.000063                                        LR 0.000500    Time 1.236342    
2024-06-06 19:21:22,440 - Epoch: [36][   90/  193]    Overall Loss 0.000063    Objective Loss 0.000063                                        LR 0.000500    Time 1.205352    
2024-06-06 19:21:32,080 - Epoch: [36][  100/  193]    Overall Loss 0.000063    Objective Loss 0.000063                                        LR 0.000500    Time 1.180898    
2024-06-06 19:21:41,708 - Epoch: [36][  110/  193]    Overall Loss 0.000066    Objective Loss 0.000066                                        LR 0.000500    Time 1.160779    
2024-06-06 19:21:51,401 - Epoch: [36][  120/  193]    Overall Loss 0.000063    Objective Loss 0.000063                                        LR 0.000500    Time 1.144600    
2024-06-06 19:22:01,114 - Epoch: [36][  130/  193]    Overall Loss 0.000062    Objective Loss 0.000062                                        LR 0.000500    Time 1.131091    
2024-06-06 19:22:10,727 - Epoch: [36][  140/  193]    Overall Loss 0.000059    Objective Loss 0.000059                                        LR 0.000500    Time 1.118783    
2024-06-06 19:22:20,845 - Epoch: [36][  150/  193]    Overall Loss 0.000108    Objective Loss 0.000108                                        LR 0.000500    Time 1.111441    
2024-06-06 19:22:30,487 - Epoch: [36][  160/  193]    Overall Loss 0.000180    Objective Loss 0.000180                                        LR 0.000500    Time 1.102048    
2024-06-06 19:22:40,095 - Epoch: [36][  170/  193]    Overall Loss 0.000178    Objective Loss 0.000178                                        LR 0.000500    Time 1.093575    
2024-06-06 19:22:49,743 - Epoch: [36][  180/  193]    Overall Loss 0.000232    Objective Loss 0.000232                                        LR 0.000500    Time 1.086261    
2024-06-06 19:22:59,202 - Epoch: [36][  190/  193]    Overall Loss 0.000269    Objective Loss 0.000269                                        LR 0.000500    Time 1.078725    
2024-06-06 19:23:01,475 - Epoch: [36][  193/  193]    Overall Loss 0.000276    Objective Loss 0.000276    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 1.073580    
2024-06-06 19:23:02,124 - --- validate (epoch=36)-----------
2024-06-06 19:23:02,124 - 2172 samples (64 per mini-batch)
2024-06-06 19:23:27,582 - Epoch: [36][   10/   34]    Loss 0.001158    Top1 100.000000    Top5 100.000000    
2024-06-06 19:23:33,583 - Epoch: [36][   20/   34]    Loss 0.000934    Top1 100.000000    Top5 100.000000    
2024-06-06 19:23:39,449 - Epoch: [36][   30/   34]    Loss 0.000805    Top1 100.000000    Top5 100.000000    
2024-06-06 19:23:41,799 - Epoch: [36][   34/   34]    Loss 0.000741    Top1 100.000000    Top5 100.000000    
2024-06-06 19:23:42,876 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.001

2024-06-06 19:23:42,876 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 19:23:43,130 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 36]
2024-06-06 19:23:43,130 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 19:23:43,151 - 

2024-06-06 19:23:43,151 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 19:24:25,676 - Epoch: [37][   10/  193]    Overall Loss 0.000238    Objective Loss 0.000238                                        LR 0.000500    Time 4.252206    
2024-06-06 19:24:36,199 - Epoch: [37][   20/  193]    Overall Loss 0.000196    Objective Loss 0.000196                                        LR 0.000500    Time 2.650257    
2024-06-06 19:24:46,838 - Epoch: [37][   30/  193]    Overall Loss 0.000162    Objective Loss 0.000162                                        LR 0.000500    Time 2.120524    
2024-06-06 19:24:57,140 - Epoch: [37][   40/  193]    Overall Loss 0.000213    Objective Loss 0.000213                                        LR 0.000500    Time 1.847134    
2024-06-06 19:25:07,270 - Epoch: [37][   50/  193]    Overall Loss 0.000187    Objective Loss 0.000187                                        LR 0.000500    Time 1.679665    
2024-06-06 19:25:17,404 - Epoch: [37][   60/  193]    Overall Loss 0.000175    Objective Loss 0.000175                                        LR 0.000500    Time 1.568061    
2024-06-06 19:25:27,634 - Epoch: [37][   70/  193]    Overall Loss 0.000185    Objective Loss 0.000185                                        LR 0.000500    Time 1.489652    
2024-06-06 19:25:37,623 - Epoch: [37][   80/  193]    Overall Loss 0.000168    Objective Loss 0.000168                                        LR 0.000500    Time 1.428002    
2024-06-06 19:25:47,649 - Epoch: [37][   90/  193]    Overall Loss 0.000177    Objective Loss 0.000177                                        LR 0.000500    Time 1.380451    
2024-06-06 19:25:57,673 - Epoch: [37][  100/  193]    Overall Loss 0.000169    Objective Loss 0.000169                                        LR 0.000500    Time 1.342368    
2024-06-06 19:26:07,687 - Epoch: [37][  110/  193]    Overall Loss 0.000165    Objective Loss 0.000165                                        LR 0.000500    Time 1.311066    
2024-06-06 19:26:17,689 - Epoch: [37][  120/  193]    Overall Loss 0.000154    Objective Loss 0.000154                                        LR 0.000500    Time 1.284866    
2024-06-06 19:26:27,749 - Epoch: [37][  130/  193]    Overall Loss 0.000144    Objective Loss 0.000144                                        LR 0.000500    Time 1.263203    
2024-06-06 19:26:37,705 - Epoch: [37][  140/  193]    Overall Loss 0.000138    Objective Loss 0.000138                                        LR 0.000500    Time 1.243853    
2024-06-06 19:26:47,722 - Epoch: [37][  150/  193]    Overall Loss 0.000133    Objective Loss 0.000133                                        LR 0.000500    Time 1.227556    
2024-06-06 19:26:58,100 - Epoch: [37][  160/  193]    Overall Loss 0.000299    Objective Loss 0.000299                                        LR 0.000500    Time 1.215529    
2024-06-06 19:27:09,438 - Epoch: [37][  170/  193]    Overall Loss 0.000511    Objective Loss 0.000511                                        LR 0.000500    Time 1.210388    
2024-06-06 19:27:19,812 - Epoch: [37][  180/  193]    Overall Loss 0.001096    Objective Loss 0.001096                                        LR 0.000500    Time 1.200605    
2024-06-06 19:27:29,793 - Epoch: [37][  190/  193]    Overall Loss 0.001536    Objective Loss 0.001536                                        LR 0.000500    Time 1.189794    
2024-06-06 19:27:32,181 - Epoch: [37][  193/  193]    Overall Loss 0.001779    Objective Loss 0.001779    Top1 98.876404    Top5 100.000000    LR 0.000500    Time 1.183504    
2024-06-06 19:27:32,840 - --- validate (epoch=37)-----------
2024-06-06 19:27:32,841 - 2172 samples (64 per mini-batch)
2024-06-06 19:27:58,626 - Epoch: [37][   10/   34]    Loss 0.000259    Top1 100.000000    Top5 100.000000    
2024-06-06 19:28:04,324 - Epoch: [37][   20/   34]    Loss 0.000705    Top1 100.000000    Top5 100.000000    
2024-06-06 19:28:09,810 - Epoch: [37][   30/   34]    Loss 0.000680    Top1 100.000000    Top5 100.000000    
2024-06-06 19:28:11,831 - Epoch: [37][   34/   34]    Loss 0.000623    Top1 100.000000    Top5 100.000000    
2024-06-06 19:28:12,803 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.001

2024-06-06 19:28:12,803 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 19:28:13,005 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 37]
2024-06-06 19:28:13,006 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 19:28:13,023 - 

2024-06-06 19:28:13,023 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 19:28:44,069 - Epoch: [38][   10/  193]    Overall Loss 0.003181    Objective Loss 0.003181                                        LR 0.000500    Time 3.104486    
2024-06-06 19:28:54,610 - Epoch: [38][   20/  193]    Overall Loss 0.005083    Objective Loss 0.005083                                        LR 0.000500    Time 2.078193    
2024-06-06 19:29:05,006 - Epoch: [38][   30/  193]    Overall Loss 0.003709    Objective Loss 0.003709                                        LR 0.000500    Time 1.731180    
2024-06-06 19:29:15,553 - Epoch: [38][   40/  193]    Overall Loss 0.003443    Objective Loss 0.003443                                        LR 0.000500    Time 1.561411    
2024-06-06 19:29:26,152 - Epoch: [38][   50/  193]    Overall Loss 0.002804    Objective Loss 0.002804                                        LR 0.000500    Time 1.460531    
2024-06-06 19:29:36,769 - Epoch: [38][   60/  193]    Overall Loss 0.002417    Objective Loss 0.002417                                        LR 0.000500    Time 1.393449    
2024-06-06 19:29:47,307 - Epoch: [38][   70/  193]    Overall Loss 0.002314    Objective Loss 0.002314                                        LR 0.000500    Time 1.344542    
2024-06-06 19:29:58,066 - Epoch: [38][   80/  193]    Overall Loss 0.002117    Objective Loss 0.002117                                        LR 0.000500    Time 1.310531    
2024-06-06 19:30:09,096 - Epoch: [38][   90/  193]    Overall Loss 0.001939    Objective Loss 0.001939                                        LR 0.000500    Time 1.286871    
2024-06-06 19:30:19,493 - Epoch: [38][  100/  193]    Overall Loss 0.001750    Objective Loss 0.001750                                        LR 0.000500    Time 1.261872    
2024-06-06 19:30:30,051 - Epoch: [38][  110/  193]    Overall Loss 0.001594    Objective Loss 0.001594                                        LR 0.000500    Time 1.242870    
2024-06-06 19:30:40,490 - Epoch: [38][  120/  193]    Overall Loss 0.001480    Objective Loss 0.001480                                        LR 0.000500    Time 1.226034    
2024-06-06 19:30:51,126 - Epoch: [38][  130/  193]    Overall Loss 0.001374    Objective Loss 0.001374                                        LR 0.000500    Time 1.213328    
2024-06-06 19:31:01,741 - Epoch: [38][  140/  193]    Overall Loss 0.001282    Objective Loss 0.001282                                        LR 0.000500    Time 1.202114    
2024-06-06 19:31:12,221 - Epoch: [38][  150/  193]    Overall Loss 0.001201    Objective Loss 0.001201                                        LR 0.000500    Time 1.191587    
2024-06-06 19:31:22,755 - Epoch: [38][  160/  193]    Overall Loss 0.001132    Objective Loss 0.001132                                        LR 0.000500    Time 1.182695    
2024-06-06 19:31:33,211 - Epoch: [38][  170/  193]    Overall Loss 0.001068    Objective Loss 0.001068                                        LR 0.000500    Time 1.174426    
2024-06-06 19:31:43,616 - Epoch: [38][  180/  193]    Overall Loss 0.001011    Objective Loss 0.001011                                        LR 0.000500    Time 1.166733    
2024-06-06 19:31:53,851 - Epoch: [38][  190/  193]    Overall Loss 0.000960    Objective Loss 0.000960                                        LR 0.000500    Time 1.159024    
2024-06-06 19:31:56,236 - Epoch: [38][  193/  193]    Overall Loss 0.000958    Objective Loss 0.000958    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 1.153203    
2024-06-06 19:31:56,922 - --- validate (epoch=38)-----------
2024-06-06 19:31:56,923 - 2172 samples (64 per mini-batch)
2024-06-06 19:32:24,243 - Epoch: [38][   10/   34]    Loss 0.000117    Top1 100.000000    Top5 100.000000    
2024-06-06 19:32:30,249 - Epoch: [38][   20/   34]    Loss 0.000185    Top1 100.000000    Top5 100.000000    
2024-06-06 19:32:35,845 - Epoch: [38][   30/   34]    Loss 0.000142    Top1 100.000000    Top5 100.000000    
2024-06-06 19:32:37,932 - Epoch: [38][   34/   34]    Loss 0.000129    Top1 100.000000    Top5 100.000000    
2024-06-06 19:32:38,918 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 19:32:38,919 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 19:32:39,133 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 38]
2024-06-06 19:32:39,133 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 19:32:39,150 - 

2024-06-06 19:32:39,150 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 19:33:10,291 - Epoch: [39][   10/  193]    Overall Loss 0.001118    Objective Loss 0.001118                                        LR 0.000500    Time 3.114022    
2024-06-06 19:33:20,849 - Epoch: [39][   20/  193]    Overall Loss 0.000704    Objective Loss 0.000704                                        LR 0.000500    Time 2.083248    
2024-06-06 19:33:31,600 - Epoch: [39][   30/  193]    Overall Loss 0.000541    Objective Loss 0.000541                                        LR 0.000500    Time 1.746262    
2024-06-06 19:33:42,210 - Epoch: [39][   40/  193]    Overall Loss 0.000527    Objective Loss 0.000527                                        LR 0.000500    Time 1.574184    
2024-06-06 19:33:53,433 - Epoch: [39][   50/  193]    Overall Loss 0.000428    Objective Loss 0.000428                                        LR 0.000500    Time 1.483223    
2024-06-06 19:34:04,179 - Epoch: [39][   60/  193]    Overall Loss 0.000365    Objective Loss 0.000365                                        LR 0.000500    Time 1.414445    
2024-06-06 19:34:14,719 - Epoch: [39][   70/  193]    Overall Loss 0.000326    Objective Loss 0.000326                                        LR 0.000500    Time 1.362557    
2024-06-06 19:34:25,169 - Epoch: [39][   80/  193]    Overall Loss 0.000292    Objective Loss 0.000292                                        LR 0.000500    Time 1.322358    
2024-06-06 19:34:35,833 - Epoch: [39][   90/  193]    Overall Loss 0.000269    Objective Loss 0.000269                                        LR 0.000500    Time 1.293584    
2024-06-06 19:34:46,377 - Epoch: [39][  100/  193]    Overall Loss 0.000245    Objective Loss 0.000245                                        LR 0.000500    Time 1.269399    
2024-06-06 19:34:56,960 - Epoch: [39][  110/  193]    Overall Loss 0.000234    Objective Loss 0.000234                                        LR 0.000500    Time 1.249980    
2024-06-06 19:35:07,978 - Epoch: [39][  120/  193]    Overall Loss 0.000219    Objective Loss 0.000219                                        LR 0.000500    Time 1.237326    
2024-06-06 19:35:18,572 - Epoch: [39][  130/  193]    Overall Loss 0.000208    Objective Loss 0.000208                                        LR 0.000500    Time 1.223379    
2024-06-06 19:35:29,315 - Epoch: [39][  140/  193]    Overall Loss 0.000196    Objective Loss 0.000196                                        LR 0.000500    Time 1.212477    
2024-06-06 19:35:40,104 - Epoch: [39][  150/  193]    Overall Loss 0.000187    Objective Loss 0.000187                                        LR 0.000500    Time 1.203395    
2024-06-06 19:35:51,043 - Epoch: [39][  160/  193]    Overall Loss 0.000188    Objective Loss 0.000188                                        LR 0.000500    Time 1.196333    
2024-06-06 19:36:02,009 - Epoch: [39][  170/  193]    Overall Loss 0.000180    Objective Loss 0.000180                                        LR 0.000500    Time 1.190282    
2024-06-06 19:36:13,142 - Epoch: [39][  180/  193]    Overall Loss 0.000172    Objective Loss 0.000172                                        LR 0.000500    Time 1.185830    
2024-06-06 19:36:23,818 - Epoch: [39][  190/  193]    Overall Loss 0.000171    Objective Loss 0.000171                                        LR 0.000500    Time 1.179408    
2024-06-06 19:36:26,436 - Epoch: [39][  193/  193]    Overall Loss 0.000169    Objective Loss 0.000169    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 1.174491    
2024-06-06 19:36:27,138 - --- validate (epoch=39)-----------
2024-06-06 19:36:27,138 - 2172 samples (64 per mini-batch)
2024-06-06 19:36:53,906 - Epoch: [39][   10/   34]    Loss 0.000069    Top1 100.000000    Top5 100.000000    
2024-06-06 19:36:59,961 - Epoch: [39][   20/   34]    Loss 0.000062    Top1 100.000000    Top5 100.000000    
2024-06-06 19:37:05,805 - Epoch: [39][   30/   34]    Loss 0.000049    Top1 100.000000    Top5 100.000000    
2024-06-06 19:37:07,936 - Epoch: [39][   34/   34]    Loss 0.000054    Top1 100.000000    Top5 100.000000    
2024-06-06 19:37:08,961 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 19:37:08,962 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 19:37:09,180 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 39]
2024-06-06 19:37:09,180 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 19:37:09,197 - 

2024-06-06 19:37:09,197 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 19:37:41,173 - Epoch: [40][   10/  193]    Overall Loss 0.000066    Objective Loss 0.000066                                        LR 0.000500    Time 3.197555    
2024-06-06 19:37:52,014 - Epoch: [40][   20/  193]    Overall Loss 0.000078    Objective Loss 0.000078                                        LR 0.000500    Time 2.139222    
2024-06-06 19:38:02,734 - Epoch: [40][   30/  193]    Overall Loss 0.000064    Objective Loss 0.000064                                        LR 0.000500    Time 1.782166    
2024-06-06 19:38:13,469 - Epoch: [40][   40/  193]    Overall Loss 0.000071    Objective Loss 0.000071                                        LR 0.000500    Time 1.604131    
2024-06-06 19:38:23,975 - Epoch: [40][   50/  193]    Overall Loss 0.000068    Objective Loss 0.000068                                        LR 0.000500    Time 1.492570    
2024-06-06 19:38:34,956 - Epoch: [40][   60/  193]    Overall Loss 0.000069    Objective Loss 0.000069                                        LR 0.000500    Time 1.426253    
2024-06-06 19:38:45,605 - Epoch: [40][   70/  193]    Overall Loss 0.000081    Objective Loss 0.000081                                        LR 0.000500    Time 1.374184    
2024-06-06 19:38:56,106 - Epoch: [40][   80/  193]    Overall Loss 0.000079    Objective Loss 0.000079                                        LR 0.000500    Time 1.333282    
2024-06-06 19:39:06,553 - Epoch: [40][   90/  193]    Overall Loss 0.000076    Objective Loss 0.000076                                        LR 0.000500    Time 1.300831    
2024-06-06 19:39:16,918 - Epoch: [40][  100/  193]    Overall Loss 0.000074    Objective Loss 0.000074                                        LR 0.000500    Time 1.274087    
2024-06-06 19:39:27,485 - Epoch: [40][  110/  193]    Overall Loss 0.000070    Objective Loss 0.000070                                        LR 0.000500    Time 1.254088    
2024-06-06 19:39:38,191 - Epoch: [40][  120/  193]    Overall Loss 0.000068    Objective Loss 0.000068                                        LR 0.000500    Time 1.238567    
2024-06-06 19:39:48,736 - Epoch: [40][  130/  193]    Overall Loss 0.000065    Objective Loss 0.000065                                        LR 0.000500    Time 1.224199    
2024-06-06 19:39:59,303 - Epoch: [40][  140/  193]    Overall Loss 0.000062    Objective Loss 0.000062                                        LR 0.000500    Time 1.212082    
2024-06-06 19:40:09,991 - Epoch: [40][  150/  193]    Overall Loss 0.000059    Objective Loss 0.000059                                        LR 0.000500    Time 1.202301    
2024-06-06 19:40:20,823 - Epoch: [40][  160/  193]    Overall Loss 0.000058    Objective Loss 0.000058                                        LR 0.000500    Time 1.194579    
2024-06-06 19:40:31,783 - Epoch: [40][  170/  193]    Overall Loss 0.000060    Objective Loss 0.000060                                        LR 0.000500    Time 1.188559    
2024-06-06 19:40:42,721 - Epoch: [40][  180/  193]    Overall Loss 0.000063    Objective Loss 0.000063                                        LR 0.000500    Time 1.183161    
2024-06-06 19:40:53,050 - Epoch: [40][  190/  193]    Overall Loss 0.000061    Objective Loss 0.000061                                        LR 0.000500    Time 1.175109    
2024-06-06 19:40:55,598 - Epoch: [40][  193/  193]    Overall Loss 0.000060    Objective Loss 0.000060    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 1.169839    
2024-06-06 19:40:56,271 - --- validate (epoch=40)-----------
2024-06-06 19:40:56,271 - 2172 samples (64 per mini-batch)
2024-06-06 19:41:31,289 - Epoch: [40][   10/   34]    Loss 0.000035    Top1 100.000000    Top5 100.000000    
2024-06-06 19:41:37,239 - Epoch: [40][   20/   34]    Loss 0.000130    Top1 100.000000    Top5 100.000000    
2024-06-06 19:41:42,930 - Epoch: [40][   30/   34]    Loss 0.000095    Top1 100.000000    Top5 100.000000    
2024-06-06 19:41:45,046 - Epoch: [40][   34/   34]    Loss 0.000089    Top1 100.000000    Top5 100.000000    
2024-06-06 19:41:46,076 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 19:41:46,077 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 19:41:46,284 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 40]
2024-06-06 19:41:46,284 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 19:41:46,301 - 

2024-06-06 19:41:46,302 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 19:42:18,436 - Epoch: [41][   10/  193]    Overall Loss 0.000050    Objective Loss 0.000050                                        LR 0.000500    Time 3.213402    
2024-06-06 19:42:29,455 - Epoch: [41][   20/  193]    Overall Loss 0.000042    Objective Loss 0.000042                                        LR 0.000500    Time 2.155762    
2024-06-06 19:42:40,177 - Epoch: [41][   30/  193]    Overall Loss 0.000045    Objective Loss 0.000045                                        LR 0.000500    Time 1.793178    
2024-06-06 19:42:51,083 - Epoch: [41][   40/  193]    Overall Loss 0.000056    Objective Loss 0.000056                                        LR 0.000500    Time 1.616501    
2024-06-06 19:43:01,933 - Epoch: [41][   50/  193]    Overall Loss 0.000049    Objective Loss 0.000049                                        LR 0.000500    Time 1.509452    
2024-06-06 19:43:12,888 - Epoch: [41][   60/  193]    Overall Loss 0.000045    Objective Loss 0.000045                                        LR 0.000500    Time 1.439962    
2024-06-06 19:43:23,922 - Epoch: [41][   70/  193]    Overall Loss 0.000045    Objective Loss 0.000045                                        LR 0.000500    Time 1.391304    
2024-06-06 19:43:35,286 - Epoch: [41][   80/  193]    Overall Loss 0.000044    Objective Loss 0.000044                                        LR 0.000500    Time 1.358953    
2024-06-06 19:43:46,427 - Epoch: [41][   90/  193]    Overall Loss 0.000042    Objective Loss 0.000042                                        LR 0.000500    Time 1.331268    
2024-06-06 19:43:57,518 - Epoch: [41][  100/  193]    Overall Loss 0.000040    Objective Loss 0.000040                                        LR 0.000500    Time 1.308685    
2024-06-06 19:44:08,582 - Epoch: [41][  110/  193]    Overall Loss 0.000038    Objective Loss 0.000038                                        LR 0.000500    Time 1.289936    
2024-06-06 19:44:19,566 - Epoch: [41][  120/  193]    Overall Loss 0.000037    Objective Loss 0.000037                                        LR 0.000500    Time 1.273613    
2024-06-06 19:44:30,543 - Epoch: [41][  130/  193]    Overall Loss 0.000035    Objective Loss 0.000035                                        LR 0.000500    Time 1.259789    
2024-06-06 19:44:41,276 - Epoch: [41][  140/  193]    Overall Loss 0.000054    Objective Loss 0.000054                                        LR 0.000500    Time 1.246172    
2024-06-06 19:44:52,170 - Epoch: [41][  150/  193]    Overall Loss 0.000056    Objective Loss 0.000056                                        LR 0.000500    Time 1.235482    
2024-06-06 19:45:03,037 - Epoch: [41][  160/  193]    Overall Loss 0.000056    Objective Loss 0.000056                                        LR 0.000500    Time 1.226008    
2024-06-06 19:45:13,693 - Epoch: [41][  170/  193]    Overall Loss 0.000054    Objective Loss 0.000054                                        LR 0.000500    Time 1.216312    
2024-06-06 19:45:24,447 - Epoch: [41][  180/  193]    Overall Loss 0.000052    Objective Loss 0.000052                                        LR 0.000500    Time 1.208308    
2024-06-06 19:45:35,135 - Epoch: [41][  190/  193]    Overall Loss 0.000052    Objective Loss 0.000052                                        LR 0.000500    Time 1.200812    
2024-06-06 19:45:37,641 - Epoch: [41][  193/  193]    Overall Loss 0.000052    Objective Loss 0.000052    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 1.194978    
2024-06-06 19:45:38,341 - --- validate (epoch=41)-----------
2024-06-06 19:45:38,341 - 2172 samples (64 per mini-batch)
2024-06-06 19:46:06,686 - Epoch: [41][   10/   34]    Loss 0.000032    Top1 100.000000    Top5 100.000000    
2024-06-06 19:46:12,931 - Epoch: [41][   20/   34]    Loss 0.000317    Top1 100.000000    Top5 100.000000    
2024-06-06 19:46:18,776 - Epoch: [41][   30/   34]    Loss 0.000219    Top1 100.000000    Top5 100.000000    
2024-06-06 19:46:20,896 - Epoch: [41][   34/   34]    Loss 0.000197    Top1 100.000000    Top5 100.000000    
2024-06-06 19:46:21,892 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 19:46:21,892 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 19:46:22,115 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 41]
2024-06-06 19:46:22,115 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 19:46:22,139 - 

2024-06-06 19:46:22,139 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 19:46:54,710 - Epoch: [42][   10/  193]    Overall Loss 0.000060    Objective Loss 0.000060                                        LR 0.000500    Time 3.257107    
2024-06-06 19:47:05,918 - Epoch: [42][   20/  193]    Overall Loss 0.000048    Objective Loss 0.000048                                        LR 0.000500    Time 2.186372    
2024-06-06 19:47:17,024 - Epoch: [42][   30/  193]    Overall Loss 0.000040    Objective Loss 0.000040                                        LR 0.000500    Time 1.826634    
2024-06-06 19:47:28,286 - Epoch: [42][   40/  193]    Overall Loss 0.000036    Objective Loss 0.000036                                        LR 0.000500    Time 1.650664    
2024-06-06 19:47:39,613 - Epoch: [42][   50/  193]    Overall Loss 0.000032    Objective Loss 0.000032                                        LR 0.000500    Time 1.546337    
2024-06-06 19:47:50,798 - Epoch: [42][   60/  193]    Overall Loss 0.000029    Objective Loss 0.000029                                        LR 0.000500    Time 1.474545    
2024-06-06 19:48:02,191 - Epoch: [42][   70/  193]    Overall Loss 0.000031    Objective Loss 0.000031                                        LR 0.000500    Time 1.426200    
2024-06-06 19:48:13,624 - Epoch: [42][   80/  193]    Overall Loss 0.000030    Objective Loss 0.000030                                        LR 0.000500    Time 1.390401    
2024-06-06 19:48:24,813 - Epoch: [42][   90/  193]    Overall Loss 0.000033    Objective Loss 0.000033                                        LR 0.000500    Time 1.359846    
2024-06-06 19:48:36,642 - Epoch: [42][  100/  193]    Overall Loss 0.000032    Objective Loss 0.000032                                        LR 0.000500    Time 1.341788    
2024-06-06 19:48:47,999 - Epoch: [42][  110/  193]    Overall Loss 0.000032    Objective Loss 0.000032                                        LR 0.000500    Time 1.322764    
2024-06-06 19:48:59,174 - Epoch: [42][  120/  193]    Overall Loss 0.000032    Objective Loss 0.000032                                        LR 0.000500    Time 1.305336    
2024-06-06 19:49:10,288 - Epoch: [42][  130/  193]    Overall Loss 0.000031    Objective Loss 0.000031                                        LR 0.000500    Time 1.290205    
2024-06-06 19:49:21,324 - Epoch: [42][  140/  193]    Overall Loss 0.000031    Objective Loss 0.000031                                        LR 0.000500    Time 1.276655    
2024-06-06 19:49:32,509 - Epoch: [42][  150/  193]    Overall Loss 0.000031    Objective Loss 0.000031                                        LR 0.000500    Time 1.265843    
2024-06-06 19:49:43,870 - Epoch: [42][  160/  193]    Overall Loss 0.000031    Objective Loss 0.000031                                        LR 0.000500    Time 1.257520    
2024-06-06 19:49:54,730 - Epoch: [42][  170/  193]    Overall Loss 0.000031    Objective Loss 0.000031                                        LR 0.000500    Time 1.247225    
2024-06-06 19:50:05,694 - Epoch: [42][  180/  193]    Overall Loss 0.000033    Objective Loss 0.000033                                        LR 0.000500    Time 1.238655    
2024-06-06 19:50:16,285 - Epoch: [42][  190/  193]    Overall Loss 0.000033    Objective Loss 0.000033                                        LR 0.000500    Time 1.229020    
2024-06-06 19:50:18,807 - Epoch: [42][  193/  193]    Overall Loss 0.000033    Objective Loss 0.000033    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 1.222785    
2024-06-06 19:50:19,485 - --- validate (epoch=42)-----------
2024-06-06 19:50:19,485 - 2172 samples (64 per mini-batch)
2024-06-06 19:50:46,708 - Epoch: [42][   10/   34]    Loss 0.000034    Top1 100.000000    Top5 100.000000    
2024-06-06 19:50:52,697 - Epoch: [42][   20/   34]    Loss 0.000045    Top1 100.000000    Top5 100.000000    
2024-06-06 19:50:58,453 - Epoch: [42][   30/   34]    Loss 0.000036    Top1 100.000000    Top5 100.000000    
2024-06-06 19:51:00,596 - Epoch: [42][   34/   34]    Loss 0.000034    Top1 100.000000    Top5 100.000000    
2024-06-06 19:51:01,584 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 19:51:01,584 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 19:51:01,810 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 42]
2024-06-06 19:51:01,810 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 19:51:01,830 - 

2024-06-06 19:51:01,830 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 19:51:33,114 - Epoch: [43][   10/  193]    Overall Loss 0.000015    Objective Loss 0.000015                                        LR 0.000500    Time 3.128460    
2024-06-06 19:51:43,903 - Epoch: [43][   20/  193]    Overall Loss 0.000065    Objective Loss 0.000065                                        LR 0.000500    Time 2.102151    
2024-06-06 19:51:54,334 - Epoch: [43][   30/  193]    Overall Loss 0.000051    Objective Loss 0.000051                                        LR 0.000500    Time 1.747894    
2024-06-06 19:52:04,980 - Epoch: [43][   40/  193]    Overall Loss 0.000044    Objective Loss 0.000044                                        LR 0.000500    Time 1.576242    
2024-06-06 19:52:17,060 - Epoch: [43][   50/  193]    Overall Loss 0.000039    Objective Loss 0.000039                                        LR 0.000500    Time 1.501905    
2024-06-06 19:52:27,443 - Epoch: [43][   60/  193]    Overall Loss 0.000039    Objective Loss 0.000039                                        LR 0.000500    Time 1.423990    
2024-06-06 19:52:38,015 - Epoch: [43][   70/  193]    Overall Loss 0.000037    Objective Loss 0.000037                                        LR 0.000500    Time 1.371073    
2024-06-06 19:52:48,375 - Epoch: [43][   80/  193]    Overall Loss 0.000039    Objective Loss 0.000039                                        LR 0.000500    Time 1.328821    
2024-06-06 19:52:58,743 - Epoch: [43][   90/  193]    Overall Loss 0.000043    Objective Loss 0.000043                                        LR 0.000500    Time 1.295900    
2024-06-06 19:53:09,232 - Epoch: [43][  100/  193]    Overall Loss 0.000042    Objective Loss 0.000042                                        LR 0.000500    Time 1.270855    
2024-06-06 19:53:19,694 - Epoch: [43][  110/  193]    Overall Loss 0.000039    Objective Loss 0.000039                                        LR 0.000500    Time 1.250173    
2024-06-06 19:53:30,246 - Epoch: [43][  120/  193]    Overall Loss 0.000037    Objective Loss 0.000037                                        LR 0.000500    Time 1.233664    
2024-06-06 19:53:41,521 - Epoch: [43][  130/  193]    Overall Loss 0.000042    Objective Loss 0.000042                                        LR 0.000500    Time 1.225298    
2024-06-06 19:53:52,197 - Epoch: [43][  140/  193]    Overall Loss 0.000049    Objective Loss 0.000049                                        LR 0.000500    Time 1.213807    
2024-06-06 19:54:02,864 - Epoch: [43][  150/  193]    Overall Loss 0.000049    Objective Loss 0.000049                                        LR 0.000500    Time 1.203771    
2024-06-06 19:54:13,543 - Epoch: [43][  160/  193]    Overall Loss 0.000050    Objective Loss 0.000050                                        LR 0.000500    Time 1.195048    
2024-06-06 19:54:24,159 - Epoch: [43][  170/  193]    Overall Loss 0.000048    Objective Loss 0.000048                                        LR 0.000500    Time 1.186988    
2024-06-06 19:54:34,938 - Epoch: [43][  180/  193]    Overall Loss 0.000048    Objective Loss 0.000048                                        LR 0.000500    Time 1.180763    
2024-06-06 19:54:45,628 - Epoch: [43][  190/  193]    Overall Loss 0.000047    Objective Loss 0.000047                                        LR 0.000500    Time 1.174667    
2024-06-06 19:54:48,157 - Epoch: [43][  193/  193]    Overall Loss 0.000046    Objective Loss 0.000046    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 1.169317    
2024-06-06 19:54:48,824 - --- validate (epoch=43)-----------
2024-06-06 19:54:48,824 - 2172 samples (64 per mini-batch)
2024-06-06 19:55:23,975 - Epoch: [43][   10/   34]    Loss 0.000055    Top1 100.000000    Top5 100.000000    
2024-06-06 19:55:29,932 - Epoch: [43][   20/   34]    Loss 0.000047    Top1 100.000000    Top5 100.000000    
2024-06-06 19:55:35,612 - Epoch: [43][   30/   34]    Loss 0.000039    Top1 100.000000    Top5 100.000000    
2024-06-06 19:55:37,789 - Epoch: [43][   34/   34]    Loss 0.000041    Top1 100.000000    Top5 100.000000    
2024-06-06 19:55:38,772 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 19:55:38,772 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 19:55:38,986 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 43]
2024-06-06 19:55:38,986 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 19:55:39,004 - 

2024-06-06 19:55:39,004 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 19:56:09,995 - Epoch: [44][   10/  193]    Overall Loss 0.000038    Objective Loss 0.000038                                        LR 0.000500    Time 3.099074    
2024-06-06 19:56:20,402 - Epoch: [44][   20/  193]    Overall Loss 0.000199    Objective Loss 0.000199                                        LR 0.000500    Time 2.068407    
2024-06-06 19:56:30,817 - Epoch: [44][   30/  193]    Overall Loss 0.000185    Objective Loss 0.000185                                        LR 0.000500    Time 1.724927    
2024-06-06 19:56:41,400 - Epoch: [44][   40/  193]    Overall Loss 0.000155    Objective Loss 0.000155                                        LR 0.000500    Time 1.557472    
2024-06-06 19:56:51,797 - Epoch: [44][   50/  193]    Overall Loss 0.000128    Objective Loss 0.000128                                        LR 0.000500    Time 1.453341    
2024-06-06 19:57:02,666 - Epoch: [44][   60/  193]    Overall Loss 0.000113    Objective Loss 0.000113                                        LR 0.000500    Time 1.391816    
2024-06-06 19:57:13,407 - Epoch: [44][   70/  193]    Overall Loss 0.000106    Objective Loss 0.000106                                        LR 0.000500    Time 1.345971    
2024-06-06 19:57:23,915 - Epoch: [44][   80/  193]    Overall Loss 0.000095    Objective Loss 0.000095                                        LR 0.000500    Time 1.308788    
2024-06-06 19:57:34,522 - Epoch: [44][   90/  193]    Overall Loss 0.000104    Objective Loss 0.000104                                        LR 0.000500    Time 1.280820    
2024-06-06 19:57:45,294 - Epoch: [44][  100/  193]    Overall Loss 0.000099    Objective Loss 0.000099                                        LR 0.000500    Time 1.260096    
2024-06-06 19:57:56,020 - Epoch: [44][  110/  193]    Overall Loss 0.000092    Objective Loss 0.000092                                        LR 0.000500    Time 1.242742    
2024-06-06 19:58:06,872 - Epoch: [44][  120/  193]    Overall Loss 0.000088    Objective Loss 0.000088                                        LR 0.000500    Time 1.229385    
2024-06-06 19:58:17,775 - Epoch: [44][  130/  193]    Overall Loss 0.000082    Objective Loss 0.000082                                        LR 0.000500    Time 1.218412    
2024-06-06 19:58:29,361 - Epoch: [44][  140/  193]    Overall Loss 0.000085    Objective Loss 0.000085                                        LR 0.000500    Time 1.213805    
2024-06-06 19:58:40,588 - Epoch: [44][  150/  193]    Overall Loss 0.000081    Objective Loss 0.000081                                        LR 0.000500    Time 1.207440    
2024-06-06 19:58:51,670 - Epoch: [44][  160/  193]    Overall Loss 0.000080    Objective Loss 0.000080                                        LR 0.000500    Time 1.200989    
2024-06-06 19:59:02,535 - Epoch: [44][  170/  193]    Overall Loss 0.000077    Objective Loss 0.000077                                        LR 0.000500    Time 1.194028    
2024-06-06 19:59:13,376 - Epoch: [44][  180/  193]    Overall Loss 0.000093    Objective Loss 0.000093                                        LR 0.000500    Time 1.187784    
2024-06-06 19:59:23,839 - Epoch: [44][  190/  193]    Overall Loss 0.000139    Objective Loss 0.000139                                        LR 0.000500    Time 1.180120    
2024-06-06 19:59:26,335 - Epoch: [44][  193/  193]    Overall Loss 0.000137    Objective Loss 0.000137    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 1.174515    
2024-06-06 19:59:27,011 - --- validate (epoch=44)-----------
2024-06-06 19:59:27,011 - 2172 samples (64 per mini-batch)
2024-06-06 19:59:54,450 - Epoch: [44][   10/   34]    Loss 0.000029    Top1 100.000000    Top5 100.000000    
2024-06-06 20:00:00,615 - Epoch: [44][   20/   34]    Loss 0.000718    Top1 100.000000    Top5 100.000000    
2024-06-06 20:00:06,391 - Epoch: [44][   30/   34]    Loss 0.000495    Top1 100.000000    Top5 100.000000    
2024-06-06 20:00:08,538 - Epoch: [44][   34/   34]    Loss 0.000442    Top1 100.000000    Top5 100.000000    
2024-06-06 20:00:09,506 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 20:00:09,507 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 20:00:09,726 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 44]
2024-06-06 20:00:09,727 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 20:00:09,744 - 

2024-06-06 20:00:09,745 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 20:00:42,492 - Epoch: [45][   10/  193]    Overall Loss 0.000089    Objective Loss 0.000089                                        LR 0.000500    Time 3.274757    
2024-06-06 20:00:53,571 - Epoch: [45][   20/  193]    Overall Loss 0.000131    Objective Loss 0.000131                                        LR 0.000500    Time 2.189647    
2024-06-06 20:01:04,735 - Epoch: [45][   30/  193]    Overall Loss 0.000104    Objective Loss 0.000104                                        LR 0.000500    Time 1.830461    
2024-06-06 20:01:15,801 - Epoch: [45][   40/  193]    Overall Loss 0.000086    Objective Loss 0.000086                                        LR 0.000500    Time 1.648608    
2024-06-06 20:01:27,279 - Epoch: [45][   50/  193]    Overall Loss 0.000083    Objective Loss 0.000083                                        LR 0.000500    Time 1.547358    
2024-06-06 20:01:37,914 - Epoch: [45][   60/  193]    Overall Loss 0.000083    Objective Loss 0.000083                                        LR 0.000500    Time 1.466089    
2024-06-06 20:01:48,753 - Epoch: [45][   70/  193]    Overall Loss 0.000075    Objective Loss 0.000075                                        LR 0.000500    Time 1.411095    
2024-06-06 20:01:59,418 - Epoch: [45][   80/  193]    Overall Loss 0.000071    Objective Loss 0.000071                                        LR 0.000500    Time 1.367555    
2024-06-06 20:02:10,207 - Epoch: [45][   90/  193]    Overall Loss 0.000068    Objective Loss 0.000068                                        LR 0.000500    Time 1.335096    
2024-06-06 20:02:21,233 - Epoch: [45][  100/  193]    Overall Loss 0.000077    Objective Loss 0.000077                                        LR 0.000500    Time 1.311586    
2024-06-06 20:02:31,805 - Epoch: [45][  110/  193]    Overall Loss 0.000076    Objective Loss 0.000076                                        LR 0.000500    Time 1.288167    
2024-06-06 20:02:42,498 - Epoch: [45][  120/  193]    Overall Loss 0.000071    Objective Loss 0.000071                                        LR 0.000500    Time 1.269686    
2024-06-06 20:02:53,387 - Epoch: [45][  130/  193]    Overall Loss 0.000068    Objective Loss 0.000068                                        LR 0.000500    Time 1.255532    
2024-06-06 20:36:51,976 - Epoch: [45][  140/  193]    Overall Loss 0.000065    Objective Loss 0.000065                                        LR 0.000500    Time 15.726909    
2024-06-06 20:37:03,312 - Epoch: [45][  150/  193]    Overall Loss 0.000064    Objective Loss 0.000064                                        LR 0.000500    Time 14.753612    
2024-06-06 20:37:13,953 - Epoch: [45][  160/  193]    Overall Loss 0.000062    Objective Loss 0.000062                                        LR 0.000500    Time 13.897755    
2024-06-06 20:37:24,389 - Epoch: [45][  170/  193]    Overall Loss 0.000060    Objective Loss 0.000060                                        LR 0.000500    Time 13.141429    
2024-06-06 20:37:35,578 - Epoch: [45][  180/  193]    Overall Loss 0.000065    Objective Loss 0.000065                                        LR 0.000500    Time 12.473324    
2024-06-06 20:37:46,218 - Epoch: [45][  190/  193]    Overall Loss 0.000065    Objective Loss 0.000065                                        LR 0.000500    Time 11.872668    
2024-06-06 20:37:48,729 - Epoch: [45][  193/  193]    Overall Loss 0.000064    Objective Loss 0.000064    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 11.700900    
2024-06-06 20:37:49,404 - --- validate (epoch=45)-----------
2024-06-06 20:37:49,404 - 2172 samples (64 per mini-batch)
2024-06-06 20:38:18,172 - Epoch: [45][   10/   34]    Loss 0.000075    Top1 100.000000    Top5 100.000000    
2024-06-06 20:38:23,900 - Epoch: [45][   20/   34]    Loss 0.001407    Top1 99.921875    Top5 100.000000    
2024-06-06 20:38:29,373 - Epoch: [45][   30/   34]    Loss 0.000958    Top1 99.947917    Top5 100.000000    
2024-06-06 20:38:31,488 - Epoch: [45][   34/   34]    Loss 0.000848    Top1 99.953959    Top5 100.000000    
2024-06-06 20:38:32,458 - ==> Top1: 99.954    Top5: 100.000    Loss: 0.001

2024-06-06 20:38:32,459 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 227   0   0   0   1   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 20:38:32,694 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 44]
2024-06-06 20:38:32,695 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 20:38:32,701 - 

2024-06-06 20:38:32,701 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 20:39:02,628 - Epoch: [46][   10/  193]    Overall Loss 0.000039    Objective Loss 0.000039                                        LR 0.000500    Time 2.992759    
2024-06-06 20:39:12,193 - Epoch: [46][   20/  193]    Overall Loss 0.000043    Objective Loss 0.000043                                        LR 0.000500    Time 1.973162    
2024-06-06 20:39:21,846 - Epoch: [46][   30/  193]    Overall Loss 0.000037    Objective Loss 0.000037                                        LR 0.000500    Time 1.636211    
2024-06-06 20:39:31,629 - Epoch: [46][   40/  193]    Overall Loss 0.000040    Objective Loss 0.000040                                        LR 0.000500    Time 1.471026    
2024-06-06 20:39:41,290 - Epoch: [46][   50/  193]    Overall Loss 0.000034    Objective Loss 0.000034                                        LR 0.000500    Time 1.369499    
2024-06-06 20:39:50,960 - Epoch: [46][   60/  193]    Overall Loss 0.000032    Objective Loss 0.000032                                        LR 0.000500    Time 1.301868    
2024-06-06 20:40:00,702 - Epoch: [46][   70/  193]    Overall Loss 0.000072    Objective Loss 0.000072                                        LR 0.000500    Time 1.254760    
2024-06-06 20:40:10,309 - Epoch: [46][   80/  193]    Overall Loss 0.000073    Objective Loss 0.000073                                        LR 0.000500    Time 1.217714    
2024-06-06 20:40:20,000 - Epoch: [46][   90/  193]    Overall Loss 0.000086    Objective Loss 0.000086                                        LR 0.000500    Time 1.189725    
2024-06-06 20:40:29,712 - Epoch: [46][  100/  193]    Overall Loss 0.000083    Objective Loss 0.000083                                        LR 0.000500    Time 1.167476    
2024-06-06 20:40:39,384 - Epoch: [46][  110/  193]    Overall Loss 0.000079    Objective Loss 0.000079                                        LR 0.000500    Time 1.149041    
2024-06-06 20:40:49,848 - Epoch: [46][  120/  193]    Overall Loss 0.000077    Objective Loss 0.000077                                        LR 0.000500    Time 1.140259    
2024-06-06 20:41:00,429 - Epoch: [46][  130/  193]    Overall Loss 0.000074    Objective Loss 0.000074                                        LR 0.000500    Time 1.133628    
2024-06-06 20:41:10,831 - Epoch: [46][  140/  193]    Overall Loss 0.000073    Objective Loss 0.000073                                        LR 0.000500    Time 1.126689    
2024-06-06 20:41:21,382 - Epoch: [46][  150/  193]    Overall Loss 0.000106    Objective Loss 0.000106                                        LR 0.000500    Time 1.121729    
2024-06-06 20:41:32,524 - Epoch: [46][  160/  193]    Overall Loss 0.000137    Objective Loss 0.000137                                        LR 0.000500    Time 1.121026    
2024-06-06 20:41:43,540 - Epoch: [46][  170/  193]    Overall Loss 0.000195    Objective Loss 0.000195                                        LR 0.000500    Time 1.119740    
2024-06-06 20:41:53,963 - Epoch: [46][  180/  193]    Overall Loss 0.000253    Objective Loss 0.000253                                        LR 0.000500    Time 1.115256    
2024-06-06 20:42:04,096 - Epoch: [46][  190/  193]    Overall Loss 0.000633    Objective Loss 0.000633                                        LR 0.000500    Time 1.109711    
2024-06-06 20:42:06,508 - Epoch: [46][  193/  193]    Overall Loss 0.000822    Objective Loss 0.000822    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 1.104793    
2024-06-06 20:42:07,161 - --- validate (epoch=46)-----------
2024-06-06 20:42:07,161 - 2172 samples (64 per mini-batch)
2024-06-06 20:42:42,276 - Epoch: [46][   10/   34]    Loss 0.001019    Top1 100.000000    Top5 100.000000    
2024-06-06 20:42:47,971 - Epoch: [46][   20/   34]    Loss 0.000710    Top1 100.000000    Top5 100.000000    
2024-06-06 20:42:53,410 - Epoch: [46][   30/   34]    Loss 0.001020    Top1 99.947917    Top5 100.000000    
2024-06-06 20:42:55,381 - Epoch: [46][   34/   34]    Loss 0.000929    Top1 99.953959    Top5 100.000000    
2024-06-06 20:42:56,346 - ==> Top1: 99.954    Top5: 100.000    Loss: 0.001

2024-06-06 20:42:56,346 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  1 230   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 20:42:56,557 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 44]
2024-06-06 20:42:56,557 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 20:42:56,565 - 

2024-06-06 20:42:56,565 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 20:43:27,728 - Epoch: [47][   10/  193]    Overall Loss 0.034433    Objective Loss 0.034433                                        LR 0.000500    Time 3.116233    
2024-06-06 20:43:38,275 - Epoch: [47][   20/  193]    Overall Loss 0.021163    Objective Loss 0.021163                                        LR 0.000500    Time 2.083685    
2024-06-06 20:43:48,767 - Epoch: [47][   30/  193]    Overall Loss 0.016582    Objective Loss 0.016582                                        LR 0.000500    Time 1.737862    
2024-06-06 20:43:59,343 - Epoch: [47][   40/  193]    Overall Loss 0.012622    Objective Loss 0.012622                                        LR 0.000500    Time 1.566961    
2024-06-06 20:44:09,969 - Epoch: [47][   50/  193]    Overall Loss 0.010270    Objective Loss 0.010270                                        LR 0.000500    Time 1.465075    
2024-06-06 20:44:20,623 - Epoch: [47][   60/  193]    Overall Loss 0.010285    Objective Loss 0.010285                                        LR 0.000500    Time 1.397949    
2024-06-06 20:44:31,196 - Epoch: [47][   70/  193]    Overall Loss 0.009130    Objective Loss 0.009130                                        LR 0.000500    Time 1.348756    
2024-06-06 20:44:41,745 - Epoch: [47][   80/  193]    Overall Loss 0.008625    Objective Loss 0.008625                                        LR 0.000500    Time 1.311589    
2024-06-06 20:44:52,782 - Epoch: [47][   90/  193]    Overall Loss 0.008557    Objective Loss 0.008557                                        LR 0.000500    Time 1.288249    
2024-06-06 20:45:03,716 - Epoch: [47][  100/  193]    Overall Loss 0.008662    Objective Loss 0.008662                                        LR 0.000500    Time 1.268232    
2024-06-06 20:45:14,140 - Epoch: [47][  110/  193]    Overall Loss 0.008513    Objective Loss 0.008513                                        LR 0.000500    Time 1.247439    
2024-06-06 20:45:24,685 - Epoch: [47][  120/  193]    Overall Loss 0.009384    Objective Loss 0.009384                                        LR 0.000500    Time 1.231102    
2024-06-06 20:45:35,152 - Epoch: [47][  130/  193]    Overall Loss 0.009174    Objective Loss 0.009174                                        LR 0.000500    Time 1.216659    
2024-06-06 20:45:45,579 - Epoch: [47][  140/  193]    Overall Loss 0.009133    Objective Loss 0.009133                                        LR 0.000500    Time 1.203988    
2024-06-06 20:45:56,049 - Epoch: [47][  150/  193]    Overall Loss 0.008601    Objective Loss 0.008601                                        LR 0.000500    Time 1.193276    
2024-06-06 20:46:06,909 - Epoch: [47][  160/  193]    Overall Loss 0.008164    Objective Loss 0.008164                                        LR 0.000500    Time 1.186334    
2024-06-06 20:46:17,393 - Epoch: [47][  170/  193]    Overall Loss 0.007696    Objective Loss 0.007696                                        LR 0.000500    Time 1.178050    
2024-06-06 20:46:28,088 - Epoch: [47][  180/  193]    Overall Loss 0.007288    Objective Loss 0.007288                                        LR 0.000500    Time 1.171845    
2024-06-06 20:46:38,395 - Epoch: [47][  190/  193]    Overall Loss 0.006908    Objective Loss 0.006908                                        LR 0.000500    Time 1.164186    
2024-06-06 20:46:40,899 - Epoch: [47][  193/  193]    Overall Loss 0.006801    Objective Loss 0.006801    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 1.158905    
2024-06-06 20:46:41,573 - --- validate (epoch=47)-----------
2024-06-06 20:46:41,573 - 2172 samples (64 per mini-batch)
2024-06-06 20:47:07,869 - Epoch: [47][   10/   34]    Loss 0.001502    Top1 99.843750    Top5 100.000000    
2024-06-06 20:47:13,668 - Epoch: [47][   20/   34]    Loss 0.000872    Top1 99.921875    Top5 100.000000    
2024-06-06 20:47:19,234 - Epoch: [47][   30/   34]    Loss 0.000625    Top1 99.947917    Top5 100.000000    
2024-06-06 20:47:21,297 - Epoch: [47][   34/   34]    Loss 0.000563    Top1 99.953959    Top5 100.000000    
2024-06-06 20:47:22,271 - ==> Top1: 99.954    Top5: 100.000    Loss: 0.001

2024-06-06 20:47:22,271 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  1   0   0   0   0  53   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 20:47:22,506 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 44]
2024-06-06 20:47:22,506 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 20:47:22,512 - 

2024-06-06 20:47:22,513 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 20:47:53,375 - Epoch: [48][   10/  193]    Overall Loss 0.000308    Objective Loss 0.000308                                        LR 0.000500    Time 3.086020    
2024-06-06 20:48:03,961 - Epoch: [48][   20/  193]    Overall Loss 0.000407    Objective Loss 0.000407                                        LR 0.000500    Time 2.071110    
2024-06-06 20:48:14,347 - Epoch: [48][   30/  193]    Overall Loss 0.000294    Objective Loss 0.000294                                        LR 0.000500    Time 1.725884    
2024-06-06 20:48:24,944 - Epoch: [48][   40/  193]    Overall Loss 0.000246    Objective Loss 0.000246                                        LR 0.000500    Time 1.558191    
2024-06-06 20:48:35,520 - Epoch: [48][   50/  193]    Overall Loss 0.000292    Objective Loss 0.000292                                        LR 0.000500    Time 1.457399    
2024-06-06 20:48:45,989 - Epoch: [48][   60/  193]    Overall Loss 0.000266    Objective Loss 0.000266                                        LR 0.000500    Time 1.388448    
2024-06-06 20:48:56,855 - Epoch: [48][   70/  193]    Overall Loss 0.000258    Objective Loss 0.000258                                        LR 0.000500    Time 1.344812    
2024-06-06 20:49:07,274 - Epoch: [48][   80/  193]    Overall Loss 0.000234    Objective Loss 0.000234                                        LR 0.000500    Time 1.306558    
2024-06-06 20:49:17,861 - Epoch: [48][   90/  193]    Overall Loss 0.000255    Objective Loss 0.000255                                        LR 0.000500    Time 1.278533    
2024-06-06 20:49:28,509 - Epoch: [48][  100/  193]    Overall Loss 0.000232    Objective Loss 0.000232                                        LR 0.000500    Time 1.256834    
2024-06-06 20:49:39,216 - Epoch: [48][  110/  193]    Overall Loss 0.000217    Objective Loss 0.000217                                        LR 0.000500    Time 1.239540    
2024-06-06 20:49:49,800 - Epoch: [48][  120/  193]    Overall Loss 0.000215    Objective Loss 0.000215                                        LR 0.000500    Time 1.224243    
2024-06-06 20:50:00,237 - Epoch: [48][  130/  193]    Overall Loss 0.000213    Objective Loss 0.000213                                        LR 0.000500    Time 1.210081    
2024-06-06 20:50:10,678 - Epoch: [48][  140/  193]    Overall Loss 0.000205    Objective Loss 0.000205                                        LR 0.000500    Time 1.198055    
2024-06-06 20:50:21,191 - Epoch: [48][  150/  193]    Overall Loss 0.000201    Objective Loss 0.000201                                        LR 0.000500    Time 1.188076    
2024-06-06 20:50:31,675 - Epoch: [48][  160/  193]    Overall Loss 0.000199    Objective Loss 0.000199                                        LR 0.000500    Time 1.179140    
2024-06-06 20:50:42,132 - Epoch: [48][  170/  193]    Overall Loss 0.000191    Objective Loss 0.000191                                        LR 0.000500    Time 1.171086    
2024-06-06 20:50:52,959 - Epoch: [48][  180/  193]    Overall Loss 0.000198    Objective Loss 0.000198                                        LR 0.000500    Time 1.165989    
2024-06-06 20:51:03,114 - Epoch: [48][  190/  193]    Overall Loss 0.000231    Objective Loss 0.000231                                        LR 0.000500    Time 1.157934    
2024-06-06 20:51:05,539 - Epoch: [48][  193/  193]    Overall Loss 0.000228    Objective Loss 0.000228    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 1.152351    
2024-06-06 20:51:06,204 - --- validate (epoch=48)-----------
2024-06-06 20:51:06,204 - 2172 samples (64 per mini-batch)
2024-06-06 20:51:32,702 - Epoch: [48][   10/   34]    Loss 0.000041    Top1 100.000000    Top5 100.000000    
2024-06-06 20:51:38,615 - Epoch: [48][   20/   34]    Loss 0.000069    Top1 100.000000    Top5 100.000000    
2024-06-06 20:51:44,478 - Epoch: [48][   30/   34]    Loss 0.000081    Top1 100.000000    Top5 100.000000    
2024-06-06 20:51:46,604 - Epoch: [48][   34/   34]    Loss 0.000076    Top1 100.000000    Top5 100.000000    
2024-06-06 20:51:47,573 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 20:51:47,574 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 20:51:47,775 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 48]
2024-06-06 20:51:47,775 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 20:51:47,794 - 

2024-06-06 20:51:47,794 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 20:52:19,543 - Epoch: [49][   10/  193]    Overall Loss 0.000096    Objective Loss 0.000096                                        LR 0.000500    Time 3.174791    
2024-06-06 20:52:30,183 - Epoch: [49][   20/  193]    Overall Loss 0.000137    Objective Loss 0.000137                                        LR 0.000500    Time 2.117441    
2024-06-06 20:52:40,595 - Epoch: [49][   30/  193]    Overall Loss 0.000099    Objective Loss 0.000099                                        LR 0.000500    Time 1.757819    
2024-06-06 20:52:51,191 - Epoch: [49][   40/  193]    Overall Loss 0.000089    Objective Loss 0.000089                                        LR 0.000500    Time 1.582347    
2024-06-06 20:53:01,707 - Epoch: [49][   50/  193]    Overall Loss 0.000080    Objective Loss 0.000080                                        LR 0.000500    Time 1.475634    
2024-06-06 20:53:12,320 - Epoch: [49][   60/  193]    Overall Loss 0.000075    Objective Loss 0.000075                                        LR 0.000500    Time 1.406122    
2024-06-06 20:53:23,055 - Epoch: [49][   70/  193]    Overall Loss 0.000095    Objective Loss 0.000095                                        LR 0.000500    Time 1.358255    
2024-06-06 20:53:33,404 - Epoch: [49][   80/  193]    Overall Loss 0.000088    Objective Loss 0.000088                                        LR 0.000500    Time 1.317472    
2024-06-06 20:53:43,873 - Epoch: [49][   90/  193]    Overall Loss 0.000082    Objective Loss 0.000082                                        LR 0.000500    Time 1.286988    
2024-06-06 20:53:54,359 - Epoch: [49][  100/  193]    Overall Loss 0.000090    Objective Loss 0.000090                                        LR 0.000500    Time 1.262832    
2024-06-06 20:54:05,021 - Epoch: [49][  110/  193]    Overall Loss 0.000084    Objective Loss 0.000084                                        LR 0.000500    Time 1.244698    
2024-06-06 20:54:15,474 - Epoch: [49][  120/  193]    Overall Loss 0.000082    Objective Loss 0.000082                                        LR 0.000500    Time 1.227840    
2024-06-06 20:54:26,052 - Epoch: [49][  130/  193]    Overall Loss 0.000083    Objective Loss 0.000083                                        LR 0.000500    Time 1.214442    
2024-06-06 20:54:36,386 - Epoch: [49][  140/  193]    Overall Loss 0.000079    Objective Loss 0.000079                                        LR 0.000500    Time 1.201291    
2024-06-06 20:54:46,749 - Epoch: [49][  150/  193]    Overall Loss 0.000081    Objective Loss 0.000081                                        LR 0.000500    Time 1.190112    
2024-06-06 20:54:57,186 - Epoch: [49][  160/  193]    Overall Loss 0.000082    Objective Loss 0.000082                                        LR 0.000500    Time 1.180756    
2024-06-06 20:55:07,640 - Epoch: [49][  170/  193]    Overall Loss 0.000080    Objective Loss 0.000080                                        LR 0.000500    Time 1.172611    
2024-06-06 20:55:18,069 - Epoch: [49][  180/  193]    Overall Loss 0.000083    Objective Loss 0.000083                                        LR 0.000500    Time 1.165212    
2024-06-06 20:55:28,316 - Epoch: [49][  190/  193]    Overall Loss 0.000081    Objective Loss 0.000081                                        LR 0.000500    Time 1.157676    
2024-06-06 20:55:30,757 - Epoch: [49][  193/  193]    Overall Loss 0.000080    Objective Loss 0.000080    Top1 100.000000    Top5 100.000000    LR 0.000500    Time 1.152185    
2024-06-06 20:55:31,413 - --- validate (epoch=49)-----------
2024-06-06 20:55:31,414 - 2172 samples (64 per mini-batch)
2024-06-06 20:55:58,350 - Epoch: [49][   10/   34]    Loss 0.000018    Top1 100.000000    Top5 100.000000    
2024-06-06 20:56:04,185 - Epoch: [49][   20/   34]    Loss 0.000040    Top1 100.000000    Top5 100.000000    
2024-06-06 20:56:09,675 - Epoch: [49][   30/   34]    Loss 0.000040    Top1 100.000000    Top5 100.000000    
2024-06-06 20:56:11,700 - Epoch: [49][   34/   34]    Loss 0.000050    Top1 100.000000    Top5 100.000000    
2024-06-06 20:56:12,662 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 20:56:12,662 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 20:56:12,869 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 49]
2024-06-06 20:56:12,870 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 20:56:12,887 - 

2024-06-06 20:56:12,887 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 20:56:44,933 - Epoch: [50][   10/  193]    Overall Loss 0.000021    Objective Loss 0.000021                                        LR 0.000250    Time 3.204604    
2024-06-06 20:56:55,475 - Epoch: [50][   20/  193]    Overall Loss 0.000025    Objective Loss 0.000025                                        LR 0.000250    Time 2.127876    
2024-06-06 20:57:06,183 - Epoch: [50][   30/  193]    Overall Loss 0.000035    Objective Loss 0.000035                                        LR 0.000250    Time 1.774635    
2024-06-06 20:57:16,721 - Epoch: [50][   40/  193]    Overall Loss 0.000050    Objective Loss 0.000050                                        LR 0.000250    Time 1.593709    
2024-06-06 20:57:27,295 - Epoch: [50][   50/  193]    Overall Loss 0.000043    Objective Loss 0.000043                                        LR 0.000250    Time 1.485820    
2024-06-06 20:57:37,841 - Epoch: [50][   60/  193]    Overall Loss 0.000040    Objective Loss 0.000040                                        LR 0.000250    Time 1.413517    
2024-06-06 20:57:48,428 - Epoch: [50][   70/  193]    Overall Loss 0.000040    Objective Loss 0.000040                                        LR 0.000250    Time 1.362489    
2024-06-06 20:57:58,983 - Epoch: [50][   80/  193]    Overall Loss 0.000043    Objective Loss 0.000043                                        LR 0.000250    Time 1.323721    
2024-06-06 20:58:09,428 - Epoch: [50][   90/  193]    Overall Loss 0.000055    Objective Loss 0.000055                                        LR 0.000250    Time 1.292422    
2024-06-06 20:58:19,922 - Epoch: [50][  100/  193]    Overall Loss 0.000054    Objective Loss 0.000054                                        LR 0.000250    Time 1.267797    
2024-06-06 20:58:30,629 - Epoch: [50][  110/  193]    Overall Loss 0.000054    Objective Loss 0.000054                                        LR 0.000250    Time 1.249675    
2024-06-06 20:58:41,314 - Epoch: [50][  120/  193]    Overall Loss 0.000051    Objective Loss 0.000051                                        LR 0.000250    Time 1.234251    
2024-06-06 20:58:52,032 - Epoch: [50][  130/  193]    Overall Loss 0.000049    Objective Loss 0.000049                                        LR 0.000250    Time 1.221528    
2024-06-06 20:59:02,715 - Epoch: [50][  140/  193]    Overall Loss 0.000046    Objective Loss 0.000046                                        LR 0.000250    Time 1.210311    
2024-06-06 20:59:13,519 - Epoch: [50][  150/  193]    Overall Loss 0.000046    Objective Loss 0.000046                                        LR 0.000250    Time 1.201427    
2024-06-06 20:59:24,378 - Epoch: [50][  160/  193]    Overall Loss 0.000052    Objective Loss 0.000052                                        LR 0.000250    Time 1.194026    
2024-06-06 20:59:35,266 - Epoch: [50][  170/  193]    Overall Loss 0.000050    Objective Loss 0.000050                                        LR 0.000250    Time 1.187653    
2024-06-06 20:59:45,986 - Epoch: [50][  180/  193]    Overall Loss 0.000048    Objective Loss 0.000048                                        LR 0.000250    Time 1.181082    
2024-06-06 20:59:56,911 - Epoch: [50][  190/  193]    Overall Loss 0.000047    Objective Loss 0.000047                                        LR 0.000250    Time 1.176212    
2024-06-06 20:59:59,397 - Epoch: [50][  193/  193]    Overall Loss 0.000047    Objective Loss 0.000047    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.170646    
2024-06-06 21:00:00,093 - --- validate (epoch=50)-----------
2024-06-06 21:00:00,093 - 2172 samples (64 per mini-batch)
2024-06-06 21:00:27,003 - Epoch: [50][   10/   34]    Loss 0.000036    Top1 100.000000    Top5 100.000000    
2024-06-06 21:00:32,911 - Epoch: [50][   20/   34]    Loss 0.000048    Top1 100.000000    Top5 100.000000    
2024-06-06 21:00:38,427 - Epoch: [50][   30/   34]    Loss 0.000039    Top1 100.000000    Top5 100.000000    
2024-06-06 21:00:40,479 - Epoch: [50][   34/   34]    Loss 0.000037    Top1 100.000000    Top5 100.000000    
2024-06-06 21:00:41,463 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 21:00:41,464 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 21:00:41,692 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 50]
2024-06-06 21:00:41,692 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 21:00:41,710 - 

2024-06-06 21:00:41,710 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 21:01:14,001 - Epoch: [51][   10/  193]    Overall Loss 0.000057    Objective Loss 0.000057                                        LR 0.000250    Time 3.229070    
2024-06-06 21:01:24,609 - Epoch: [51][   20/  193]    Overall Loss 0.000065    Objective Loss 0.000065                                        LR 0.000250    Time 2.142690    
2024-06-06 21:01:35,249 - Epoch: [51][   30/  193]    Overall Loss 0.000054    Objective Loss 0.000054                                        LR 0.000250    Time 1.781911    
2024-06-06 21:01:45,762 - Epoch: [51][   40/  193]    Overall Loss 0.000046    Objective Loss 0.000046                                        LR 0.000250    Time 1.598250    
2024-06-06 21:01:56,131 - Epoch: [51][   50/  193]    Overall Loss 0.000039    Objective Loss 0.000039                                        LR 0.000250    Time 1.485440    
2024-06-06 21:02:06,509 - Epoch: [51][   60/  193]    Overall Loss 0.000036    Objective Loss 0.000036                                        LR 0.000250    Time 1.410299    
2024-06-06 21:02:16,918 - Epoch: [51][   70/  193]    Overall Loss 0.000046    Objective Loss 0.000046                                        LR 0.000250    Time 1.357049    
2024-06-06 21:02:27,264 - Epoch: [51][   80/  193]    Overall Loss 0.000042    Objective Loss 0.000042                                        LR 0.000250    Time 1.316353    
2024-06-06 21:02:37,829 - Epoch: [51][   90/  193]    Overall Loss 0.000043    Objective Loss 0.000043                                        LR 0.000250    Time 1.287174    
2024-06-06 21:02:48,246 - Epoch: [51][  100/  193]    Overall Loss 0.000040    Objective Loss 0.000040                                        LR 0.000250    Time 1.262286    
2024-06-06 21:02:58,709 - Epoch: [51][  110/  193]    Overall Loss 0.000040    Objective Loss 0.000040                                        LR 0.000250    Time 1.242367    
2024-06-06 21:03:09,039 - Epoch: [51][  120/  193]    Overall Loss 0.000042    Objective Loss 0.000042                                        LR 0.000250    Time 1.224635    
2024-06-06 21:03:19,386 - Epoch: [51][  130/  193]    Overall Loss 0.000040    Objective Loss 0.000040                                        LR 0.000250    Time 1.209779    
2024-06-06 21:03:29,783 - Epoch: [51][  140/  193]    Overall Loss 0.000039    Objective Loss 0.000039                                        LR 0.000250    Time 1.197453    
2024-06-06 21:03:40,259 - Epoch: [51][  150/  193]    Overall Loss 0.000056    Objective Loss 0.000056                                        LR 0.000250    Time 1.187257    
2024-06-06 21:03:51,021 - Epoch: [51][  160/  193]    Overall Loss 0.000057    Objective Loss 0.000057                                        LR 0.000250    Time 1.179969    
2024-06-06 21:04:01,310 - Epoch: [51][  170/  193]    Overall Loss 0.000056    Objective Loss 0.000056                                        LR 0.000250    Time 1.170882    
2024-06-06 21:04:11,777 - Epoch: [51][  180/  193]    Overall Loss 0.000054    Objective Loss 0.000054                                        LR 0.000250    Time 1.163831    
2024-06-06 21:04:21,759 - Epoch: [51][  190/  193]    Overall Loss 0.000053    Objective Loss 0.000053                                        LR 0.000250    Time 1.154937    
2024-06-06 21:04:24,190 - Epoch: [51][  193/  193]    Overall Loss 0.000052    Objective Loss 0.000052    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.149412    
2024-06-06 21:04:24,833 - --- validate (epoch=51)-----------
2024-06-06 21:04:24,833 - 2172 samples (64 per mini-batch)
2024-06-06 21:04:51,443 - Epoch: [51][   10/   34]    Loss 0.000027    Top1 100.000000    Top5 100.000000    
2024-06-06 21:04:57,274 - Epoch: [51][   20/   34]    Loss 0.000030    Top1 100.000000    Top5 100.000000    
2024-06-06 21:05:02,743 - Epoch: [51][   30/   34]    Loss 0.000031    Top1 100.000000    Top5 100.000000    
2024-06-06 21:05:04,749 - Epoch: [51][   34/   34]    Loss 0.000437    Top1 99.953959    Top5 100.000000    
2024-06-06 21:05:05,747 - ==> Top1: 99.954    Top5: 100.000    Loss: 0.000

2024-06-06 21:05:05,748 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  1   0 227   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 21:05:05,958 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 50]
2024-06-06 21:05:05,959 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 21:05:05,963 - 

2024-06-06 21:05:05,963 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 21:05:36,919 - Epoch: [52][   10/  193]    Overall Loss 0.000057    Objective Loss 0.000057                                        LR 0.000250    Time 3.095462    
2024-06-06 21:05:47,404 - Epoch: [52][   20/  193]    Overall Loss 0.000063    Objective Loss 0.000063                                        LR 0.000250    Time 2.070083    
2024-06-06 21:05:57,842 - Epoch: [52][   30/  193]    Overall Loss 0.000049    Objective Loss 0.000049                                        LR 0.000250    Time 1.727131    
2024-06-06 21:06:08,370 - Epoch: [52][   40/  193]    Overall Loss 0.000043    Objective Loss 0.000043                                        LR 0.000250    Time 1.557672    
2024-06-06 21:06:18,726 - Epoch: [52][   50/  193]    Overall Loss 0.000043    Objective Loss 0.000043                                        LR 0.000250    Time 1.452589    
2024-06-06 21:06:29,243 - Epoch: [52][   60/  193]    Overall Loss 0.000042    Objective Loss 0.000042                                        LR 0.000250    Time 1.385241    
2024-06-06 21:06:39,955 - Epoch: [52][   70/  193]    Overall Loss 0.000080    Objective Loss 0.000080                                        LR 0.000250    Time 1.339882    
2024-06-06 21:06:52,127 - Epoch: [52][   80/  193]    Overall Loss 0.000073    Objective Loss 0.000073                                        LR 0.000250    Time 1.324131    
2024-06-06 21:07:03,499 - Epoch: [52][   90/  193]    Overall Loss 0.000074    Objective Loss 0.000074                                        LR 0.000250    Time 1.302861    
2024-06-06 21:07:14,533 - Epoch: [52][  100/  193]    Overall Loss 0.000068    Objective Loss 0.000068                                        LR 0.000250    Time 1.282521    
2024-06-06 21:07:24,985 - Epoch: [52][  110/  193]    Overall Loss 0.000065    Objective Loss 0.000065                                        LR 0.000250    Time 1.260658    
2024-06-06 21:07:35,448 - Epoch: [52][  120/  193]    Overall Loss 0.000061    Objective Loss 0.000061                                        LR 0.000250    Time 1.242527    
2024-06-06 21:07:45,668 - Epoch: [52][  130/  193]    Overall Loss 0.000058    Objective Loss 0.000058                                        LR 0.000250    Time 1.225298    
2024-06-06 21:07:56,025 - Epoch: [52][  140/  193]    Overall Loss 0.000056    Objective Loss 0.000056                                        LR 0.000250    Time 1.211505    
2024-06-06 21:08:06,417 - Epoch: [52][  150/  193]    Overall Loss 0.000054    Objective Loss 0.000054                                        LR 0.000250    Time 1.199780    
2024-06-06 21:08:16,925 - Epoch: [52][  160/  193]    Overall Loss 0.000063    Objective Loss 0.000063                                        LR 0.000250    Time 1.190277    
2024-06-06 21:08:27,231 - Epoch: [52][  170/  193]    Overall Loss 0.000060    Objective Loss 0.000060                                        LR 0.000250    Time 1.180615    
2024-06-06 21:08:37,705 - Epoch: [52][  180/  193]    Overall Loss 0.000059    Objective Loss 0.000059                                        LR 0.000250    Time 1.173065    
2024-06-06 21:08:47,768 - Epoch: [52][  190/  193]    Overall Loss 0.000057    Objective Loss 0.000057                                        LR 0.000250    Time 1.164167    
2024-06-06 21:08:50,185 - Epoch: [52][  193/  193]    Overall Loss 0.000056    Objective Loss 0.000056    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.158449    
2024-06-06 21:08:50,834 - --- validate (epoch=52)-----------
2024-06-06 21:08:50,835 - 2172 samples (64 per mini-batch)
2024-06-06 21:09:16,773 - Epoch: [52][   10/   34]    Loss 0.000023    Top1 100.000000    Top5 100.000000    
2024-06-06 21:09:22,700 - Epoch: [52][   20/   34]    Loss 0.000022    Top1 100.000000    Top5 100.000000    
2024-06-06 21:09:29,045 - Epoch: [52][   30/   34]    Loss 0.000020    Top1 100.000000    Top5 100.000000    
2024-06-06 21:09:31,095 - Epoch: [52][   34/   34]    Loss 0.000020    Top1 100.000000    Top5 100.000000    
2024-06-06 21:09:32,086 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 21:09:32,087 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 21:09:32,347 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 52]
2024-06-06 21:09:32,347 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 21:09:32,370 - 

2024-06-06 21:09:32,370 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 21:10:02,903 - Epoch: [53][   10/  193]    Overall Loss 0.000028    Objective Loss 0.000028                                        LR 0.000250    Time 3.053266    
2024-06-06 21:10:13,331 - Epoch: [53][   20/  193]    Overall Loss 0.000044    Objective Loss 0.000044                                        LR 0.000250    Time 2.045972    
2024-06-06 21:10:23,571 - Epoch: [53][   30/  193]    Overall Loss 0.000040    Objective Loss 0.000040                                        LR 0.000250    Time 1.704242    
2024-06-06 21:10:34,161 - Epoch: [53][   40/  193]    Overall Loss 0.000034    Objective Loss 0.000034                                        LR 0.000250    Time 1.542305    
2024-06-06 21:10:44,919 - Epoch: [53][   50/  193]    Overall Loss 0.000030    Objective Loss 0.000030                                        LR 0.000250    Time 1.448350    
2024-06-06 21:10:55,379 - Epoch: [53][   60/  193]    Overall Loss 0.000032    Objective Loss 0.000032                                        LR 0.000250    Time 1.380726    
2024-06-06 21:11:05,869 - Epoch: [53][   70/  193]    Overall Loss 0.000036    Objective Loss 0.000036                                        LR 0.000250    Time 1.332860    
2024-06-06 21:11:16,191 - Epoch: [53][   80/  193]    Overall Loss 0.000036    Objective Loss 0.000036                                        LR 0.000250    Time 1.294906    
2024-06-06 21:11:27,691 - Epoch: [53][   90/  193]    Overall Loss 0.000038    Objective Loss 0.000038                                        LR 0.000250    Time 1.278479    
2024-06-06 21:11:39,387 - Epoch: [53][  100/  193]    Overall Loss 0.000038    Objective Loss 0.000038                                        LR 0.000250    Time 1.267202    
2024-06-06 21:11:50,071 - Epoch: [53][  110/  193]    Overall Loss 0.000037    Objective Loss 0.000037                                        LR 0.000250    Time 1.248877    
2024-06-06 21:12:00,759 - Epoch: [53][  120/  193]    Overall Loss 0.000061    Objective Loss 0.000061                                        LR 0.000250    Time 1.233667    
2024-06-06 21:12:11,602 - Epoch: [53][  130/  193]    Overall Loss 0.000058    Objective Loss 0.000058                                        LR 0.000250    Time 1.221944    
2024-06-06 21:12:22,142 - Epoch: [53][  140/  193]    Overall Loss 0.000057    Objective Loss 0.000057                                        LR 0.000250    Time 1.209730    
2024-06-06 21:12:32,833 - Epoch: [53][  150/  193]    Overall Loss 0.000054    Objective Loss 0.000054                                        LR 0.000250    Time 1.200137    
2024-06-06 21:12:43,838 - Epoch: [53][  160/  193]    Overall Loss 0.000053    Objective Loss 0.000053                                        LR 0.000250    Time 1.193635    
2024-06-06 21:12:54,651 - Epoch: [53][  170/  193]    Overall Loss 0.000051    Objective Loss 0.000051                                        LR 0.000250    Time 1.186767    
2024-06-06 21:13:05,588 - Epoch: [53][  180/  193]    Overall Loss 0.000052    Objective Loss 0.000052                                        LR 0.000250    Time 1.181458    
2024-06-06 21:13:16,052 - Epoch: [53][  190/  193]    Overall Loss 0.000050    Objective Loss 0.000050                                        LR 0.000250    Time 1.174198    
2024-06-06 21:13:18,515 - Epoch: [53][  193/  193]    Overall Loss 0.000049    Objective Loss 0.000049    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.168518    
2024-06-06 21:13:19,257 - --- validate (epoch=53)-----------
2024-06-06 21:13:19,257 - 2172 samples (64 per mini-batch)
2024-06-06 21:13:46,310 - Epoch: [53][   10/   34]    Loss 0.000079    Top1 100.000000    Top5 100.000000    
2024-06-06 21:13:52,210 - Epoch: [53][   20/   34]    Loss 0.000065    Top1 100.000000    Top5 100.000000    
2024-06-06 21:13:57,812 - Epoch: [53][   30/   34]    Loss 0.000061    Top1 100.000000    Top5 100.000000    
2024-06-06 21:13:59,886 - Epoch: [53][   34/   34]    Loss 0.000056    Top1 100.000000    Top5 100.000000    
2024-06-06 21:14:00,873 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 21:14:00,873 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 21:14:01,086 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 53]
2024-06-06 21:14:01,086 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 21:14:01,105 - 

2024-06-06 21:14:01,106 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 21:14:32,552 - Epoch: [54][   10/  193]    Overall Loss 0.000042    Objective Loss 0.000042                                        LR 0.000250    Time 3.144603    
2024-06-06 21:14:43,421 - Epoch: [54][   20/  193]    Overall Loss 0.000040    Objective Loss 0.000040                                        LR 0.000250    Time 2.114376    
2024-06-06 21:14:53,869 - Epoch: [54][   30/  193]    Overall Loss 0.000034    Objective Loss 0.000034                                        LR 0.000250    Time 1.756574    
2024-06-06 21:15:04,540 - Epoch: [54][   40/  193]    Overall Loss 0.000032    Objective Loss 0.000032                                        LR 0.000250    Time 1.583272    
2024-06-06 21:15:15,109 - Epoch: [54][   50/  193]    Overall Loss 0.000027    Objective Loss 0.000027                                        LR 0.000250    Time 1.477447    
2024-06-06 21:15:25,716 - Epoch: [54][   60/  193]    Overall Loss 0.000033    Objective Loss 0.000033                                        LR 0.000250    Time 1.407425    
2024-06-06 21:15:36,689 - Epoch: [54][   70/  193]    Overall Loss 0.000036    Objective Loss 0.000036                                        LR 0.000250    Time 1.362658    
2024-06-06 21:15:47,244 - Epoch: [54][   80/  193]    Overall Loss 0.000040    Objective Loss 0.000040                                        LR 0.000250    Time 1.323919    
2024-06-06 21:15:57,736 - Epoch: [54][   90/  193]    Overall Loss 0.000041    Objective Loss 0.000041                                        LR 0.000250    Time 1.292997    
2024-06-06 21:16:08,272 - Epoch: [54][  100/  193]    Overall Loss 0.000039    Objective Loss 0.000039                                        LR 0.000250    Time 1.268730    
2024-06-06 21:16:18,818 - Epoch: [54][  110/  193]    Overall Loss 0.000038    Objective Loss 0.000038                                        LR 0.000250    Time 1.248990    
2024-06-06 21:16:29,546 - Epoch: [54][  120/  193]    Overall Loss 0.000037    Objective Loss 0.000037                                        LR 0.000250    Time 1.233981    
2024-06-06 21:16:40,580 - Epoch: [54][  130/  193]    Overall Loss 0.000036    Objective Loss 0.000036                                        LR 0.000250    Time 1.223656    
2024-06-06 21:16:51,313 - Epoch: [54][  140/  193]    Overall Loss 0.000036    Objective Loss 0.000036                                        LR 0.000250    Time 1.212683    
2024-06-06 21:17:02,222 - Epoch: [54][  150/  193]    Overall Loss 0.000036    Objective Loss 0.000036                                        LR 0.000250    Time 1.204332    
2024-06-06 21:17:12,948 - Epoch: [54][  160/  193]    Overall Loss 0.000036    Objective Loss 0.000036                                        LR 0.000250    Time 1.195851    
2024-06-06 21:17:23,552 - Epoch: [54][  170/  193]    Overall Loss 0.000035    Objective Loss 0.000035                                        LR 0.000250    Time 1.187694    
2024-06-06 21:17:34,318 - Epoch: [54][  180/  193]    Overall Loss 0.000035    Objective Loss 0.000035                                        LR 0.000250    Time 1.181398    
2024-06-06 21:17:44,821 - Epoch: [54][  190/  193]    Overall Loss 0.000034    Objective Loss 0.000034                                        LR 0.000250    Time 1.174307    
2024-06-06 21:17:47,332 - Epoch: [54][  193/  193]    Overall Loss 0.000034    Objective Loss 0.000034    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.168921    
2024-06-06 21:17:48,004 - --- validate (epoch=54)-----------
2024-06-06 21:17:48,005 - 2172 samples (64 per mini-batch)
2024-06-06 21:18:14,912 - Epoch: [54][   10/   34]    Loss 0.000024    Top1 100.000000    Top5 100.000000    
2024-06-06 21:18:21,059 - Epoch: [54][   20/   34]    Loss 0.000079    Top1 100.000000    Top5 100.000000    
2024-06-06 21:18:26,840 - Epoch: [54][   30/   34]    Loss 0.000058    Top1 100.000000    Top5 100.000000    
2024-06-06 21:18:29,080 - Epoch: [54][   34/   34]    Loss 0.000056    Top1 100.000000    Top5 100.000000    
2024-06-06 21:18:30,050 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 21:18:30,050 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 21:18:30,267 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 54]
2024-06-06 21:18:30,268 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 21:18:30,289 - 

2024-06-06 21:18:30,289 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 21:19:02,808 - Epoch: [55][   10/  193]    Overall Loss 0.000066    Objective Loss 0.000066                                        LR 0.000250    Time 3.251891    
2024-06-06 21:19:13,772 - Epoch: [55][   20/  193]    Overall Loss 0.000084    Objective Loss 0.000084                                        LR 0.000250    Time 2.172344    
2024-06-06 21:19:24,613 - Epoch: [55][   30/  193]    Overall Loss 0.000089    Objective Loss 0.000089                                        LR 0.000250    Time 1.808749    
2024-06-06 21:19:35,763 - Epoch: [55][   40/  193]    Overall Loss 0.000072    Objective Loss 0.000072                                        LR 0.000250    Time 1.634530    
2024-06-06 21:19:46,633 - Epoch: [55][   50/  193]    Overall Loss 0.000060    Objective Loss 0.000060                                        LR 0.000250    Time 1.524414    
2024-06-06 21:19:57,386 - Epoch: [55][   60/  193]    Overall Loss 0.000053    Objective Loss 0.000053                                        LR 0.000250    Time 1.449024    
2024-06-06 21:20:08,173 - Epoch: [55][   70/  193]    Overall Loss 0.000071    Objective Loss 0.000071                                        LR 0.000250    Time 1.395673    
2024-06-06 21:20:18,766 - Epoch: [55][   80/  193]    Overall Loss 0.000065    Objective Loss 0.000065                                        LR 0.000250    Time 1.353156    
2024-06-06 21:20:29,305 - Epoch: [55][   90/  193]    Overall Loss 0.000062    Objective Loss 0.000062                                        LR 0.000250    Time 1.319538    
2024-06-06 21:20:40,137 - Epoch: [55][  100/  193]    Overall Loss 0.000057    Objective Loss 0.000057                                        LR 0.000250    Time 1.295599    
2024-06-06 21:20:51,198 - Epoch: [55][  110/  193]    Overall Loss 0.000055    Objective Loss 0.000055                                        LR 0.000250    Time 1.278080    
2024-06-06 21:21:01,934 - Epoch: [55][  120/  193]    Overall Loss 0.000052    Objective Loss 0.000052                                        LR 0.000250    Time 1.260810    
2024-06-06 21:21:13,453 - Epoch: [55][  130/  193]    Overall Loss 0.000049    Objective Loss 0.000049                                        LR 0.000250    Time 1.252183    
2024-06-06 21:21:24,049 - Epoch: [55][  140/  193]    Overall Loss 0.000047    Objective Loss 0.000047                                        LR 0.000250    Time 1.238192    
2024-06-06 21:21:34,813 - Epoch: [55][  150/  193]    Overall Loss 0.000045    Objective Loss 0.000045                                        LR 0.000250    Time 1.227196    
2024-06-06 21:21:45,793 - Epoch: [55][  160/  193]    Overall Loss 0.000043    Objective Loss 0.000043                                        LR 0.000250    Time 1.218666    
2024-06-06 21:21:56,878 - Epoch: [55][  170/  193]    Overall Loss 0.000043    Objective Loss 0.000043                                        LR 0.000250    Time 1.211989    
2024-06-06 21:22:07,918 - Epoch: [55][  180/  193]    Overall Loss 0.000054    Objective Loss 0.000054                                        LR 0.000250    Time 1.205767    
2024-06-06 21:22:18,177 - Epoch: [55][  190/  193]    Overall Loss 0.000052    Objective Loss 0.000052                                        LR 0.000250    Time 1.196147    
2024-06-06 21:22:20,580 - Epoch: [55][  193/  193]    Overall Loss 0.000052    Objective Loss 0.000052    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.189858    
2024-06-06 21:22:21,264 - --- validate (epoch=55)-----------
2024-06-06 21:22:21,265 - 2172 samples (64 per mini-batch)
2024-06-06 21:22:47,553 - Epoch: [55][   10/   34]    Loss 0.000019    Top1 100.000000    Top5 100.000000    
2024-06-06 21:22:53,288 - Epoch: [55][   20/   34]    Loss 0.000045    Top1 100.000000    Top5 100.000000    
2024-06-06 21:22:58,818 - Epoch: [55][   30/   34]    Loss 0.000032    Top1 100.000000    Top5 100.000000    
2024-06-06 21:23:00,888 - Epoch: [55][   34/   34]    Loss 0.000032    Top1 100.000000    Top5 100.000000    
2024-06-06 21:23:01,866 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 21:23:01,866 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 21:23:02,180 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 55]
2024-06-06 21:23:02,181 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 21:23:02,197 - 

2024-06-06 21:23:02,198 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 21:23:33,540 - Epoch: [56][   10/  193]    Overall Loss 0.000045    Objective Loss 0.000045                                        LR 0.000250    Time 3.134059    
2024-06-06 21:23:44,183 - Epoch: [56][   20/  193]    Overall Loss 0.000067    Objective Loss 0.000067                                        LR 0.000250    Time 2.097516    
2024-06-06 21:23:54,602 - Epoch: [56][   30/  193]    Overall Loss 0.000051    Objective Loss 0.000051                                        LR 0.000250    Time 1.744557    
2024-06-06 21:24:05,192 - Epoch: [56][   40/  193]    Overall Loss 0.000041    Objective Loss 0.000041                                        LR 0.000250    Time 1.572565    
2024-06-06 21:24:15,719 - Epoch: [56][   50/  193]    Overall Loss 0.000035    Objective Loss 0.000035                                        LR 0.000250    Time 1.468108    
2024-06-06 21:24:26,263 - Epoch: [56][   60/  193]    Overall Loss 0.000032    Objective Loss 0.000032                                        LR 0.000250    Time 1.398641    
2024-06-06 21:24:36,859 - Epoch: [56][   70/  193]    Overall Loss 0.000036    Objective Loss 0.000036                                        LR 0.000250    Time 1.349796    
2024-06-06 21:24:47,471 - Epoch: [56][   80/  193]    Overall Loss 0.000033    Objective Loss 0.000033                                        LR 0.000250    Time 1.313356    
2024-06-06 21:24:58,046 - Epoch: [56][   90/  193]    Overall Loss 0.000033    Objective Loss 0.000033                                        LR 0.000250    Time 1.284516    
2024-06-06 21:25:08,694 - Epoch: [56][  100/  193]    Overall Loss 0.000031    Objective Loss 0.000031                                        LR 0.000250    Time 1.262188    
2024-06-06 21:25:19,330 - Epoch: [56][  110/  193]    Overall Loss 0.000030    Objective Loss 0.000030                                        LR 0.000250    Time 1.243865    
2024-06-06 21:25:29,975 - Epoch: [56][  120/  193]    Overall Loss 0.000030    Objective Loss 0.000030                                        LR 0.000250    Time 1.228607    
2024-06-06 21:25:40,609 - Epoch: [56][  130/  193]    Overall Loss 0.000028    Objective Loss 0.000028                                        LR 0.000250    Time 1.215658    
2024-06-06 21:25:51,241 - Epoch: [56][  140/  193]    Overall Loss 0.000027    Objective Loss 0.000027                                        LR 0.000250    Time 1.204594    
2024-06-06 21:26:01,941 - Epoch: [56][  150/  193]    Overall Loss 0.000030    Objective Loss 0.000030                                        LR 0.000250    Time 1.195390    
2024-06-06 21:26:12,646 - Epoch: [56][  160/  193]    Overall Loss 0.000030    Objective Loss 0.000030                                        LR 0.000250    Time 1.187409    
2024-06-06 21:26:23,458 - Epoch: [56][  170/  193]    Overall Loss 0.000029    Objective Loss 0.000029                                        LR 0.000250    Time 1.181013    
2024-06-06 21:26:35,254 - Epoch: [56][  180/  193]    Overall Loss 0.000029    Objective Loss 0.000029                                        LR 0.000250    Time 1.180743    
2024-06-06 21:26:46,632 - Epoch: [56][  190/  193]    Overall Loss 0.000029    Objective Loss 0.000029                                        LR 0.000250    Time 1.178080    
2024-06-06 21:26:49,270 - Epoch: [56][  193/  193]    Overall Loss 0.000029    Objective Loss 0.000029    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.173237    
2024-06-06 21:26:49,937 - --- validate (epoch=56)-----------
2024-06-06 21:26:49,938 - 2172 samples (64 per mini-batch)
2024-06-06 21:27:26,394 - Epoch: [56][   10/   34]    Loss 0.000016    Top1 100.000000    Top5 100.000000    
2024-06-06 21:27:32,621 - Epoch: [56][   20/   34]    Loss 0.000022    Top1 100.000000    Top5 100.000000    
2024-06-06 21:27:38,528 - Epoch: [56][   30/   34]    Loss 0.000036    Top1 100.000000    Top5 100.000000    
2024-06-06 21:27:40,747 - Epoch: [56][   34/   34]    Loss 0.000033    Top1 100.000000    Top5 100.000000    
2024-06-06 21:27:41,767 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 21:27:41,767 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 21:27:42,007 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 56]
2024-06-06 21:27:42,008 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 21:27:42,026 - 

2024-06-06 21:27:42,026 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 21:28:15,082 - Epoch: [57][   10/  193]    Overall Loss 0.000035    Objective Loss 0.000035                                        LR 0.000250    Time 3.305399    
2024-06-06 21:28:26,689 - Epoch: [57][   20/  193]    Overall Loss 0.000037    Objective Loss 0.000037                                        LR 0.000250    Time 2.230629    
2024-06-06 21:28:37,964 - Epoch: [57][   30/  193]    Overall Loss 0.000035    Objective Loss 0.000035                                        LR 0.000250    Time 1.861677    
2024-06-06 21:28:49,340 - Epoch: [57][   40/  193]    Overall Loss 0.000029    Objective Loss 0.000029                                        LR 0.000250    Time 1.679644    
2024-06-06 21:29:00,384 - Epoch: [57][   50/  193]    Overall Loss 0.000026    Objective Loss 0.000026                                        LR 0.000250    Time 1.564068    
2024-06-06 21:29:11,490 - Epoch: [57][   60/  193]    Overall Loss 0.000024    Objective Loss 0.000024                                        LR 0.000250    Time 1.487886    
2024-06-06 21:29:25,415 - Epoch: [57][   70/  193]    Overall Loss 0.000026    Objective Loss 0.000026                                        LR 0.000250    Time 1.473723    
2024-06-06 21:29:41,028 - Epoch: [57][   80/  193]    Overall Loss 0.000025    Objective Loss 0.000025                                        LR 0.000250    Time 1.483499    
2024-06-06 21:29:55,590 - Epoch: [57][   90/  193]    Overall Loss 0.000025    Objective Loss 0.000025                                        LR 0.000250    Time 1.479760    
2024-06-06 21:30:09,690 - Epoch: [57][  100/  193]    Overall Loss 0.000024    Objective Loss 0.000024                                        LR 0.000250    Time 1.471955    
2024-06-06 21:30:23,857 - Epoch: [57][  110/  193]    Overall Loss 0.000026    Objective Loss 0.000026                                        LR 0.000250    Time 1.466381    
2024-06-06 21:30:36,876 - Epoch: [57][  120/  193]    Overall Loss 0.000024    Objective Loss 0.000024                                        LR 0.000250    Time 1.452124    
2024-06-06 21:30:49,967 - Epoch: [57][  130/  193]    Overall Loss 0.000024    Objective Loss 0.000024                                        LR 0.000250    Time 1.440758    
2024-06-06 21:31:00,960 - Epoch: [57][  140/  193]    Overall Loss 0.000023    Objective Loss 0.000023                                        LR 0.000250    Time 1.416117    
2024-06-06 21:31:12,019 - Epoch: [57][  150/  193]    Overall Loss 0.000024    Objective Loss 0.000024                                        LR 0.000250    Time 1.395215    
2024-06-06 21:31:23,253 - Epoch: [57][  160/  193]    Overall Loss 0.000029    Objective Loss 0.000029                                        LR 0.000250    Time 1.378012    
2024-06-06 21:31:34,433 - Epoch: [57][  170/  193]    Overall Loss 0.000028    Objective Loss 0.000028                                        LR 0.000250    Time 1.362484    
2024-06-06 21:31:46,029 - Epoch: [57][  180/  193]    Overall Loss 0.000028    Objective Loss 0.000028                                        LR 0.000250    Time 1.350973    
2024-06-06 21:31:57,404 - Epoch: [57][  190/  193]    Overall Loss 0.000028    Objective Loss 0.000028                                        LR 0.000250    Time 1.339573    
2024-06-06 21:32:00,051 - Epoch: [57][  193/  193]    Overall Loss 0.000027    Objective Loss 0.000027    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.332182    
2024-06-06 21:32:00,737 - --- validate (epoch=57)-----------
2024-06-06 21:32:00,737 - 2172 samples (64 per mini-batch)
2024-06-06 21:32:28,452 - Epoch: [57][   10/   34]    Loss 0.000049    Top1 100.000000    Top5 100.000000    
2024-06-06 21:32:34,617 - Epoch: [57][   20/   34]    Loss 0.000077    Top1 100.000000    Top5 100.000000    
2024-06-06 21:32:40,332 - Epoch: [57][   30/   34]    Loss 0.000065    Top1 100.000000    Top5 100.000000    
2024-06-06 21:32:42,529 - Epoch: [57][   34/   34]    Loss 0.000060    Top1 100.000000    Top5 100.000000    
2024-06-06 21:32:43,543 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 21:32:43,544 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 21:32:43,774 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 57]
2024-06-06 21:32:43,775 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 21:32:43,793 - 

2024-06-06 21:32:43,793 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 21:33:16,189 - Epoch: [58][   10/  193]    Overall Loss 0.000015    Objective Loss 0.000015                                        LR 0.000250    Time 3.239333    
2024-06-06 21:33:26,943 - Epoch: [58][   20/  193]    Overall Loss 0.000021    Objective Loss 0.000021                                        LR 0.000250    Time 2.155707    
2024-06-06 21:33:37,500 - Epoch: [58][   30/  193]    Overall Loss 0.000040    Objective Loss 0.000040                                        LR 0.000250    Time 1.788046    
2024-06-06 21:33:48,256 - Epoch: [58][   40/  193]    Overall Loss 0.000034    Objective Loss 0.000034                                        LR 0.000250    Time 1.609233    
2024-06-06 21:33:58,817 - Epoch: [58][   50/  193]    Overall Loss 0.000029    Objective Loss 0.000029                                        LR 0.000250    Time 1.497840    
2024-06-06 21:34:09,260 - Epoch: [58][   60/  193]    Overall Loss 0.000027    Objective Loss 0.000027                                        LR 0.000250    Time 1.421761    
2024-06-06 21:34:19,705 - Epoch: [58][   70/  193]    Overall Loss 0.000027    Objective Loss 0.000027                                        LR 0.000250    Time 1.367441    
2024-06-06 21:34:30,124 - Epoch: [58][   80/  193]    Overall Loss 0.000026    Objective Loss 0.000026                                        LR 0.000250    Time 1.326338    
2024-06-06 21:34:40,875 - Epoch: [58][   90/  193]    Overall Loss 0.000025    Objective Loss 0.000025                                        LR 0.000250    Time 1.298067    
2024-06-06 21:34:51,437 - Epoch: [58][  100/  193]    Overall Loss 0.000024    Objective Loss 0.000024                                        LR 0.000250    Time 1.273612    
2024-06-06 21:35:01,879 - Epoch: [58][  110/  193]    Overall Loss 0.000023    Objective Loss 0.000023                                        LR 0.000250    Time 1.252503    
2024-06-06 21:35:12,270 - Epoch: [58][  120/  193]    Overall Loss 0.000025    Objective Loss 0.000025                                        LR 0.000250    Time 1.234485    
2024-06-06 21:35:22,591 - Epoch: [58][  130/  193]    Overall Loss 0.000023    Objective Loss 0.000023                                        LR 0.000250    Time 1.218636    
2024-06-06 21:35:32,819 - Epoch: [58][  140/  193]    Overall Loss 0.000029    Objective Loss 0.000029                                        LR 0.000250    Time 1.204348    
2024-06-06 21:35:43,242 - Epoch: [58][  150/  193]    Overall Loss 0.000028    Objective Loss 0.000028                                        LR 0.000250    Time 1.193309    
2024-06-06 21:35:53,759 - Epoch: [58][  160/  193]    Overall Loss 0.000028    Objective Loss 0.000028                                        LR 0.000250    Time 1.184285    
2024-06-06 21:36:04,139 - Epoch: [58][  170/  193]    Overall Loss 0.000029    Objective Loss 0.000029                                        LR 0.000250    Time 1.175484    
2024-06-06 21:36:14,641 - Epoch: [58][  180/  193]    Overall Loss 0.000029    Objective Loss 0.000029                                        LR 0.000250    Time 1.168377    
2024-06-06 21:36:24,783 - Epoch: [58][  190/  193]    Overall Loss 0.000028    Objective Loss 0.000028                                        LR 0.000250    Time 1.160112    
2024-06-06 21:36:27,502 - Epoch: [58][  193/  193]    Overall Loss 0.000028    Objective Loss 0.000028    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.155966    
2024-06-06 21:36:28,319 - --- validate (epoch=58)-----------
2024-06-06 21:36:28,319 - 2172 samples (64 per mini-batch)
2024-06-06 21:36:59,210 - Epoch: [58][   10/   34]    Loss 0.000038    Top1 100.000000    Top5 100.000000    
2024-06-06 21:37:05,554 - Epoch: [58][   20/   34]    Loss 0.000038    Top1 100.000000    Top5 100.000000    
2024-06-06 21:37:11,356 - Epoch: [58][   30/   34]    Loss 0.000182    Top1 100.000000    Top5 100.000000    
2024-06-06 21:37:13,409 - Epoch: [58][   34/   34]    Loss 0.000164    Top1 100.000000    Top5 100.000000    
2024-06-06 21:37:14,428 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 21:37:14,429 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 21:37:14,649 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 58]
2024-06-06 21:37:14,649 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 21:37:14,668 - 

2024-06-06 21:37:14,669 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 21:37:46,825 - Epoch: [59][   10/  193]    Overall Loss 0.000018    Objective Loss 0.000018                                        LR 0.000250    Time 3.215482    
2024-06-06 21:37:57,906 - Epoch: [59][   20/  193]    Overall Loss 0.000029    Objective Loss 0.000029                                        LR 0.000250    Time 2.160073    
2024-06-06 21:38:08,789 - Epoch: [59][   30/  193]    Overall Loss 0.000023    Objective Loss 0.000023                                        LR 0.000250    Time 1.801444    
2024-06-06 21:38:19,866 - Epoch: [59][   40/  193]    Overall Loss 0.000021    Objective Loss 0.000021                                        LR 0.000250    Time 1.627293    
2024-06-06 21:38:30,885 - Epoch: [59][   50/  193]    Overall Loss 0.000021    Objective Loss 0.000021                                        LR 0.000250    Time 1.521694    
2024-06-06 21:38:42,014 - Epoch: [59][   60/  193]    Overall Loss 0.000020    Objective Loss 0.000020                                        LR 0.000250    Time 1.453013    
2024-06-06 21:38:53,149 - Epoch: [59][   70/  193]    Overall Loss 0.000023    Objective Loss 0.000023                                        LR 0.000250    Time 1.403987    
2024-06-06 21:39:04,170 - Epoch: [59][   80/  193]    Overall Loss 0.000022    Objective Loss 0.000022                                        LR 0.000250    Time 1.365850    
2024-06-06 21:39:15,004 - Epoch: [59][   90/  193]    Overall Loss 0.000024    Objective Loss 0.000024                                        LR 0.000250    Time 1.334080    
2024-06-06 21:39:25,734 - Epoch: [59][  100/  193]    Overall Loss 0.000023    Objective Loss 0.000023                                        LR 0.000250    Time 1.307677    
2024-06-06 21:39:36,532 - Epoch: [59][  110/  193]    Overall Loss 0.000022    Objective Loss 0.000022                                        LR 0.000250    Time 1.286642    
2024-06-06 21:39:47,324 - Epoch: [59][  120/  193]    Overall Loss 0.000021    Objective Loss 0.000021                                        LR 0.000250    Time 1.269079    
2024-06-06 21:39:58,180 - Epoch: [59][  130/  193]    Overall Loss 0.000021    Objective Loss 0.000021                                        LR 0.000250    Time 1.254712    
2024-06-06 21:40:08,921 - Epoch: [59][  140/  193]    Overall Loss 0.000020    Objective Loss 0.000020                                        LR 0.000250    Time 1.241540    
2024-06-06 21:40:19,682 - Epoch: [59][  150/  193]    Overall Loss 0.000020    Objective Loss 0.000020                                        LR 0.000250    Time 1.230351    
2024-06-06 21:40:30,350 - Epoch: [59][  160/  193]    Overall Loss 0.000048    Objective Loss 0.000048                                        LR 0.000250    Time 1.219892    
2024-06-06 21:40:41,026 - Epoch: [59][  170/  193]    Overall Loss 0.000047    Objective Loss 0.000047                                        LR 0.000250    Time 1.210754    
2024-06-06 21:40:52,288 - Epoch: [59][  180/  193]    Overall Loss 0.000062    Objective Loss 0.000062                                        LR 0.000250    Time 1.205844    
2024-06-06 21:41:02,626 - Epoch: [59][  190/  193]    Overall Loss 0.000059    Objective Loss 0.000059                                        LR 0.000250    Time 1.196518    
2024-06-06 21:41:05,101 - Epoch: [59][  193/  193]    Overall Loss 0.000059    Objective Loss 0.000059    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.190587    
2024-06-06 21:41:05,793 - --- validate (epoch=59)-----------
2024-06-06 21:41:05,793 - 2172 samples (64 per mini-batch)
2024-06-06 21:41:33,347 - Epoch: [59][   10/   34]    Loss 0.000068    Top1 100.000000    Top5 100.000000    
2024-06-06 21:41:39,551 - Epoch: [59][   20/   34]    Loss 0.000128    Top1 100.000000    Top5 100.000000    
2024-06-06 21:41:45,299 - Epoch: [59][   30/   34]    Loss 0.000098    Top1 100.000000    Top5 100.000000    
2024-06-06 21:41:47,465 - Epoch: [59][   34/   34]    Loss 0.000089    Top1 100.000000    Top5 100.000000    
2024-06-06 21:41:48,483 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-06 21:41:48,484 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-06 21:41:48,710 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 59]
2024-06-06 21:41:48,710 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-06 21:41:48,731 - 

2024-06-06 21:41:48,731 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-06 21:42:29,261 - Epoch: [60][   10/  193]    Overall Loss 0.000043    Objective Loss 0.000043                                        LR 0.000250    Time 4.052835    
2024-06-06 21:42:40,157 - Epoch: [60][   20/  193]    Overall Loss 0.000045    Objective Loss 0.000045                                        LR 0.000250    Time 2.569489    
2024-06-06 21:42:50,819 - Epoch: [60][   30/  193]    Overall Loss 0.000034    Objective Loss 0.000034                                        LR 0.000250    Time 2.067177    
2024-06-06 21:43:01,698 - Epoch: [60][   40/  193]    Overall Loss 0.000031    Objective Loss 0.000031                                        LR 0.000250    Time 1.821643    
2024-06-06 21:43:12,193 - Epoch: [60][   50/  193]    Overall Loss 0.000034    Objective Loss 0.000034                                        LR 0.000250    Time 1.666611    
2024-06-06 21:43:23,194 - Epoch: [60][   60/  193]    Overall Loss 0.000031    Objective Loss 0.000031                                        LR 0.000250    Time 1.571740    
2024-06-07 07:13:27,903 - Epoch: [60][   70/  193]    Overall Loss 0.000035    Objective Loss 0.000035                                        LR 0.000250    Time 489.985324    
2024-06-07 07:13:53,394 - Epoch: [60][   80/  193]    Overall Loss 0.000033    Objective Loss 0.000033                                        LR 0.000250    Time 429.055037    
2024-06-07 07:14:09,232 - Epoch: [60][   90/  193]    Overall Loss 0.000032    Objective Loss 0.000032                                        LR 0.000250    Time 381.557450    
2024-06-07 07:14:26,725 - Epoch: [60][  100/  193]    Overall Loss 0.000058    Objective Loss 0.000058                                        LR 0.000250    Time 343.576077    
2024-06-07 07:14:38,796 - Epoch: [60][  110/  193]    Overall Loss 0.000076    Objective Loss 0.000076                                        LR 0.000250    Time 312.451219    
2024-06-07 07:14:51,839 - Epoch: [60][  120/  193]    Overall Loss 0.000079    Objective Loss 0.000079                                        LR 0.000250    Time 286.521909    
2024-06-07 07:15:02,837 - Epoch: [60][  130/  193]    Overall Loss 0.000076    Objective Loss 0.000076                                        LR 0.000250    Time 264.566029    
2024-06-07 07:15:13,513 - Epoch: [60][  140/  193]    Overall Loss 0.000072    Objective Loss 0.000072                                        LR 0.000250    Time 245.744461    
2024-06-07 07:15:24,038 - Epoch: [60][  150/  193]    Overall Loss 0.000067    Objective Loss 0.000067                                        LR 0.000250    Time 229.431409    
2024-06-07 07:15:35,454 - Epoch: [60][  160/  193]    Overall Loss 0.000065    Objective Loss 0.000065                                        LR 0.000250    Time 215.162990    
2024-06-07 07:15:46,968 - Epoch: [60][  170/  193]    Overall Loss 0.000063    Objective Loss 0.000063                                        LR 0.000250    Time 202.573818    
2024-06-07 07:15:58,907 - Epoch: [60][  180/  193]    Overall Loss 0.000061    Objective Loss 0.000061                                        LR 0.000250    Time 191.385703    
2024-06-07 07:16:09,308 - Epoch: [60][  190/  193]    Overall Loss 0.000058    Objective Loss 0.000058                                        LR 0.000250    Time 181.367267    
2024-06-07 07:16:11,836 - Epoch: [60][  193/  193]    Overall Loss 0.000058    Objective Loss 0.000058    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 178.560980    
2024-06-07 07:16:12,623 - --- validate (epoch=60)-----------
2024-06-07 07:16:12,623 - 2172 samples (64 per mini-batch)
2024-06-07 07:16:45,970 - Epoch: [60][   10/   34]    Loss 0.000026    Top1 100.000000    Top5 100.000000    
2024-06-07 07:16:51,874 - Epoch: [60][   20/   34]    Loss 0.000060    Top1 100.000000    Top5 100.000000    
2024-06-07 07:16:57,419 - Epoch: [60][   30/   34]    Loss 0.000045    Top1 100.000000    Top5 100.000000    
2024-06-07 07:16:59,720 - Epoch: [60][   34/   34]    Loss 0.000041    Top1 100.000000    Top5 100.000000    
2024-06-07 07:17:00,805 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 07:17:00,806 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 07:17:01,053 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 60]
2024-06-07 07:17:01,053 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 07:17:01,073 - 

2024-06-07 07:17:01,073 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 07:17:42,746 - Epoch: [61][   10/  193]    Overall Loss 0.000164    Objective Loss 0.000164                                        LR 0.000250    Time 4.167323    
2024-06-07 07:17:55,180 - Epoch: [61][   20/  193]    Overall Loss 0.000098    Objective Loss 0.000098                                        LR 0.000250    Time 2.702410    
2024-06-07 07:18:07,341 - Epoch: [61][   30/  193]    Overall Loss 0.000075    Objective Loss 0.000075                                        LR 0.000250    Time 2.205014    
2024-06-07 07:18:19,190 - Epoch: [61][   40/  193]    Overall Loss 0.000061    Objective Loss 0.000061                                        LR 0.000250    Time 1.948462    
2024-06-07 07:18:31,396 - Epoch: [61][   50/  193]    Overall Loss 0.000053    Objective Loss 0.000053                                        LR 0.000250    Time 1.801736    
2024-06-07 07:18:44,137 - Epoch: [61][   60/  193]    Overall Loss 0.000045    Objective Loss 0.000045                                        LR 0.000250    Time 1.712889    
2024-06-07 07:18:57,387 - Epoch: [61][   70/  193]    Overall Loss 0.000052    Objective Loss 0.000052                                        LR 0.000250    Time 1.656871    
2024-06-07 07:19:10,256 - Epoch: [61][   80/  193]    Overall Loss 0.000050    Objective Loss 0.000050                                        LR 0.000250    Time 1.610017    
2024-06-07 07:19:23,556 - Epoch: [61][   90/  193]    Overall Loss 0.000046    Objective Loss 0.000046                                        LR 0.000250    Time 1.578321    
2024-06-07 07:19:35,127 - Epoch: [61][  100/  193]    Overall Loss 0.000043    Objective Loss 0.000043                                        LR 0.000250    Time 1.535661    
2024-06-07 07:19:45,872 - Epoch: [61][  110/  193]    Overall Loss 0.000040    Objective Loss 0.000040                                        LR 0.000250    Time 1.493280    
2024-06-07 07:19:56,480 - Epoch: [61][  120/  193]    Overall Loss 0.000044    Objective Loss 0.000044                                        LR 0.000250    Time 1.456896    
2024-06-07 07:20:07,242 - Epoch: [61][  130/  193]    Overall Loss 0.000041    Objective Loss 0.000041                                        LR 0.000250    Time 1.427325    
2024-06-07 07:20:17,865 - Epoch: [61][  140/  193]    Overall Loss 0.000040    Objective Loss 0.000040                                        LR 0.000250    Time 1.400940    
2024-06-07 07:20:28,335 - Epoch: [61][  150/  193]    Overall Loss 0.000038    Objective Loss 0.000038                                        LR 0.000250    Time 1.377057    
2024-06-07 07:20:39,627 - Epoch: [61][  160/  193]    Overall Loss 0.000041    Objective Loss 0.000041                                        LR 0.000250    Time 1.361412    
2024-06-07 07:20:51,580 - Epoch: [61][  170/  193]    Overall Loss 0.000041    Objective Loss 0.000041                                        LR 0.000250    Time 1.351367    
2024-06-07 07:21:03,917 - Epoch: [61][  180/  193]    Overall Loss 0.000044    Objective Loss 0.000044                                        LR 0.000250    Time 1.344625    
2024-06-07 07:21:15,285 - Epoch: [61][  190/  193]    Overall Loss 0.000043    Objective Loss 0.000043                                        LR 0.000250    Time 1.333295    
2024-06-07 07:21:17,799 - Epoch: [61][  193/  193]    Overall Loss 0.000042    Objective Loss 0.000042    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.325451    
2024-06-07 07:21:18,486 - --- validate (epoch=61)-----------
2024-06-07 07:21:18,486 - 2172 samples (64 per mini-batch)
2024-06-07 07:21:55,160 - Epoch: [61][   10/   34]    Loss 0.000026    Top1 100.000000    Top5 100.000000    
2024-06-07 07:22:00,960 - Epoch: [61][   20/   34]    Loss 0.000048    Top1 100.000000    Top5 100.000000    
2024-06-07 07:22:06,479 - Epoch: [61][   30/   34]    Loss 0.000042    Top1 100.000000    Top5 100.000000    
2024-06-07 07:22:08,480 - Epoch: [61][   34/   34]    Loss 0.000038    Top1 100.000000    Top5 100.000000    
2024-06-07 07:22:09,446 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 07:22:09,447 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 07:22:09,658 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 61]
2024-06-07 07:22:09,658 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 07:22:09,674 - 

2024-06-07 07:22:09,674 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 07:22:40,073 - Epoch: [62][   10/  193]    Overall Loss 0.000029    Objective Loss 0.000029                                        LR 0.000250    Time 3.039789    
2024-06-07 07:22:51,342 - Epoch: [62][   20/  193]    Overall Loss 0.000046    Objective Loss 0.000046                                        LR 0.000250    Time 2.082070    
2024-06-07 07:23:02,664 - Epoch: [62][   30/  193]    Overall Loss 0.000038    Objective Loss 0.000038                                        LR 0.000250    Time 1.764169    
2024-06-07 07:23:14,007 - Epoch: [62][   40/  193]    Overall Loss 0.000032    Objective Loss 0.000032                                        LR 0.000250    Time 1.605878    
2024-06-07 07:23:24,693 - Epoch: [62][   50/  193]    Overall Loss 0.000028    Objective Loss 0.000028                                        LR 0.000250    Time 1.497568    
2024-06-07 07:23:35,019 - Epoch: [62][   60/  193]    Overall Loss 0.000025    Objective Loss 0.000025                                        LR 0.000250    Time 1.419572    
2024-06-07 07:23:45,021 - Epoch: [62][   70/  193]    Overall Loss 0.000024    Objective Loss 0.000024                                        LR 0.000250    Time 1.359220    
2024-06-07 07:23:55,055 - Epoch: [62][   80/  193]    Overall Loss 0.000023    Objective Loss 0.000023                                        LR 0.000250    Time 1.314435    
2024-06-07 07:24:04,985 - Epoch: [62][   90/  193]    Overall Loss 0.000025    Objective Loss 0.000025                                        LR 0.000250    Time 1.278365    
2024-06-07 07:24:14,982 - Epoch: [62][  100/  193]    Overall Loss 0.000024    Objective Loss 0.000024                                        LR 0.000250    Time 1.250171    
2024-06-07 07:24:24,954 - Epoch: [62][  110/  193]    Overall Loss 0.000024    Objective Loss 0.000024                                        LR 0.000250    Time 1.226907    
2024-06-07 07:24:34,896 - Epoch: [62][  120/  193]    Overall Loss 0.000023    Objective Loss 0.000023                                        LR 0.000250    Time 1.207291    
2024-06-07 07:24:44,883 - Epoch: [62][  130/  193]    Overall Loss 0.000023    Objective Loss 0.000023                                        LR 0.000250    Time 1.190996    
2024-06-07 07:24:54,842 - Epoch: [62][  140/  193]    Overall Loss 0.000023    Objective Loss 0.000023                                        LR 0.000250    Time 1.176842    
2024-06-07 07:25:04,842 - Epoch: [62][  150/  193]    Overall Loss 0.000022    Objective Loss 0.000022                                        LR 0.000250    Time 1.164848    
2024-06-07 07:25:14,893 - Epoch: [62][  160/  193]    Overall Loss 0.000021    Objective Loss 0.000021                                        LR 0.000250    Time 1.154678    
2024-06-07 07:25:24,910 - Epoch: [62][  170/  193]    Overall Loss 0.000022    Objective Loss 0.000022                                        LR 0.000250    Time 1.145544    
2024-06-07 07:25:35,241 - Epoch: [62][  180/  193]    Overall Loss 0.000022    Objective Loss 0.000022                                        LR 0.000250    Time 1.139141    
2024-06-07 07:25:45,087 - Epoch: [62][  190/  193]    Overall Loss 0.000021    Objective Loss 0.000021                                        LR 0.000250    Time 1.130865    
2024-06-07 07:25:47,450 - Epoch: [62][  193/  193]    Overall Loss 0.000022    Objective Loss 0.000022    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.125393    
2024-06-07 07:25:48,121 - --- validate (epoch=62)-----------
2024-06-07 07:25:48,121 - 2172 samples (64 per mini-batch)
2024-06-07 07:26:13,347 - Epoch: [62][   10/   34]    Loss 0.000005    Top1 100.000000    Top5 100.000000    
2024-06-07 07:26:18,882 - Epoch: [62][   20/   34]    Loss 0.000020    Top1 100.000000    Top5 100.000000    
2024-06-07 07:26:24,238 - Epoch: [62][   30/   34]    Loss 0.000017    Top1 100.000000    Top5 100.000000    
2024-06-07 07:26:26,232 - Epoch: [62][   34/   34]    Loss 0.000016    Top1 100.000000    Top5 100.000000    
2024-06-07 07:26:27,183 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 07:26:27,184 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 07:26:27,390 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 62]
2024-06-07 07:26:27,391 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 07:26:27,408 - 

2024-06-07 07:26:27,408 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 07:26:58,368 - Epoch: [63][   10/  193]    Overall Loss 0.000010    Objective Loss 0.000010                                        LR 0.000250    Time 3.095864    
2024-06-07 07:27:08,793 - Epoch: [63][   20/  193]    Overall Loss 0.000029    Objective Loss 0.000029                                        LR 0.000250    Time 2.067682    
2024-06-07 07:27:19,313 - Epoch: [63][   30/  193]    Overall Loss 0.000025    Objective Loss 0.000025                                        LR 0.000250    Time 1.728279    
2024-06-07 07:27:30,109 - Epoch: [63][   40/  193]    Overall Loss 0.000022    Objective Loss 0.000022                                        LR 0.000250    Time 1.565050    
2024-06-07 07:27:40,544 - Epoch: [63][   50/  193]    Overall Loss 0.000021    Objective Loss 0.000021                                        LR 0.000250    Time 1.459900    
2024-06-07 07:27:51,082 - Epoch: [63][   60/  193]    Overall Loss 0.000019    Objective Loss 0.000019                                        LR 0.000250    Time 1.391682    
2024-06-07 07:28:01,679 - Epoch: [63][   70/  193]    Overall Loss 0.000022    Objective Loss 0.000022                                        LR 0.000250    Time 1.343689    
2024-06-07 07:28:12,098 - Epoch: [63][   80/  193]    Overall Loss 0.000023    Objective Loss 0.000023                                        LR 0.000250    Time 1.305676    
2024-06-07 07:28:22,636 - Epoch: [63][   90/  193]    Overall Loss 0.000022    Objective Loss 0.000022                                        LR 0.000250    Time 1.277306    
2024-06-07 07:28:33,400 - Epoch: [63][  100/  193]    Overall Loss 0.000037    Objective Loss 0.000037                                        LR 0.000250    Time 1.256800    
2024-06-07 07:28:44,073 - Epoch: [63][  110/  193]    Overall Loss 0.000034    Objective Loss 0.000034                                        LR 0.000250    Time 1.239216    
2024-06-07 07:28:54,770 - Epoch: [63][  120/  193]    Overall Loss 0.000032    Objective Loss 0.000032                                        LR 0.000250    Time 1.224710    
2024-06-07 07:29:05,493 - Epoch: [63][  130/  193]    Overall Loss 0.000035    Objective Loss 0.000035                                        LR 0.000250    Time 1.212698    
2024-06-07 07:29:16,989 - Epoch: [63][  140/  193]    Overall Loss 0.000033    Objective Loss 0.000033                                        LR 0.000250    Time 1.207961    
2024-06-07 07:29:29,324 - Epoch: [63][  150/  193]    Overall Loss 0.000033    Objective Loss 0.000033                                        LR 0.000250    Time 1.209273    
2024-06-07 07:29:40,200 - Epoch: [63][  160/  193]    Overall Loss 0.000033    Objective Loss 0.000033                                        LR 0.000250    Time 1.201449    
2024-06-07 07:29:51,233 - Epoch: [63][  170/  193]    Overall Loss 0.000032    Objective Loss 0.000032                                        LR 0.000250    Time 1.195454    
2024-06-07 07:30:02,307 - Epoch: [63][  180/  193]    Overall Loss 0.000034    Objective Loss 0.000034                                        LR 0.000250    Time 1.190387    
2024-06-07 07:30:13,046 - Epoch: [63][  190/  193]    Overall Loss 0.000035    Objective Loss 0.000035                                        LR 0.000250    Time 1.184077    
2024-06-07 07:30:16,024 - Epoch: [63][  193/  193]    Overall Loss 0.000035    Objective Loss 0.000035    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.180908    
2024-06-07 07:30:16,812 - --- validate (epoch=63)-----------
2024-06-07 07:30:16,812 - 2172 samples (64 per mini-batch)
2024-06-07 07:30:43,684 - Epoch: [63][   10/   34]    Loss 0.000010    Top1 100.000000    Top5 100.000000    
2024-06-07 07:30:49,590 - Epoch: [63][   20/   34]    Loss 0.000039    Top1 100.000000    Top5 100.000000    
2024-06-07 07:30:55,458 - Epoch: [63][   30/   34]    Loss 0.000031    Top1 100.000000    Top5 100.000000    
2024-06-07 07:30:57,570 - Epoch: [63][   34/   34]    Loss 0.000028    Top1 100.000000    Top5 100.000000    
2024-06-07 07:30:58,528 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 07:30:58,528 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 07:30:58,766 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 63]
2024-06-07 07:30:58,766 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 07:30:58,784 - 

2024-06-07 07:30:58,784 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 07:31:30,418 - Epoch: [64][   10/  193]    Overall Loss 0.000052    Objective Loss 0.000052                                        LR 0.000250    Time 3.163322    
2024-06-07 07:31:41,166 - Epoch: [64][   20/  193]    Overall Loss 0.000067    Objective Loss 0.000067                                        LR 0.000250    Time 2.117697    
2024-06-07 07:31:51,686 - Epoch: [64][   30/  193]    Overall Loss 0.000063    Objective Loss 0.000063                                        LR 0.000250    Time 1.761190    
2024-06-07 07:32:02,428 - Epoch: [64][   40/  193]    Overall Loss 0.000050    Objective Loss 0.000050                                        LR 0.000250    Time 1.588764    
2024-06-07 07:32:12,947 - Epoch: [64][   50/  193]    Overall Loss 0.000045    Objective Loss 0.000045                                        LR 0.000250    Time 1.480755    
2024-06-07 07:32:23,537 - Epoch: [64][   60/  193]    Overall Loss 0.000041    Objective Loss 0.000041                                        LR 0.000250    Time 1.409931    
2024-06-07 07:32:34,295 - Epoch: [64][   70/  193]    Overall Loss 0.000038    Objective Loss 0.000038                                        LR 0.000250    Time 1.361760    
2024-06-07 07:32:44,812 - Epoch: [64][   80/  193]    Overall Loss 0.000035    Objective Loss 0.000035                                        LR 0.000250    Time 1.322647    
2024-06-07 07:32:55,438 - Epoch: [64][   90/  193]    Overall Loss 0.000034    Objective Loss 0.000034                                        LR 0.000250    Time 1.293434    
2024-06-07 07:33:06,154 - Epoch: [64][  100/  193]    Overall Loss 0.000032    Objective Loss 0.000032                                        LR 0.000250    Time 1.270850    
2024-06-07 07:33:16,795 - Epoch: [64][  110/  193]    Overall Loss 0.000030    Objective Loss 0.000030                                        LR 0.000250    Time 1.251780    
2024-06-07 07:33:27,477 - Epoch: [64][  120/  193]    Overall Loss 0.000028    Objective Loss 0.000028                                        LR 0.000250    Time 1.236134    
2024-06-07 07:33:38,268 - Epoch: [64][  130/  193]    Overall Loss 0.000028    Objective Loss 0.000028                                        LR 0.000250    Time 1.223788    
2024-06-07 07:33:48,823 - Epoch: [64][  140/  193]    Overall Loss 0.000026    Objective Loss 0.000026                                        LR 0.000250    Time 1.211554    
2024-06-07 07:33:59,463 - Epoch: [64][  150/  193]    Overall Loss 0.000025    Objective Loss 0.000025                                        LR 0.000250    Time 1.201420    
2024-06-07 07:34:10,298 - Epoch: [64][  160/  193]    Overall Loss 0.000024    Objective Loss 0.000024                                        LR 0.000250    Time 1.193786    
2024-06-07 07:34:21,808 - Epoch: [64][  170/  193]    Overall Loss 0.000023    Objective Loss 0.000023                                        LR 0.000250    Time 1.191093    
2024-06-07 07:34:32,849 - Epoch: [64][  180/  193]    Overall Loss 0.000023    Objective Loss 0.000023                                        LR 0.000250    Time 1.186051    
2024-06-07 07:34:43,205 - Epoch: [64][  190/  193]    Overall Loss 0.000024    Objective Loss 0.000024                                        LR 0.000250    Time 1.177968    
2024-06-07 07:34:45,653 - Epoch: [64][  193/  193]    Overall Loss 0.000024    Objective Loss 0.000024    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.172188    
2024-06-07 07:34:46,328 - --- validate (epoch=64)-----------
2024-06-07 07:34:46,328 - 2172 samples (64 per mini-batch)
2024-06-07 07:35:13,044 - Epoch: [64][   10/   34]    Loss 0.000016    Top1 100.000000    Top5 100.000000    
2024-06-07 07:35:18,974 - Epoch: [64][   20/   34]    Loss 0.000012    Top1 100.000000    Top5 100.000000    
2024-06-07 07:35:24,559 - Epoch: [64][   30/   34]    Loss 0.000011    Top1 100.000000    Top5 100.000000    
2024-06-07 07:35:26,628 - Epoch: [64][   34/   34]    Loss 0.000016    Top1 100.000000    Top5 100.000000    
2024-06-07 07:35:27,630 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 07:35:27,630 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 07:35:27,836 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 64]
2024-06-07 07:35:27,836 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 07:35:27,856 - 

2024-06-07 07:35:27,856 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 07:35:59,577 - Epoch: [65][   10/  193]    Overall Loss 0.000055    Objective Loss 0.000055                                        LR 0.000250    Time 3.172044    
2024-06-07 07:36:10,292 - Epoch: [65][   20/  193]    Overall Loss 0.000034    Objective Loss 0.000034                                        LR 0.000250    Time 2.120301    
2024-06-07 07:36:20,888 - Epoch: [65][   30/  193]    Overall Loss 0.000025    Objective Loss 0.000025                                        LR 0.000250    Time 1.765511    
2024-06-07 07:36:31,614 - Epoch: [65][   40/  193]    Overall Loss 0.000025    Objective Loss 0.000025                                        LR 0.000250    Time 1.591521    
2024-06-07 07:36:42,220 - Epoch: [65][   50/  193]    Overall Loss 0.000021    Objective Loss 0.000021                                        LR 0.000250    Time 1.484654    
2024-06-07 07:36:52,823 - Epoch: [65][   60/  193]    Overall Loss 0.000019    Objective Loss 0.000019                                        LR 0.000250    Time 1.413456    
2024-06-07 07:37:03,583 - Epoch: [65][   70/  193]    Overall Loss 0.000017    Objective Loss 0.000017                                        LR 0.000250    Time 1.364739    
2024-06-07 07:37:14,187 - Epoch: [65][   80/  193]    Overall Loss 0.000017    Objective Loss 0.000017                                        LR 0.000250    Time 1.326353    
2024-06-07 07:37:24,859 - Epoch: [65][   90/  193]    Overall Loss 0.000018    Objective Loss 0.000018                                        LR 0.000250    Time 1.297225    
2024-06-07 07:37:35,446 - Epoch: [65][  100/  193]    Overall Loss 0.000017    Objective Loss 0.000017                                        LR 0.000250    Time 1.273113    
2024-06-07 07:37:46,074 - Epoch: [65][  110/  193]    Overall Loss 0.000016    Objective Loss 0.000016                                        LR 0.000250    Time 1.253795    
2024-06-07 07:37:57,220 - Epoch: [65][  120/  193]    Overall Loss 0.000015    Objective Loss 0.000015                                        LR 0.000250    Time 1.241916    
2024-06-07 07:38:07,937 - Epoch: [65][  130/  193]    Overall Loss 0.000015    Objective Loss 0.000015                                        LR 0.000250    Time 1.228570    
2024-06-07 07:38:18,634 - Epoch: [65][  140/  193]    Overall Loss 0.000015    Objective Loss 0.000015                                        LR 0.000250    Time 1.216970    
2024-06-07 07:38:29,725 - Epoch: [65][  150/  193]    Overall Loss 0.000016    Objective Loss 0.000016                                        LR 0.000250    Time 1.209560    
2024-06-07 07:38:40,706 - Epoch: [65][  160/  193]    Overall Loss 0.000016    Objective Loss 0.000016                                        LR 0.000250    Time 1.202380    
2024-06-07 07:38:51,484 - Epoch: [65][  170/  193]    Overall Loss 0.000016    Objective Loss 0.000016                                        LR 0.000250    Time 1.194861    
2024-06-07 07:39:02,477 - Epoch: [65][  180/  193]    Overall Loss 0.000015    Objective Loss 0.000015                                        LR 0.000250    Time 1.189363    
2024-06-07 07:39:13,324 - Epoch: [65][  190/  193]    Overall Loss 0.000016    Objective Loss 0.000016                                        LR 0.000250    Time 1.183714    
2024-06-07 07:39:15,974 - Epoch: [65][  193/  193]    Overall Loss 0.000015    Objective Loss 0.000015    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.178893    
2024-06-07 07:39:16,925 - --- validate (epoch=65)-----------
2024-06-07 07:39:16,926 - 2172 samples (64 per mini-batch)
2024-06-07 07:39:45,527 - Epoch: [65][   10/   34]    Loss 0.000025    Top1 100.000000    Top5 100.000000    
2024-06-07 07:39:51,743 - Epoch: [65][   20/   34]    Loss 0.000024    Top1 100.000000    Top5 100.000000    
2024-06-07 07:39:57,924 - Epoch: [65][   30/   34]    Loss 0.000029    Top1 100.000000    Top5 100.000000    
2024-06-07 07:40:00,113 - Epoch: [65][   34/   34]    Loss 0.000027    Top1 100.000000    Top5 100.000000    
2024-06-07 07:40:01,171 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 07:40:01,172 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 07:40:01,467 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 65]
2024-06-07 07:40:01,468 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 07:40:01,491 - 

2024-06-07 07:40:01,491 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 07:40:34,281 - Epoch: [66][   10/  193]    Overall Loss 0.000102    Objective Loss 0.000102                                        LR 0.000250    Time 3.279004    
2024-06-07 07:40:45,417 - Epoch: [66][   20/  193]    Overall Loss 0.000055    Objective Loss 0.000055                                        LR 0.000250    Time 2.194475    
2024-06-07 07:40:56,140 - Epoch: [66][   30/  193]    Overall Loss 0.000040    Objective Loss 0.000040                                        LR 0.000250    Time 1.819184    
2024-06-07 07:41:07,058 - Epoch: [66][   40/  193]    Overall Loss 0.000034    Objective Loss 0.000034                                        LR 0.000250    Time 1.636495    
2024-06-07 07:41:17,787 - Epoch: [66][   50/  193]    Overall Loss 0.000030    Objective Loss 0.000030                                        LR 0.000250    Time 1.523125    
2024-06-07 07:41:28,504 - Epoch: [66][   60/  193]    Overall Loss 0.000028    Objective Loss 0.000028                                        LR 0.000250    Time 1.447141    
2024-06-07 07:41:39,094 - Epoch: [66][   70/  193]    Overall Loss 0.000033    Objective Loss 0.000033                                        LR 0.000250    Time 1.391251    
2024-06-07 07:41:49,622 - Epoch: [66][   80/  193]    Overall Loss 0.000031    Objective Loss 0.000031                                        LR 0.000250    Time 1.348498    
2024-06-07 07:42:00,328 - Epoch: [66][   90/  193]    Overall Loss 0.000029    Objective Loss 0.000029                                        LR 0.000250    Time 1.317297    
2024-06-07 07:42:10,835 - Epoch: [66][  100/  193]    Overall Loss 0.000027    Objective Loss 0.000027                                        LR 0.000250    Time 1.290303    
2024-06-07 07:42:21,513 - Epoch: [66][  110/  193]    Overall Loss 0.000025    Objective Loss 0.000025                                        LR 0.000250    Time 1.269711    
2024-06-07 07:42:32,520 - Epoch: [66][  120/  193]    Overall Loss 0.000023    Objective Loss 0.000023                                        LR 0.000250    Time 1.255334    
2024-06-07 07:42:43,384 - Epoch: [66][  130/  193]    Overall Loss 0.000022    Objective Loss 0.000022                                        LR 0.000250    Time 1.242099    
2024-06-07 07:42:54,160 - Epoch: [66][  140/  193]    Overall Loss 0.000021    Objective Loss 0.000021                                        LR 0.000250    Time 1.230153    
2024-06-07 07:43:05,193 - Epoch: [66][  150/  193]    Overall Loss 0.000020    Objective Loss 0.000020                                        LR 0.000250    Time 1.221499    
2024-06-07 07:43:16,145 - Epoch: [66][  160/  193]    Overall Loss 0.000022    Objective Loss 0.000022                                        LR 0.000250    Time 1.213445    
2024-06-07 07:43:27,340 - Epoch: [66][  170/  193]    Overall Loss 0.000021    Objective Loss 0.000021                                        LR 0.000250    Time 1.207746    
2024-06-07 07:43:38,602 - Epoch: [66][  180/  193]    Overall Loss 0.000020    Objective Loss 0.000020                                        LR 0.000250    Time 1.203031    
2024-06-07 07:43:49,468 - Epoch: [66][  190/  193]    Overall Loss 0.000020    Objective Loss 0.000020                                        LR 0.000250    Time 1.196792    
2024-06-07 07:43:52,098 - Epoch: [66][  193/  193]    Overall Loss 0.000021    Objective Loss 0.000021    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.191624    
2024-06-07 07:43:52,787 - --- validate (epoch=66)-----------
2024-06-07 07:43:52,787 - 2172 samples (64 per mini-batch)
2024-06-07 07:44:21,106 - Epoch: [66][   10/   34]    Loss 0.000007    Top1 100.000000    Top5 100.000000    
2024-06-07 07:44:27,591 - Epoch: [66][   20/   34]    Loss 0.000008    Top1 100.000000    Top5 100.000000    
2024-06-07 07:44:33,571 - Epoch: [66][   30/   34]    Loss 0.000008    Top1 100.000000    Top5 100.000000    
2024-06-07 07:44:35,835 - Epoch: [66][   34/   34]    Loss 0.000008    Top1 100.000000    Top5 100.000000    
2024-06-07 07:44:36,875 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 07:44:36,875 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 07:44:37,100 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 66]
2024-06-07 07:44:37,101 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 07:44:37,121 - 

2024-06-07 07:44:37,121 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 07:45:10,133 - Epoch: [67][   10/  193]    Overall Loss 0.000013    Objective Loss 0.000013                                        LR 0.000250    Time 3.301035    
2024-06-07 07:45:21,437 - Epoch: [67][   20/  193]    Overall Loss 0.000023    Objective Loss 0.000023                                        LR 0.000250    Time 2.214192    
2024-06-07 07:45:32,898 - Epoch: [67][   30/  193]    Overall Loss 0.000021    Objective Loss 0.000021                                        LR 0.000250    Time 1.857166    
2024-06-07 07:45:44,077 - Epoch: [67][   40/  193]    Overall Loss 0.000021    Objective Loss 0.000021                                        LR 0.000250    Time 1.671415    
2024-06-07 07:45:54,888 - Epoch: [67][   50/  193]    Overall Loss 0.000020    Objective Loss 0.000020                                        LR 0.000250    Time 1.552554    
2024-06-07 07:46:06,042 - Epoch: [67][   60/  193]    Overall Loss 0.000018    Objective Loss 0.000018                                        LR 0.000250    Time 1.479108    
2024-06-07 07:46:17,163 - Epoch: [67][   70/  193]    Overall Loss 0.000019    Objective Loss 0.000019                                        LR 0.000250    Time 1.426332    
2024-06-07 07:46:28,308 - Epoch: [67][   80/  193]    Overall Loss 0.000018    Objective Loss 0.000018                                        LR 0.000250    Time 1.386935    
2024-06-07 07:46:39,429 - Epoch: [67][   90/  193]    Overall Loss 0.000019    Objective Loss 0.000019                                        LR 0.000250    Time 1.356050    
2024-06-07 07:46:50,609 - Epoch: [67][  100/  193]    Overall Loss 0.000019    Objective Loss 0.000019                                        LR 0.000250    Time 1.331944    
2024-06-07 07:47:01,827 - Epoch: [67][  110/  193]    Overall Loss 0.000018    Objective Loss 0.000018                                        LR 0.000250    Time 1.312485    
2024-06-07 07:47:13,097 - Epoch: [67][  120/  193]    Overall Loss 0.000023    Objective Loss 0.000023                                        LR 0.000250    Time 1.296545    
2024-06-07 07:47:24,314 - Epoch: [67][  130/  193]    Overall Loss 0.000022    Objective Loss 0.000022                                        LR 0.000250    Time 1.282839    
2024-06-07 07:47:35,624 - Epoch: [67][  140/  193]    Overall Loss 0.000021    Objective Loss 0.000021                                        LR 0.000250    Time 1.271763    
2024-06-07 07:47:47,028 - Epoch: [67][  150/  193]    Overall Loss 0.000023    Objective Loss 0.000023                                        LR 0.000250    Time 1.262769    
2024-06-07 07:47:58,475 - Epoch: [67][  160/  193]    Overall Loss 0.000026    Objective Loss 0.000026                                        LR 0.000250    Time 1.255136    
2024-06-07 07:48:10,164 - Epoch: [67][  170/  193]    Overall Loss 0.000025    Objective Loss 0.000025                                        LR 0.000250    Time 1.249879    
2024-06-07 07:48:21,900 - Epoch: [67][  180/  193]    Overall Loss 0.000024    Objective Loss 0.000024                                        LR 0.000250    Time 1.245431    
2024-06-07 07:48:33,304 - Epoch: [67][  190/  193]    Overall Loss 0.000024    Objective Loss 0.000024                                        LR 0.000250    Time 1.239678    
2024-06-07 07:48:36,088 - Epoch: [67][  193/  193]    Overall Loss 0.000024    Objective Loss 0.000024    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.234663    
2024-06-07 07:48:36,819 - --- validate (epoch=67)-----------
2024-06-07 07:48:36,819 - 2172 samples (64 per mini-batch)
2024-06-07 07:49:15,671 - Epoch: [67][   10/   34]    Loss 0.000009    Top1 100.000000    Top5 100.000000    
2024-06-07 07:49:22,286 - Epoch: [67][   20/   34]    Loss 0.000021    Top1 100.000000    Top5 100.000000    
2024-06-07 07:49:28,691 - Epoch: [67][   30/   34]    Loss 0.000017    Top1 100.000000    Top5 100.000000    
2024-06-07 07:49:31,031 - Epoch: [67][   34/   34]    Loss 0.000017    Top1 100.000000    Top5 100.000000    
2024-06-07 07:49:32,106 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 07:49:32,107 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 07:49:32,377 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 67]
2024-06-07 07:49:32,377 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 07:49:32,395 - 

2024-06-07 07:49:32,396 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 07:50:06,436 - Epoch: [68][   10/  193]    Overall Loss 0.000006    Objective Loss 0.000006                                        LR 0.000250    Time 3.403818    
2024-06-07 07:50:18,362 - Epoch: [68][   20/  193]    Overall Loss 0.000023    Objective Loss 0.000023                                        LR 0.000250    Time 2.296432    
2024-06-07 07:50:30,415 - Epoch: [68][   30/  193]    Overall Loss 0.000020    Objective Loss 0.000020                                        LR 0.000250    Time 1.931162    
2024-06-07 07:50:42,215 - Epoch: [68][   40/  193]    Overall Loss 0.000016    Objective Loss 0.000016                                        LR 0.000250    Time 1.742324    
2024-06-07 07:50:53,967 - Epoch: [68][   50/  193]    Overall Loss 0.000014    Objective Loss 0.000014                                        LR 0.000250    Time 1.628169    
2024-06-07 07:51:05,776 - Epoch: [68][   60/  193]    Overall Loss 0.000013    Objective Loss 0.000013                                        LR 0.000250    Time 1.553109    
2024-06-07 07:51:17,375 - Epoch: [68][   70/  193]    Overall Loss 0.000014    Objective Loss 0.000014                                        LR 0.000250    Time 1.496444    
2024-06-07 07:51:28,648 - Epoch: [68][   80/  193]    Overall Loss 0.000015    Objective Loss 0.000015                                        LR 0.000250    Time 1.449857    
2024-06-07 07:51:39,949 - Epoch: [68][   90/  193]    Overall Loss 0.000019    Objective Loss 0.000019                                        LR 0.000250    Time 1.413919    
2024-06-07 07:51:51,347 - Epoch: [68][  100/  193]    Overall Loss 0.000018    Objective Loss 0.000018                                        LR 0.000250    Time 1.386172    
2024-06-07 07:52:02,704 - Epoch: [68][  110/  193]    Overall Loss 0.000017    Objective Loss 0.000017                                        LR 0.000250    Time 1.363024    
2024-06-07 07:52:14,059 - Epoch: [68][  120/  193]    Overall Loss 0.000017    Objective Loss 0.000017                                        LR 0.000250    Time 1.343794    
2024-06-07 07:52:25,306 - Epoch: [68][  130/  193]    Overall Loss 0.000016    Objective Loss 0.000016                                        LR 0.000250    Time 1.326678    
2024-06-07 07:52:36,374 - Epoch: [68][  140/  193]    Overall Loss 0.000016    Objective Loss 0.000016                                        LR 0.000250    Time 1.310678    
2024-06-07 07:52:47,691 - Epoch: [68][  150/  193]    Overall Loss 0.000015    Objective Loss 0.000015                                        LR 0.000250    Time 1.298443    
2024-06-07 07:52:58,913 - Epoch: [68][  160/  193]    Overall Loss 0.000015    Objective Loss 0.000015                                        LR 0.000250    Time 1.287216    
2024-06-07 07:53:10,421 - Epoch: [68][  170/  193]    Overall Loss 0.000014    Objective Loss 0.000014                                        LR 0.000250    Time 1.279003    
2024-06-07 07:53:21,939 - Epoch: [68][  180/  193]    Overall Loss 0.000018    Objective Loss 0.000018                                        LR 0.000250    Time 1.271704    
2024-06-07 07:53:33,132 - Epoch: [68][  190/  193]    Overall Loss 0.000017    Objective Loss 0.000017                                        LR 0.000250    Time 1.263518    
2024-06-07 07:53:35,832 - Epoch: [68][  193/  193]    Overall Loss 0.000017    Objective Loss 0.000017    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.257647    
2024-06-07 07:53:36,566 - --- validate (epoch=68)-----------
2024-06-07 07:53:36,567 - 2172 samples (64 per mini-batch)
2024-06-07 07:54:05,692 - Epoch: [68][   10/   34]    Loss 0.000018    Top1 100.000000    Top5 100.000000    
2024-06-07 07:54:12,653 - Epoch: [68][   20/   34]    Loss 0.000016    Top1 100.000000    Top5 100.000000    
2024-06-07 07:54:18,984 - Epoch: [68][   30/   34]    Loss 0.000138    Top1 100.000000    Top5 100.000000    
2024-06-07 07:54:21,329 - Epoch: [68][   34/   34]    Loss 0.000123    Top1 100.000000    Top5 100.000000    
2024-06-07 07:54:22,437 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 07:54:22,437 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 07:54:22,687 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 68]
2024-06-07 07:54:22,688 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 07:54:22,705 - 

2024-06-07 07:54:22,706 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 07:54:57,240 - Epoch: [69][   10/  193]    Overall Loss 0.000016    Objective Loss 0.000016                                        LR 0.000250    Time 3.453359    
2024-06-07 07:55:09,303 - Epoch: [69][   20/  193]    Overall Loss 0.000024    Objective Loss 0.000024                                        LR 0.000250    Time 2.328198    
2024-06-07 07:55:20,779 - Epoch: [69][   30/  193]    Overall Loss 0.000021    Objective Loss 0.000021                                        LR 0.000250    Time 1.933342    
2024-06-07 07:55:32,734 - Epoch: [69][   40/  193]    Overall Loss 0.000026    Objective Loss 0.000026                                        LR 0.000250    Time 1.747906    
2024-06-07 07:55:44,086 - Epoch: [69][   50/  193]    Overall Loss 0.000023    Objective Loss 0.000023                                        LR 0.000250    Time 1.624504    
2024-06-07 07:55:55,371 - Epoch: [69][   60/  193]    Overall Loss 0.000024    Objective Loss 0.000024                                        LR 0.000250    Time 1.541238    
2024-06-07 07:56:06,692 - Epoch: [69][   70/  193]    Overall Loss 0.000193    Objective Loss 0.000193                                        LR 0.000250    Time 1.482422    
2024-06-07 07:56:18,079 - Epoch: [69][   80/  193]    Overall Loss 0.000284    Objective Loss 0.000284                                        LR 0.000250    Time 1.439027    
2024-06-07 07:56:29,318 - Epoch: [69][   90/  193]    Overall Loss 0.000295    Objective Loss 0.000295                                        LR 0.000250    Time 1.403649    
2024-06-07 07:56:40,542 - Epoch: [69][  100/  193]    Overall Loss 0.000273    Objective Loss 0.000273                                        LR 0.000250    Time 1.375219    
2024-06-07 07:56:52,486 - Epoch: [69][  110/  193]    Overall Loss 0.000277    Objective Loss 0.000277                                        LR 0.000250    Time 1.358395    
2024-06-07 07:57:03,985 - Epoch: [69][  120/  193]    Overall Loss 0.000257    Objective Loss 0.000257                                        LR 0.000250    Time 1.340759    
2024-06-07 07:57:15,793 - Epoch: [69][  130/  193]    Overall Loss 0.000242    Objective Loss 0.000242                                        LR 0.000250    Time 1.328139    
2024-06-07 07:57:27,378 - Epoch: [69][  140/  193]    Overall Loss 0.000231    Objective Loss 0.000231                                        LR 0.000250    Time 1.315731    
2024-06-07 07:57:39,389 - Epoch: [69][  150/  193]    Overall Loss 0.000219    Objective Loss 0.000219                                        LR 0.000250    Time 1.307849    
2024-06-07 07:57:51,281 - Epoch: [69][  160/  193]    Overall Loss 0.000210    Objective Loss 0.000210                                        LR 0.000250    Time 1.300126    
2024-06-07 07:58:03,187 - Epoch: [69][  170/  193]    Overall Loss 0.000199    Objective Loss 0.000199                                        LR 0.000250    Time 1.293460    
2024-06-07 07:58:15,421 - Epoch: [69][  180/  193]    Overall Loss 0.000190    Objective Loss 0.000190                                        LR 0.000250    Time 1.289336    
2024-06-07 07:58:27,035 - Epoch: [69][  190/  193]    Overall Loss 0.000183    Objective Loss 0.000183                                        LR 0.000250    Time 1.282376    
2024-06-07 07:58:29,909 - Epoch: [69][  193/  193]    Overall Loss 0.000180    Objective Loss 0.000180    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.277105    
2024-06-07 07:58:30,644 - --- validate (epoch=69)-----------
2024-06-07 07:58:30,644 - 2172 samples (64 per mini-batch)
2024-06-07 07:58:59,995 - Epoch: [69][   10/   34]    Loss 0.000007    Top1 100.000000    Top5 100.000000    
2024-06-07 07:59:06,764 - Epoch: [69][   20/   34]    Loss 0.000026    Top1 100.000000    Top5 100.000000    
2024-06-07 07:59:13,297 - Epoch: [69][   30/   34]    Loss 0.000025    Top1 100.000000    Top5 100.000000    
2024-06-07 07:59:15,647 - Epoch: [69][   34/   34]    Loss 0.000031    Top1 100.000000    Top5 100.000000    
2024-06-07 07:59:16,690 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 07:59:16,691 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 07:59:16,932 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 69]
2024-06-07 07:59:16,933 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 07:59:16,953 - 

2024-06-07 07:59:16,953 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 07:59:51,032 - Epoch: [70][   10/  193]    Overall Loss 0.000186    Objective Loss 0.000186                                        LR 0.000250    Time 3.407843    
2024-06-07 08:00:03,291 - Epoch: [70][   20/  193]    Overall Loss 0.000131    Objective Loss 0.000131                                        LR 0.000250    Time 2.314683    
2024-06-07 08:00:15,683 - Epoch: [70][   30/  193]    Overall Loss 0.000112    Objective Loss 0.000112                                        LR 0.000250    Time 1.954446    
2024-06-07 08:00:27,411 - Epoch: [70][   40/  193]    Overall Loss 0.000138    Objective Loss 0.000138                                        LR 0.000250    Time 1.757734    
2024-06-07 08:00:38,994 - Epoch: [70][   50/  193]    Overall Loss 0.000142    Objective Loss 0.000142                                        LR 0.000250    Time 1.636916    
2024-06-07 08:00:50,641 - Epoch: [70][   60/  193]    Overall Loss 0.000139    Objective Loss 0.000139                                        LR 0.000250    Time 1.557782    
2024-06-07 08:01:02,019 - Epoch: [70][   70/  193]    Overall Loss 0.000218    Objective Loss 0.000218                                        LR 0.000250    Time 1.497220    
2024-06-07 08:01:13,346 - Epoch: [70][   80/  193]    Overall Loss 0.000212    Objective Loss 0.000212                                        LR 0.000250    Time 1.451181    
2024-06-07 08:01:25,081 - Epoch: [70][   90/  193]    Overall Loss 0.000203    Objective Loss 0.000203                                        LR 0.000250    Time 1.419986    
2024-06-07 08:01:36,397 - Epoch: [70][  100/  193]    Overall Loss 0.000188    Objective Loss 0.000188                                        LR 0.000250    Time 1.390762    
2024-06-07 08:01:48,181 - Epoch: [70][  110/  193]    Overall Loss 0.000175    Objective Loss 0.000175                                        LR 0.000250    Time 1.371166    
2024-06-07 08:01:59,624 - Epoch: [70][  120/  193]    Overall Loss 0.000162    Objective Loss 0.000162                                        LR 0.000250    Time 1.351947    
2024-06-07 08:02:11,075 - Epoch: [70][  130/  193]    Overall Loss 0.000153    Objective Loss 0.000153                                        LR 0.000250    Time 1.335727    
2024-06-07 08:02:22,751 - Epoch: [70][  140/  193]    Overall Loss 0.000145    Objective Loss 0.000145                                        LR 0.000250    Time 1.323446    
2024-06-07 08:02:34,566 - Epoch: [70][  150/  193]    Overall Loss 0.000137    Objective Loss 0.000137                                        LR 0.000250    Time 1.313755    
2024-06-07 08:02:46,424 - Epoch: [70][  160/  193]    Overall Loss 0.000130    Objective Loss 0.000130                                        LR 0.000250    Time 1.305500    
2024-06-07 08:02:58,393 - Epoch: [70][  170/  193]    Overall Loss 0.000124    Objective Loss 0.000124                                        LR 0.000250    Time 1.298903    
2024-06-07 08:03:10,464 - Epoch: [70][  180/  193]    Overall Loss 0.000119    Objective Loss 0.000119                                        LR 0.000250    Time 1.293573    
2024-06-07 08:03:22,583 - Epoch: [70][  190/  193]    Overall Loss 0.000114    Objective Loss 0.000114                                        LR 0.000250    Time 1.289081    
2024-06-07 08:03:25,388 - Epoch: [70][  193/  193]    Overall Loss 0.000112    Objective Loss 0.000112    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.283310    
2024-06-07 08:03:26,136 - --- validate (epoch=70)-----------
2024-06-07 08:03:26,136 - 2172 samples (64 per mini-batch)
2024-06-07 08:04:04,354 - Epoch: [70][   10/   34]    Loss 0.000085    Top1 100.000000    Top5 100.000000    
2024-06-07 08:04:10,924 - Epoch: [70][   20/   34]    Loss 0.000059    Top1 100.000000    Top5 100.000000    
2024-06-07 08:04:17,297 - Epoch: [70][   30/   34]    Loss 0.000043    Top1 100.000000    Top5 100.000000    
2024-06-07 08:04:19,591 - Epoch: [70][   34/   34]    Loss 0.000040    Top1 100.000000    Top5 100.000000    
2024-06-07 08:04:20,616 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 08:04:20,617 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 08:04:20,888 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 70]
2024-06-07 08:04:20,889 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 08:04:20,907 - 

2024-06-07 08:04:20,907 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 08:04:54,572 - Epoch: [71][   10/  193]    Overall Loss 0.000019    Objective Loss 0.000019                                        LR 0.000250    Time 3.366401    
2024-06-07 08:05:06,367 - Epoch: [71][   20/  193]    Overall Loss 0.000026    Objective Loss 0.000026                                        LR 0.000250    Time 2.271114    
2024-06-07 08:05:18,271 - Epoch: [71][   30/  193]    Overall Loss 0.000029    Objective Loss 0.000029                                        LR 0.000250    Time 1.909231    
2024-06-07 08:05:30,377 - Epoch: [71][   40/  193]    Overall Loss 0.000033    Objective Loss 0.000033                                        LR 0.000250    Time 1.733852    
2024-06-07 08:05:42,282 - Epoch: [71][   50/  193]    Overall Loss 0.000030    Objective Loss 0.000030                                        LR 0.000250    Time 1.624188    
2024-06-07 08:05:54,042 - Epoch: [71][   60/  193]    Overall Loss 0.000030    Objective Loss 0.000030                                        LR 0.000250    Time 1.548773    
2024-06-07 08:06:05,770 - Epoch: [71][   70/  193]    Overall Loss 0.000042    Objective Loss 0.000042                                        LR 0.000250    Time 1.494492    
2024-06-07 08:06:17,413 - Epoch: [71][   80/  193]    Overall Loss 0.000042    Objective Loss 0.000042                                        LR 0.000250    Time 1.452873    
2024-06-07 08:06:29,316 - Epoch: [71][   90/  193]    Overall Loss 0.000041    Objective Loss 0.000041                                        LR 0.000250    Time 1.423348    
2024-06-07 08:06:41,652 - Epoch: [71][  100/  193]    Overall Loss 0.000038    Objective Loss 0.000038                                        LR 0.000250    Time 1.404022    
2024-06-07 08:06:54,293 - Epoch: [71][  110/  193]    Overall Loss 0.000036    Objective Loss 0.000036                                        LR 0.000250    Time 1.390582    
2024-06-07 08:07:06,998 - Epoch: [71][  120/  193]    Overall Loss 0.000034    Objective Loss 0.000034                                        LR 0.000250    Time 1.380105    
2024-06-07 08:07:20,771 - Epoch: [71][  130/  193]    Overall Loss 0.000032    Objective Loss 0.000032                                        LR 0.000250    Time 1.379593    
2024-06-07 08:07:32,812 - Epoch: [71][  140/  193]    Overall Loss 0.000035    Objective Loss 0.000035                                        LR 0.000250    Time 1.366601    
2024-06-07 08:07:44,752 - Epoch: [71][  150/  193]    Overall Loss 0.000033    Objective Loss 0.000033                                        LR 0.000250    Time 1.354815    
2024-06-07 08:07:56,786 - Epoch: [71][  160/  193]    Overall Loss 0.000033    Objective Loss 0.000033                                        LR 0.000250    Time 1.345120    
2024-06-07 08:08:08,741 - Epoch: [71][  170/  193]    Overall Loss 0.000031    Objective Loss 0.000031                                        LR 0.000250    Time 1.336114    
2024-06-07 08:08:20,923 - Epoch: [71][  180/  193]    Overall Loss 0.000030    Objective Loss 0.000030                                        LR 0.000250    Time 1.329339    
2024-06-07 08:08:32,515 - Epoch: [71][  190/  193]    Overall Loss 0.000029    Objective Loss 0.000029                                        LR 0.000250    Time 1.320175    
2024-06-07 08:08:35,246 - Epoch: [71][  193/  193]    Overall Loss 0.000029    Objective Loss 0.000029    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.313625    
2024-06-07 08:08:35,967 - --- validate (epoch=71)-----------
2024-06-07 08:08:35,967 - 2172 samples (64 per mini-batch)
2024-06-07 08:09:04,509 - Epoch: [71][   10/   34]    Loss 0.000007    Top1 100.000000    Top5 100.000000    
2024-06-07 08:09:10,913 - Epoch: [71][   20/   34]    Loss 0.000009    Top1 100.000000    Top5 100.000000    
2024-06-07 08:09:17,500 - Epoch: [71][   30/   34]    Loss 0.000010    Top1 100.000000    Top5 100.000000    
2024-06-07 08:09:20,044 - Epoch: [71][   34/   34]    Loss 0.000010    Top1 100.000000    Top5 100.000000    
2024-06-07 08:09:21,132 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 08:09:21,133 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 08:09:21,386 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 71]
2024-06-07 08:09:21,386 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 08:09:21,407 - 

2024-06-07 08:09:21,407 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 08:09:56,138 - Epoch: [72][   10/  193]    Overall Loss 0.000014    Objective Loss 0.000014                                        LR 0.000250    Time 3.472931    
2024-06-07 08:10:07,441 - Epoch: [72][   20/  193]    Overall Loss 0.000014    Objective Loss 0.000014                                        LR 0.000250    Time 2.300147    
2024-06-07 08:10:18,517 - Epoch: [72][   30/  193]    Overall Loss 0.000012    Objective Loss 0.000012                                        LR 0.000250    Time 1.901373    
2024-06-07 08:10:29,992 - Epoch: [72][   40/  193]    Overall Loss 0.000012    Objective Loss 0.000012                                        LR 0.000250    Time 1.711369    
2024-06-07 08:10:41,331 - Epoch: [72][   50/  193]    Overall Loss 0.000015    Objective Loss 0.000015                                        LR 0.000250    Time 1.595155    
2024-06-07 08:10:52,789 - Epoch: [72][   60/  193]    Overall Loss 0.000015    Objective Loss 0.000015                                        LR 0.000250    Time 1.519523    
2024-06-07 08:11:04,434 - Epoch: [72][   70/  193]    Overall Loss 0.000014    Objective Loss 0.000014                                        LR 0.000250    Time 1.468171    
2024-06-07 08:11:15,901 - Epoch: [72][   80/  193]    Overall Loss 0.000016    Objective Loss 0.000016                                        LR 0.000250    Time 1.427605    
2024-06-07 08:11:27,680 - Epoch: [72][   90/  193]    Overall Loss 0.000022    Objective Loss 0.000022                                        LR 0.000250    Time 1.399458    
2024-06-07 08:11:39,424 - Epoch: [72][  100/  193]    Overall Loss 0.000023    Objective Loss 0.000023                                        LR 0.000250    Time 1.376576    
2024-06-07 08:11:51,708 - Epoch: [72][  110/  193]    Overall Loss 0.000022    Objective Loss 0.000022                                        LR 0.000250    Time 1.362794    
2024-06-07 08:12:03,434 - Epoch: [72][  120/  193]    Overall Loss 0.000022    Objective Loss 0.000022                                        LR 0.000250    Time 1.346680    
2024-06-07 08:12:15,322 - Epoch: [72][  130/  193]    Overall Loss 0.000021    Objective Loss 0.000021                                        LR 0.000250    Time 1.334327    
2024-06-07 08:12:27,350 - Epoch: [72][  140/  193]    Overall Loss 0.000022    Objective Loss 0.000022                                        LR 0.000250    Time 1.324699    
2024-06-07 08:12:39,634 - Epoch: [72][  150/  193]    Overall Loss 0.000027    Objective Loss 0.000027                                        LR 0.000250    Time 1.318072    
2024-06-07 08:12:51,672 - Epoch: [72][  160/  193]    Overall Loss 0.000038    Objective Loss 0.000038                                        LR 0.000250    Time 1.310697    
2024-06-07 08:13:03,558 - Epoch: [72][  170/  193]    Overall Loss 0.000037    Objective Loss 0.000037                                        LR 0.000250    Time 1.303340    
2024-06-07 08:13:15,426 - Epoch: [72][  180/  193]    Overall Loss 0.000036    Objective Loss 0.000036                                        LR 0.000250    Time 1.296651    
2024-06-07 08:13:27,142 - Epoch: [72][  190/  193]    Overall Loss 0.000035    Objective Loss 0.000035                                        LR 0.000250    Time 1.289900    
2024-06-07 08:13:30,439 - Epoch: [72][  193/  193]    Overall Loss 0.000035    Objective Loss 0.000035    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.286723    
2024-06-07 08:13:31,188 - --- validate (epoch=72)-----------
2024-06-07 08:13:31,188 - 2172 samples (64 per mini-batch)
2024-06-07 08:14:01,131 - Epoch: [72][   10/   34]    Loss 0.000009    Top1 100.000000    Top5 100.000000    
2024-06-07 08:14:08,500 - Epoch: [72][   20/   34]    Loss 0.000071    Top1 100.000000    Top5 100.000000    
2024-06-07 08:14:15,357 - Epoch: [72][   30/   34]    Loss 0.000056    Top1 100.000000    Top5 100.000000    
2024-06-07 08:14:17,764 - Epoch: [72][   34/   34]    Loss 0.000050    Top1 100.000000    Top5 100.000000    
2024-06-07 08:14:18,820 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 08:14:18,821 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 08:14:19,199 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 72]
2024-06-07 08:14:19,200 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 08:14:19,238 - 

2024-06-07 08:14:19,239 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 08:14:57,325 - Epoch: [73][   10/  193]    Overall Loss 0.000011    Objective Loss 0.000011                                        LR 0.000250    Time 3.808577    
2024-06-07 08:15:10,765 - Epoch: [73][   20/  193]    Overall Loss 0.000018    Objective Loss 0.000018                                        LR 0.000250    Time 2.574357    
2024-06-07 08:15:26,241 - Epoch: [73][   30/  193]    Overall Loss 0.000016    Objective Loss 0.000016                                        LR 0.000250    Time 2.229822    
2024-06-07 08:15:41,851 - Epoch: [73][   40/  193]    Overall Loss 0.000017    Objective Loss 0.000017                                        LR 0.000250    Time 2.061083    
2024-06-07 08:15:57,290 - Epoch: [73][   50/  193]    Overall Loss 0.000017    Objective Loss 0.000017                                        LR 0.000250    Time 1.956496    
2024-06-07 08:16:11,503 - Epoch: [73][   60/  193]    Overall Loss 0.000017    Objective Loss 0.000017                                        LR 0.000250    Time 1.866477    
2024-06-07 08:16:25,456 - Epoch: [73][   70/  193]    Overall Loss 0.000018    Objective Loss 0.000018                                        LR 0.000250    Time 1.798463    
2024-06-07 08:16:38,802 - Epoch: [73][   80/  193]    Overall Loss 0.000018    Objective Loss 0.000018                                        LR 0.000250    Time 1.739835    
2024-06-07 08:16:52,156 - Epoch: [73][   90/  193]    Overall Loss 0.000032    Objective Loss 0.000032                                        LR 0.000250    Time 1.694287    
2024-06-07 08:17:05,835 - Epoch: [73][  100/  193]    Overall Loss 0.000033    Objective Loss 0.000033                                        LR 0.000250    Time 1.661054    
2024-06-07 08:17:19,765 - Epoch: [73][  110/  193]    Overall Loss 0.000056    Objective Loss 0.000056                                        LR 0.000250    Time 1.636165    
2024-06-07 08:17:34,202 - Epoch: [73][  120/  193]    Overall Loss 0.000068    Objective Loss 0.000068                                        LR 0.000250    Time 1.619486    
2024-06-07 08:17:47,602 - Epoch: [73][  130/  193]    Overall Loss 0.000064    Objective Loss 0.000064                                        LR 0.000250    Time 1.597516    
2024-06-07 08:18:00,852 - Epoch: [73][  140/  193]    Overall Loss 0.000063    Objective Loss 0.000063                                        LR 0.000250    Time 1.577688    
2024-06-07 08:18:14,270 - Epoch: [73][  150/  193]    Overall Loss 0.000070    Objective Loss 0.000070                                        LR 0.000250    Time 1.561555    
2024-06-07 08:18:27,872 - Epoch: [73][  160/  193]    Overall Loss 0.000071    Objective Loss 0.000071                                        LR 0.000250    Time 1.548627    
2024-06-07 08:18:41,233 - Epoch: [73][  170/  193]    Overall Loss 0.000071    Objective Loss 0.000071                                        LR 0.000250    Time 1.535851    
2024-06-07 08:18:54,347 - Epoch: [73][  180/  193]    Overall Loss 0.000068    Objective Loss 0.000068                                        LR 0.000250    Time 1.523136    
2024-06-07 08:19:07,171 - Epoch: [73][  190/  193]    Overall Loss 0.000065    Objective Loss 0.000065                                        LR 0.000250    Time 1.510179    
2024-06-07 08:19:10,312 - Epoch: [73][  193/  193]    Overall Loss 0.000064    Objective Loss 0.000064    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.502725    
2024-06-07 08:19:11,082 - --- validate (epoch=73)-----------
2024-06-07 08:19:11,082 - 2172 samples (64 per mini-batch)
2024-06-07 08:19:57,392 - Epoch: [73][   10/   34]    Loss 0.000018    Top1 100.000000    Top5 100.000000    
2024-06-07 08:20:04,337 - Epoch: [73][   20/   34]    Loss 0.000062    Top1 100.000000    Top5 100.000000    
2024-06-07 08:20:10,771 - Epoch: [73][   30/   34]    Loss 0.000042    Top1 100.000000    Top5 100.000000    
2024-06-07 08:20:13,434 - Epoch: [73][   34/   34]    Loss 0.000039    Top1 100.000000    Top5 100.000000    
2024-06-07 08:20:14,635 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 08:20:14,635 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 08:20:15,086 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 73]
2024-06-07 08:20:15,087 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 08:20:15,109 - 

2024-06-07 08:20:15,109 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 08:20:55,053 - Epoch: [74][   10/  193]    Overall Loss 0.000038    Objective Loss 0.000038                                        LR 0.000250    Time 3.994200    
2024-06-07 08:21:09,158 - Epoch: [74][   20/  193]    Overall Loss 0.000036    Objective Loss 0.000036                                        LR 0.000250    Time 2.698335    
2024-06-07 08:21:22,637 - Epoch: [74][   30/  193]    Overall Loss 0.000026    Objective Loss 0.000026                                        LR 0.000250    Time 2.246223    
2024-06-07 08:21:36,428 - Epoch: [74][   40/  193]    Overall Loss 0.000023    Objective Loss 0.000023                                        LR 0.000250    Time 2.027579    
2024-06-07 08:21:49,836 - Epoch: [74][   50/  193]    Overall Loss 0.000021    Objective Loss 0.000021                                        LR 0.000250    Time 1.889196    
2024-06-07 08:22:03,183 - Epoch: [74][   60/  193]    Overall Loss 0.000018    Objective Loss 0.000018                                        LR 0.000250    Time 1.795765    
2024-06-07 08:22:15,272 - Epoch: [74][   70/  193]    Overall Loss 0.000051    Objective Loss 0.000051                                        LR 0.000250    Time 1.711221    
2024-06-07 08:22:27,304 - Epoch: [74][   80/  193]    Overall Loss 0.000054    Objective Loss 0.000054                                        LR 0.000250    Time 1.647299    
2024-06-07 08:22:39,663 - Epoch: [74][   90/  193]    Overall Loss 0.000053    Objective Loss 0.000053                                        LR 0.000250    Time 1.601057    
2024-06-07 08:22:51,790 - Epoch: [74][  100/  193]    Overall Loss 0.000052    Objective Loss 0.000052                                        LR 0.000250    Time 1.561876    
2024-06-07 08:23:04,767 - Epoch: [74][  110/  193]    Overall Loss 0.000048    Objective Loss 0.000048                                        LR 0.000250    Time 1.537520    
2024-06-07 08:23:16,845 - Epoch: [74][  120/  193]    Overall Loss 0.000046    Objective Loss 0.000046                                        LR 0.000250    Time 1.509707    
2024-06-07 08:23:29,117 - Epoch: [74][  130/  193]    Overall Loss 0.000043    Objective Loss 0.000043                                        LR 0.000250    Time 1.487577    
2024-06-07 08:23:41,218 - Epoch: [74][  140/  193]    Overall Loss 0.000041    Objective Loss 0.000041                                        LR 0.000250    Time 1.467410    
2024-06-07 08:23:53,439 - Epoch: [74][  150/  193]    Overall Loss 0.000039    Objective Loss 0.000039                                        LR 0.000250    Time 1.450722    
2024-06-07 08:24:05,726 - Epoch: [74][  160/  193]    Overall Loss 0.000038    Objective Loss 0.000038                                        LR 0.000250    Time 1.436467    
2024-06-07 08:24:17,773 - Epoch: [74][  170/  193]    Overall Loss 0.000036    Objective Loss 0.000036                                        LR 0.000250    Time 1.422607    
2024-06-07 08:24:30,262 - Epoch: [74][  180/  193]    Overall Loss 0.000035    Objective Loss 0.000035                                        LR 0.000250    Time 1.412746    
2024-06-07 08:24:42,095 - Epoch: [74][  190/  193]    Overall Loss 0.000034    Objective Loss 0.000034                                        LR 0.000250    Time 1.400452    
2024-06-07 08:24:44,855 - Epoch: [74][  193/  193]    Overall Loss 0.000033    Objective Loss 0.000033    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.392741    
2024-06-07 08:24:45,589 - --- validate (epoch=74)-----------
2024-06-07 08:24:45,590 - 2172 samples (64 per mini-batch)
2024-06-07 08:25:25,678 - Epoch: [74][   10/   34]    Loss 0.000016    Top1 100.000000    Top5 100.000000    
2024-06-07 08:25:33,755 - Epoch: [74][   20/   34]    Loss 0.000013    Top1 100.000000    Top5 100.000000    
2024-06-07 08:25:41,073 - Epoch: [74][   30/   34]    Loss 0.000011    Top1 100.000000    Top5 100.000000    
2024-06-07 08:25:43,818 - Epoch: [74][   34/   34]    Loss 0.000010    Top1 100.000000    Top5 100.000000    
2024-06-07 08:25:45,098 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 08:25:45,099 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 08:25:45,408 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 74]
2024-06-07 08:25:45,408 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 08:25:45,431 - 

2024-06-07 08:25:45,431 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 08:26:29,478 - Epoch: [75][   10/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 4.404555    
2024-06-07 08:26:44,253 - Epoch: [75][   20/  193]    Overall Loss 0.000012    Objective Loss 0.000012                                        LR 0.000250    Time 2.936750    
2024-06-07 08:26:59,855 - Epoch: [75][   30/  193]    Overall Loss 0.000012    Objective Loss 0.000012                                        LR 0.000250    Time 2.475428    
2024-06-07 08:27:16,207 - Epoch: [75][   40/  193]    Overall Loss 0.000010    Objective Loss 0.000010                                        LR 0.000250    Time 2.263266    
2024-06-07 08:27:32,568 - Epoch: [75][   50/  193]    Overall Loss 0.000009    Objective Loss 0.000009                                        LR 0.000250    Time 2.136155    
2024-06-07 08:27:49,784 - Epoch: [75][   60/  193]    Overall Loss 0.000010    Objective Loss 0.000010                                        LR 0.000250    Time 2.065794    
2024-06-07 08:28:06,064 - Epoch: [75][   70/  193]    Overall Loss 0.000034    Objective Loss 0.000034                                        LR 0.000250    Time 2.002061    
2024-06-07 08:28:22,632 - Epoch: [75][   80/  193]    Overall Loss 0.000049    Objective Loss 0.000049                                        LR 0.000250    Time 1.957842    
2024-06-07 08:28:38,874 - Epoch: [75][   90/  193]    Overall Loss 0.000047    Objective Loss 0.000047                                        LR 0.000250    Time 1.919837    
2024-06-07 08:28:52,578 - Epoch: [75][  100/  193]    Overall Loss 0.000043    Objective Loss 0.000043                                        LR 0.000250    Time 1.864321    
2024-06-07 08:29:04,922 - Epoch: [75][  110/  193]    Overall Loss 0.000048    Objective Loss 0.000048                                        LR 0.000250    Time 1.806594    
2024-06-07 08:29:17,358 - Epoch: [75][  120/  193]    Overall Loss 0.000054    Objective Loss 0.000054                                        LR 0.000250    Time 1.759369    
2024-06-07 08:29:30,271 - Epoch: [75][  130/  193]    Overall Loss 0.000052    Objective Loss 0.000052                                        LR 0.000250    Time 1.723040    
2024-06-07 08:29:42,450 - Epoch: [75][  140/  193]    Overall Loss 0.000051    Objective Loss 0.000051                                        LR 0.000250    Time 1.686645    
2024-06-07 08:29:54,690 - Epoch: [75][  150/  193]    Overall Loss 0.000052    Objective Loss 0.000052                                        LR 0.000250    Time 1.655612    
2024-06-07 08:30:07,646 - Epoch: [75][  160/  193]    Overall Loss 0.000049    Objective Loss 0.000049                                        LR 0.000250    Time 1.632892    
2024-06-07 08:30:20,174 - Epoch: [75][  170/  193]    Overall Loss 0.000048    Objective Loss 0.000048                                        LR 0.000250    Time 1.610347    
2024-06-07 08:30:32,627 - Epoch: [75][  180/  193]    Overall Loss 0.000046    Objective Loss 0.000046                                        LR 0.000250    Time 1.589758    
2024-06-07 08:30:44,570 - Epoch: [75][  190/  193]    Overall Loss 0.000158    Objective Loss 0.000158                                        LR 0.000250    Time 1.568742    
2024-06-07 08:30:47,410 - Epoch: [75][  193/  193]    Overall Loss 0.000206    Objective Loss 0.000206    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.558894    
2024-06-07 08:30:48,129 - --- validate (epoch=75)-----------
2024-06-07 08:30:48,129 - 2172 samples (64 per mini-batch)
2024-06-07 08:31:18,073 - Epoch: [75][   10/   34]    Loss 0.000312    Top1 100.000000    Top5 100.000000    
2024-06-07 08:31:24,858 - Epoch: [75][   20/   34]    Loss 0.000250    Top1 100.000000    Top5 100.000000    
2024-06-07 08:31:31,313 - Epoch: [75][   30/   34]    Loss 0.000195    Top1 100.000000    Top5 100.000000    
2024-06-07 08:31:33,712 - Epoch: [75][   34/   34]    Loss 0.000177    Top1 100.000000    Top5 100.000000    
2024-06-07 08:31:34,750 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 08:31:34,750 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 08:31:35,017 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 75]
2024-06-07 08:31:35,017 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 08:31:35,037 - 

2024-06-07 08:31:35,037 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 08:32:09,929 - Epoch: [76][   10/  193]    Overall Loss 0.002171    Objective Loss 0.002171                                        LR 0.000250    Time 3.489153    
2024-06-07 08:32:22,113 - Epoch: [76][   20/  193]    Overall Loss 0.002903    Objective Loss 0.002903                                        LR 0.000250    Time 2.350906    
2024-06-07 08:32:34,144 - Epoch: [76][   30/  193]    Overall Loss 0.004806    Objective Loss 0.004806                                        LR 0.000250    Time 1.966902    
2024-06-07 08:32:46,350 - Epoch: [76][   40/  193]    Overall Loss 0.003996    Objective Loss 0.003996                                        LR 0.000250    Time 1.779617    
2024-06-07 08:32:58,314 - Epoch: [76][   50/  193]    Overall Loss 0.003268    Objective Loss 0.003268                                        LR 0.000250    Time 1.662225    
2024-06-07 08:33:10,334 - Epoch: [76][   60/  193]    Overall Loss 0.002908    Objective Loss 0.002908                                        LR 0.000250    Time 1.584936    
2024-06-07 08:33:22,358 - Epoch: [76][   70/  193]    Overall Loss 0.002658    Objective Loss 0.002658                                        LR 0.000250    Time 1.529702    
2024-06-07 08:33:34,003 - Epoch: [76][   80/  193]    Overall Loss 0.002374    Objective Loss 0.002374                                        LR 0.000250    Time 1.483477    
2024-06-07 08:33:45,819 - Epoch: [76][   90/  193]    Overall Loss 0.002128    Objective Loss 0.002128                                        LR 0.000250    Time 1.449506    
2024-06-07 08:33:57,471 - Epoch: [76][  100/  193]    Overall Loss 0.001919    Objective Loss 0.001919                                        LR 0.000250    Time 1.420693    
2024-06-07 08:34:09,298 - Epoch: [76][  110/  193]    Overall Loss 0.001746    Objective Loss 0.001746                                        LR 0.000250    Time 1.398745    
2024-06-07 08:34:21,104 - Epoch: [76][  120/  193]    Overall Loss 0.001629    Objective Loss 0.001629                                        LR 0.000250    Time 1.380342    
2024-06-07 08:34:33,504 - Epoch: [76][  130/  193]    Overall Loss 0.001509    Objective Loss 0.001509                                        LR 0.000250    Time 1.369247    
2024-06-07 08:34:45,835 - Epoch: [76][  140/  193]    Overall Loss 0.001445    Objective Loss 0.001445                                        LR 0.000250    Time 1.359076    
2024-06-07 08:34:57,897 - Epoch: [76][  150/  193]    Overall Loss 0.001353    Objective Loss 0.001353                                        LR 0.000250    Time 1.348620    
2024-06-07 08:35:09,929 - Epoch: [76][  160/  193]    Overall Loss 0.001316    Objective Loss 0.001316                                        LR 0.000250    Time 1.339340    
2024-06-07 08:35:22,249 - Epoch: [76][  170/  193]    Overall Loss 0.001258    Objective Loss 0.001258                                        LR 0.000250    Time 1.332778    
2024-06-07 08:35:34,767 - Epoch: [76][  180/  193]    Overall Loss 0.001225    Objective Loss 0.001225                                        LR 0.000250    Time 1.327979    
2024-06-07 08:35:46,549 - Epoch: [76][  190/  193]    Overall Loss 0.001168    Objective Loss 0.001168                                        LR 0.000250    Time 1.319884    
2024-06-07 08:35:49,327 - Epoch: [76][  193/  193]    Overall Loss 0.001150    Objective Loss 0.001150    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.313540    
2024-06-07 08:35:50,062 - --- validate (epoch=76)-----------
2024-06-07 08:35:50,062 - 2172 samples (64 per mini-batch)
2024-06-07 08:36:29,637 - Epoch: [76][   10/   34]    Loss 0.000123    Top1 100.000000    Top5 100.000000    
2024-06-07 08:36:36,394 - Epoch: [76][   20/   34]    Loss 0.000093    Top1 100.000000    Top5 100.000000    
2024-06-07 08:36:42,941 - Epoch: [76][   30/   34]    Loss 0.000111    Top1 100.000000    Top5 100.000000    
2024-06-07 08:36:45,333 - Epoch: [76][   34/   34]    Loss 0.000104    Top1 100.000000    Top5 100.000000    
2024-06-07 08:36:46,389 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 08:36:46,390 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 08:36:46,635 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 76]
2024-06-07 08:36:46,635 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 08:36:46,666 - 

2024-06-07 08:36:46,666 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 08:37:21,292 - Epoch: [77][   10/  193]    Overall Loss 0.000149    Objective Loss 0.000149                                        LR 0.000250    Time 3.462601    
2024-06-07 08:37:33,224 - Epoch: [77][   20/  193]    Overall Loss 0.000134    Objective Loss 0.000134                                        LR 0.000250    Time 2.325999    
2024-06-07 08:37:45,080 - Epoch: [77][   30/  193]    Overall Loss 0.000101    Objective Loss 0.000101                                        LR 0.000250    Time 1.944592    
2024-06-07 08:37:57,012 - Epoch: [77][   40/  193]    Overall Loss 0.000088    Objective Loss 0.000088                                        LR 0.000250    Time 1.755903    
2024-06-07 08:38:08,913 - Epoch: [77][   50/  193]    Overall Loss 0.000072    Objective Loss 0.000072                                        LR 0.000250    Time 1.641953    
2024-06-07 08:38:21,129 - Epoch: [77][   60/  193]    Overall Loss 0.000064    Objective Loss 0.000064                                        LR 0.000250    Time 1.571280    
2024-06-07 08:38:33,263 - Epoch: [77][   70/  193]    Overall Loss 0.000057    Objective Loss 0.000057                                        LR 0.000250    Time 1.519570    
2024-06-07 08:38:45,437 - Epoch: [77][   80/  193]    Overall Loss 0.000053    Objective Loss 0.000053                                        LR 0.000250    Time 1.481363    
2024-06-07 08:38:57,473 - Epoch: [77][   90/  193]    Overall Loss 0.000052    Objective Loss 0.000052                                        LR 0.000250    Time 1.450006    
2024-06-07 08:39:09,332 - Epoch: [77][  100/  193]    Overall Loss 0.000055    Objective Loss 0.000055                                        LR 0.000250    Time 1.423210    
2024-06-07 08:39:21,126 - Epoch: [77][  110/  193]    Overall Loss 0.000056    Objective Loss 0.000056                                        LR 0.000250    Time 1.400678    
2024-06-07 08:39:33,300 - Epoch: [77][  120/  193]    Overall Loss 0.000052    Objective Loss 0.000052                                        LR 0.000250    Time 1.385098    
2024-06-07 08:39:45,526 - Epoch: [77][  130/  193]    Overall Loss 0.000050    Objective Loss 0.000050                                        LR 0.000250    Time 1.372276    
2024-06-07 08:39:57,399 - Epoch: [77][  140/  193]    Overall Loss 0.000048    Objective Loss 0.000048                                        LR 0.000250    Time 1.358872    
2024-06-07 08:40:09,396 - Epoch: [77][  150/  193]    Overall Loss 0.000049    Objective Loss 0.000049                                        LR 0.000250    Time 1.348042    
2024-06-07 08:40:21,769 - Epoch: [77][  160/  193]    Overall Loss 0.000048    Objective Loss 0.000048                                        LR 0.000250    Time 1.340886    
2024-06-07 08:40:33,773 - Epoch: [77][  170/  193]    Overall Loss 0.000046    Objective Loss 0.000046                                        LR 0.000250    Time 1.332400    
2024-06-07 08:40:45,924 - Epoch: [77][  180/  193]    Overall Loss 0.000045    Objective Loss 0.000045                                        LR 0.000250    Time 1.325670    
2024-06-07 08:40:57,579 - Epoch: [77][  190/  193]    Overall Loss 0.000043    Objective Loss 0.000043                                        LR 0.000250    Time 1.316959    
2024-06-07 08:41:00,400 - Epoch: [77][  193/  193]    Overall Loss 0.000043    Objective Loss 0.000043    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.310905    
2024-06-07 08:41:01,152 - --- validate (epoch=77)-----------
2024-06-07 08:41:01,152 - 2172 samples (64 per mini-batch)
2024-06-07 08:41:30,036 - Epoch: [77][   10/   34]    Loss 0.000013    Top1 100.000000    Top5 100.000000    
2024-06-07 08:41:36,594 - Epoch: [77][   20/   34]    Loss 0.000028    Top1 100.000000    Top5 100.000000    
2024-06-07 08:41:43,422 - Epoch: [77][   30/   34]    Loss 0.000082    Top1 100.000000    Top5 100.000000    
2024-06-07 08:41:46,182 - Epoch: [77][   34/   34]    Loss 0.000073    Top1 100.000000    Top5 100.000000    
2024-06-07 08:41:47,390 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 08:41:47,391 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 08:41:47,648 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 77]
2024-06-07 08:41:47,649 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 08:41:47,669 - 

2024-06-07 08:41:47,669 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 08:42:22,174 - Epoch: [78][   10/  193]    Overall Loss 0.000051    Objective Loss 0.000051                                        LR 0.000250    Time 3.450353    
2024-06-07 08:42:34,311 - Epoch: [78][   20/  193]    Overall Loss 0.000037    Objective Loss 0.000037                                        LR 0.000250    Time 2.329793    
2024-06-07 08:42:46,453 - Epoch: [78][   30/  193]    Overall Loss 0.000029    Objective Loss 0.000029                                        LR 0.000250    Time 1.956752    
2024-06-07 08:42:58,784 - Epoch: [78][   40/  193]    Overall Loss 0.000032    Objective Loss 0.000032                                        LR 0.000250    Time 1.774996    
2024-06-07 08:43:10,800 - Epoch: [78][   50/  193]    Overall Loss 0.000029    Objective Loss 0.000029                                        LR 0.000250    Time 1.659620    
2024-06-07 08:43:22,918 - Epoch: [78][   60/  193]    Overall Loss 0.000030    Objective Loss 0.000030                                        LR 0.000250    Time 1.584401    
2024-06-07 08:43:35,197 - Epoch: [78][   70/  193]    Overall Loss 0.000028    Objective Loss 0.000028                                        LR 0.000250    Time 1.532877    
2024-06-07 08:43:47,296 - Epoch: [78][   80/  193]    Overall Loss 0.000026    Objective Loss 0.000026                                        LR 0.000250    Time 1.492015    
2024-06-07 08:43:59,343 - Epoch: [78][   90/  193]    Overall Loss 0.000026    Objective Loss 0.000026                                        LR 0.000250    Time 1.459698    
2024-06-07 08:44:11,431 - Epoch: [78][  100/  193]    Overall Loss 0.000024    Objective Loss 0.000024                                        LR 0.000250    Time 1.434211    
2024-06-07 08:44:23,473 - Epoch: [78][  110/  193]    Overall Loss 0.000023    Objective Loss 0.000023                                        LR 0.000250    Time 1.412940    
2024-06-07 08:44:35,503 - Epoch: [78][  120/  193]    Overall Loss 0.000022    Objective Loss 0.000022                                        LR 0.000250    Time 1.395058    
2024-06-07 08:44:47,551 - Epoch: [78][  130/  193]    Overall Loss 0.000021    Objective Loss 0.000021                                        LR 0.000250    Time 1.380041    
2024-06-07 08:44:59,245 - Epoch: [78][  140/  193]    Overall Loss 0.000020    Objective Loss 0.000020                                        LR 0.000250    Time 1.364671    
2024-06-07 08:45:10,969 - Epoch: [78][  150/  193]    Overall Loss 0.000021    Objective Loss 0.000021                                        LR 0.000250    Time 1.351567    
2024-06-07 08:45:22,848 - Epoch: [78][  160/  193]    Overall Loss 0.000021    Objective Loss 0.000021                                        LR 0.000250    Time 1.341080    
2024-06-07 08:45:34,923 - Epoch: [78][  170/  193]    Overall Loss 0.000025    Objective Loss 0.000025                                        LR 0.000250    Time 1.332992    
2024-06-07 08:45:46,702 - Epoch: [78][  180/  193]    Overall Loss 0.000024    Objective Loss 0.000024                                        LR 0.000250    Time 1.324235    
2024-06-07 08:45:58,264 - Epoch: [78][  190/  193]    Overall Loss 0.000023    Objective Loss 0.000023                                        LR 0.000250    Time 1.315041    
2024-06-07 08:46:01,125 - Epoch: [78][  193/  193]    Overall Loss 0.000023    Objective Loss 0.000023    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.309248    
2024-06-07 08:46:01,854 - --- validate (epoch=78)-----------
2024-06-07 08:46:01,854 - 2172 samples (64 per mini-batch)
2024-06-07 08:46:31,084 - Epoch: [78][   10/   34]    Loss 0.000013    Top1 100.000000    Top5 100.000000    
2024-06-07 08:46:37,830 - Epoch: [78][   20/   34]    Loss 0.000014    Top1 100.000000    Top5 100.000000    
2024-06-07 08:46:44,200 - Epoch: [78][   30/   34]    Loss 0.000014    Top1 100.000000    Top5 100.000000    
2024-06-07 08:46:46,546 - Epoch: [78][   34/   34]    Loss 0.000014    Top1 100.000000    Top5 100.000000    
2024-06-07 08:46:47,735 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 08:46:47,736 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 08:46:48,007 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 78]
2024-06-07 08:46:48,007 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 08:46:48,030 - 

2024-06-07 08:46:48,030 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 08:47:22,712 - Epoch: [79][   10/  193]    Overall Loss 0.000019    Objective Loss 0.000019                                        LR 0.000250    Time 3.468023    
2024-06-07 08:47:34,851 - Epoch: [79][   20/  193]    Overall Loss 0.000017    Objective Loss 0.000017                                        LR 0.000250    Time 2.339358    
2024-06-07 08:47:46,688 - Epoch: [79][   30/  193]    Overall Loss 0.000013    Objective Loss 0.000013                                        LR 0.000250    Time 1.953013    
2024-06-07 08:47:58,733 - Epoch: [79][   40/  193]    Overall Loss 0.000011    Objective Loss 0.000011                                        LR 0.000250    Time 1.765124    
2024-06-07 08:48:10,742 - Epoch: [79][   50/  193]    Overall Loss 0.000011    Objective Loss 0.000011                                        LR 0.000250    Time 1.651558    
2024-06-07 08:48:22,826 - Epoch: [79][   60/  193]    Overall Loss 0.000012    Objective Loss 0.000012                                        LR 0.000250    Time 1.577170    
2024-06-07 08:48:34,875 - Epoch: [79][   70/  193]    Overall Loss 0.000016    Objective Loss 0.000016                                        LR 0.000250    Time 1.523444    
2024-06-07 08:48:46,783 - Epoch: [79][   80/  193]    Overall Loss 0.000030    Objective Loss 0.000030                                        LR 0.000250    Time 1.481449    
2024-06-07 08:48:59,030 - Epoch: [79][   90/  193]    Overall Loss 0.000027    Objective Loss 0.000027                                        LR 0.000250    Time 1.452560    
2024-06-07 08:49:11,034 - Epoch: [79][  100/  193]    Overall Loss 0.000026    Objective Loss 0.000026                                        LR 0.000250    Time 1.426981    
2024-06-07 08:49:23,072 - Epoch: [79][  110/  193]    Overall Loss 0.000024    Objective Loss 0.000024                                        LR 0.000250    Time 1.406416    
2024-06-07 08:49:35,164 - Epoch: [79][  120/  193]    Overall Loss 0.000023    Objective Loss 0.000023                                        LR 0.000250    Time 1.389610    
2024-06-07 08:49:47,151 - Epoch: [79][  130/  193]    Overall Loss 0.000022    Objective Loss 0.000022                                        LR 0.000250    Time 1.374575    
2024-06-07 08:49:59,252 - Epoch: [79][  140/  193]    Overall Loss 0.000021    Objective Loss 0.000021                                        LR 0.000250    Time 1.362499    
2024-06-07 08:50:11,282 - Epoch: [79][  150/  193]    Overall Loss 0.000020    Objective Loss 0.000020                                        LR 0.000250    Time 1.351619    
2024-06-07 08:50:23,551 - Epoch: [79][  160/  193]    Overall Loss 0.000024    Objective Loss 0.000024                                        LR 0.000250    Time 1.343540    
2024-06-07 08:50:35,534 - Epoch: [79][  170/  193]    Overall Loss 0.000023    Objective Loss 0.000023                                        LR 0.000250    Time 1.334751    
2024-06-07 08:50:47,453 - Epoch: [79][  180/  193]    Overall Loss 0.000023    Objective Loss 0.000023                                        LR 0.000250    Time 1.326600    
2024-06-07 08:50:59,267 - Epoch: [79][  190/  193]    Overall Loss 0.000022    Objective Loss 0.000022                                        LR 0.000250    Time 1.318771    
2024-06-07 08:51:02,035 - Epoch: [79][  193/  193]    Overall Loss 0.000022    Objective Loss 0.000022    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.312382    
2024-06-07 08:51:02,766 - --- validate (epoch=79)-----------
2024-06-07 08:51:02,767 - 2172 samples (64 per mini-batch)
2024-06-07 08:51:40,567 - Epoch: [79][   10/   34]    Loss 0.000004    Top1 100.000000    Top5 100.000000    
2024-06-07 08:51:47,910 - Epoch: [79][   20/   34]    Loss 0.000014    Top1 100.000000    Top5 100.000000    
2024-06-07 08:51:54,296 - Epoch: [79][   30/   34]    Loss 0.000011    Top1 100.000000    Top5 100.000000    
2024-06-07 08:51:56,590 - Epoch: [79][   34/   34]    Loss 0.000011    Top1 100.000000    Top5 100.000000    
2024-06-07 08:51:57,641 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 08:51:57,641 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 08:51:57,993 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 79]
2024-06-07 08:51:57,994 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 08:51:58,012 - 

2024-06-07 08:51:58,012 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 08:52:32,193 - Epoch: [80][   10/  193]    Overall Loss 0.000032    Objective Loss 0.000032                                        LR 0.000250    Time 3.417926    
2024-06-07 08:52:44,284 - Epoch: [80][   20/  193]    Overall Loss 0.000019    Objective Loss 0.000019                                        LR 0.000250    Time 2.311588    
2024-06-07 08:52:56,287 - Epoch: [80][   30/  193]    Overall Loss 0.000015    Objective Loss 0.000015                                        LR 0.000250    Time 1.939414    
2024-06-07 08:53:08,286 - Epoch: [80][   40/  193]    Overall Loss 0.000013    Objective Loss 0.000013                                        LR 0.000250    Time 1.753211    
2024-06-07 08:53:20,285 - Epoch: [80][   50/  193]    Overall Loss 0.000011    Objective Loss 0.000011                                        LR 0.000250    Time 1.641639    
2024-06-07 08:53:32,490 - Epoch: [80][   60/  193]    Overall Loss 0.000010    Objective Loss 0.000010                                        LR 0.000250    Time 1.570670    
2024-06-07 08:53:44,593 - Epoch: [80][   70/  193]    Overall Loss 0.000011    Objective Loss 0.000011                                        LR 0.000250    Time 1.518465    
2024-06-07 08:53:56,791 - Epoch: [80][   80/  193]    Overall Loss 0.000011    Objective Loss 0.000011                                        LR 0.000250    Time 1.480682    
2024-06-07 08:54:08,894 - Epoch: [80][   90/  193]    Overall Loss 0.000010    Objective Loss 0.000010                                        LR 0.000250    Time 1.450230    
2024-06-07 08:54:20,831 - Epoch: [80][  100/  193]    Overall Loss 0.000010    Objective Loss 0.000010                                        LR 0.000250    Time 1.424247    
2024-06-07 08:54:32,926 - Epoch: [80][  110/  193]    Overall Loss 0.000010    Objective Loss 0.000010                                        LR 0.000250    Time 1.404441    
2024-06-07 08:54:44,996 - Epoch: [80][  120/  193]    Overall Loss 0.000010    Objective Loss 0.000010                                        LR 0.000250    Time 1.387636    
2024-06-07 08:54:57,163 - Epoch: [80][  130/  193]    Overall Loss 0.000010    Objective Loss 0.000010                                        LR 0.000250    Time 1.374196    
2024-06-07 08:55:09,086 - Epoch: [80][  140/  193]    Overall Loss 0.000010    Objective Loss 0.000010                                        LR 0.000250    Time 1.360941    
2024-06-07 08:55:21,236 - Epoch: [80][  150/  193]    Overall Loss 0.000010    Objective Loss 0.000010                                        LR 0.000250    Time 1.350935    
2024-06-07 08:55:33,946 - Epoch: [80][  160/  193]    Overall Loss 0.000010    Objective Loss 0.000010                                        LR 0.000250    Time 1.345715    
2024-06-07 08:55:45,931 - Epoch: [80][  170/  193]    Overall Loss 0.000010    Objective Loss 0.000010                                        LR 0.000250    Time 1.336830    
2024-06-07 08:55:58,123 - Epoch: [80][  180/  193]    Overall Loss 0.000017    Objective Loss 0.000017                                        LR 0.000250    Time 1.330084    
2024-06-07 08:56:09,792 - Epoch: [80][  190/  193]    Overall Loss 0.000017    Objective Loss 0.000017                                        LR 0.000250    Time 1.321246    
2024-06-07 08:56:12,548 - Epoch: [80][  193/  193]    Overall Loss 0.000017    Objective Loss 0.000017    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.314810    
2024-06-07 08:56:13,268 - --- validate (epoch=80)-----------
2024-06-07 08:56:13,268 - 2172 samples (64 per mini-batch)
2024-06-07 08:56:43,156 - Epoch: [80][   10/   34]    Loss 0.000011    Top1 100.000000    Top5 100.000000    
2024-06-07 08:56:49,965 - Epoch: [80][   20/   34]    Loss 0.000021    Top1 100.000000    Top5 100.000000    
2024-06-07 08:56:56,519 - Epoch: [80][   30/   34]    Loss 0.000070    Top1 100.000000    Top5 100.000000    
2024-06-07 08:56:58,906 - Epoch: [80][   34/   34]    Loss 0.000064    Top1 100.000000    Top5 100.000000    
2024-06-07 08:57:00,015 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 08:57:00,016 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 08:57:00,268 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 80]
2024-06-07 08:57:00,268 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 08:57:00,287 - 

2024-06-07 08:57:00,287 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 08:57:34,852 - Epoch: [81][   10/  193]    Overall Loss 0.000006    Objective Loss 0.000006                                        LR 0.000250    Time 3.456432    
2024-06-07 08:57:46,863 - Epoch: [81][   20/  193]    Overall Loss 0.000014    Objective Loss 0.000014                                        LR 0.000250    Time 2.326115    
2024-06-07 08:57:59,414 - Epoch: [81][   30/  193]    Overall Loss 0.000014    Objective Loss 0.000014                                        LR 0.000250    Time 1.967516    
2024-06-07 08:58:11,579 - Epoch: [81][   40/  193]    Overall Loss 0.000012    Objective Loss 0.000012                                        LR 0.000250    Time 1.778922    
2024-06-07 08:58:23,679 - Epoch: [81][   50/  193]    Overall Loss 0.000010    Objective Loss 0.000010                                        LR 0.000250    Time 1.664178    
2024-06-07 08:58:35,686 - Epoch: [81][   60/  193]    Overall Loss 0.000010    Objective Loss 0.000010                                        LR 0.000250    Time 1.586241    
2024-06-07 08:58:47,811 - Epoch: [81][   70/  193]    Overall Loss 0.000010    Objective Loss 0.000010                                        LR 0.000250    Time 1.532299    
2024-06-07 08:59:00,124 - Epoch: [81][   80/  193]    Overall Loss 0.000013    Objective Loss 0.000013                                        LR 0.000250    Time 1.494223    
2024-06-07 08:59:12,177 - Epoch: [81][   90/  193]    Overall Loss 0.000014    Objective Loss 0.000014                                        LR 0.000250    Time 1.461574    
2024-06-07 08:59:24,168 - Epoch: [81][  100/  193]    Overall Loss 0.000015    Objective Loss 0.000015                                        LR 0.000250    Time 1.434956    
2024-06-07 08:59:36,303 - Epoch: [81][  110/  193]    Overall Loss 0.000014    Objective Loss 0.000014                                        LR 0.000250    Time 1.414509    
2024-06-07 08:59:48,479 - Epoch: [81][  120/  193]    Overall Loss 0.000013    Objective Loss 0.000013                                        LR 0.000250    Time 1.397795    
2024-06-07 09:00:00,788 - Epoch: [81][  130/  193]    Overall Loss 0.000013    Objective Loss 0.000013                                        LR 0.000250    Time 1.384673    
2024-06-07 09:00:13,091 - Epoch: [81][  140/  193]    Overall Loss 0.000012    Objective Loss 0.000012                                        LR 0.000250    Time 1.373260    
2024-06-07 09:00:25,206 - Epoch: [81][  150/  193]    Overall Loss 0.000013    Objective Loss 0.000013                                        LR 0.000250    Time 1.362219    
2024-06-07 09:00:37,357 - Epoch: [81][  160/  193]    Overall Loss 0.000013    Objective Loss 0.000013                                        LR 0.000250    Time 1.352754    
2024-06-07 09:00:49,355 - Epoch: [81][  170/  193]    Overall Loss 0.000013    Objective Loss 0.000013                                        LR 0.000250    Time 1.343531    
2024-06-07 09:01:01,649 - Epoch: [81][  180/  193]    Overall Loss 0.000012    Objective Loss 0.000012                                        LR 0.000250    Time 1.336944    
2024-06-07 09:01:13,334 - Epoch: [81][  190/  193]    Overall Loss 0.000012    Objective Loss 0.000012                                        LR 0.000250    Time 1.327846    
2024-06-07 09:01:16,140 - Epoch: [81][  193/  193]    Overall Loss 0.000012    Objective Loss 0.000012    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.321554    
2024-06-07 09:01:16,883 - --- validate (epoch=81)-----------
2024-06-07 09:01:16,883 - 2172 samples (64 per mini-batch)
2024-06-07 09:01:46,852 - Epoch: [81][   10/   34]    Loss 0.000012    Top1 100.000000    Top5 100.000000    
2024-06-07 09:01:53,514 - Epoch: [81][   20/   34]    Loss 0.000022    Top1 100.000000    Top5 100.000000    
2024-06-07 09:01:59,920 - Epoch: [81][   30/   34]    Loss 0.000019    Top1 100.000000    Top5 100.000000    
2024-06-07 09:02:02,264 - Epoch: [81][   34/   34]    Loss 0.000017    Top1 100.000000    Top5 100.000000    
2024-06-07 09:02:03,340 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 09:02:03,340 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 09:02:03,576 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 81]
2024-06-07 09:02:03,576 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 09:02:03,600 - 

2024-06-07 09:02:03,601 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 09:02:37,925 - Epoch: [82][   10/  193]    Overall Loss 0.000009    Objective Loss 0.000009                                        LR 0.000250    Time 3.432324    
2024-06-07 09:02:49,924 - Epoch: [82][   20/  193]    Overall Loss 0.000020    Objective Loss 0.000020                                        LR 0.000250    Time 2.314133    
2024-06-07 09:03:02,010 - Epoch: [82][   30/  193]    Overall Loss 0.000015    Objective Loss 0.000015                                        LR 0.000250    Time 1.944382    
2024-06-07 09:03:14,076 - Epoch: [82][   40/  193]    Overall Loss 0.000015    Objective Loss 0.000015                                        LR 0.000250    Time 1.758861    
2024-06-07 09:03:26,190 - Epoch: [82][   50/  193]    Overall Loss 0.000012    Objective Loss 0.000012                                        LR 0.000250    Time 1.648488    
2024-06-07 09:03:38,294 - Epoch: [82][   60/  193]    Overall Loss 0.000011    Objective Loss 0.000011                                        LR 0.000250    Time 1.574856    
2024-06-07 09:03:50,294 - Epoch: [82][   70/  193]    Overall Loss 0.000010    Objective Loss 0.000010                                        LR 0.000250    Time 1.520794    
2024-06-07 09:04:02,505 - Epoch: [82][   80/  193]    Overall Loss 0.000046    Objective Loss 0.000046                                        LR 0.000250    Time 1.482899    
2024-06-07 09:04:14,602 - Epoch: [82][   90/  193]    Overall Loss 0.000065    Objective Loss 0.000065                                        LR 0.000250    Time 1.452092    
2024-06-07 09:04:26,484 - Epoch: [82][  100/  193]    Overall Loss 0.000064    Objective Loss 0.000064                                        LR 0.000250    Time 1.425333    
2024-06-07 09:04:38,510 - Epoch: [82][  110/  193]    Overall Loss 0.000067    Objective Loss 0.000067                                        LR 0.000250    Time 1.404725    
2024-06-07 09:04:50,636 - Epoch: [82][  120/  193]    Overall Loss 0.000062    Objective Loss 0.000062                                        LR 0.000250    Time 1.388473    
2024-06-07 09:05:02,862 - Epoch: [82][  130/  193]    Overall Loss 0.000081    Objective Loss 0.000081                                        LR 0.000250    Time 1.375301    
2024-06-07 09:05:14,976 - Epoch: [82][  140/  193]    Overall Loss 0.000083    Objective Loss 0.000083                                        LR 0.000250    Time 1.363310    
2024-06-07 09:05:27,286 - Epoch: [82][  150/  193]    Overall Loss 0.000079    Objective Loss 0.000079                                        LR 0.000250    Time 1.354215    
2024-06-07 09:05:40,069 - Epoch: [82][  160/  193]    Overall Loss 0.000076    Objective Loss 0.000076                                        LR 0.000250    Time 1.349225    
2024-06-07 09:05:52,214 - Epoch: [82][  170/  193]    Overall Loss 0.000073    Objective Loss 0.000073                                        LR 0.000250    Time 1.341026    
2024-06-07 09:06:04,452 - Epoch: [82][  180/  193]    Overall Loss 0.000071    Objective Loss 0.000071                                        LR 0.000250    Time 1.334289    
2024-06-07 09:06:16,210 - Epoch: [82][  190/  193]    Overall Loss 0.000068    Objective Loss 0.000068                                        LR 0.000250    Time 1.325773    
2024-06-07 09:06:18,986 - Epoch: [82][  193/  193]    Overall Loss 0.000068    Objective Loss 0.000068    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.319329    
2024-06-07 09:06:19,736 - --- validate (epoch=82)-----------
2024-06-07 09:06:19,737 - 2172 samples (64 per mini-batch)
2024-06-07 09:06:57,675 - Epoch: [82][   10/   34]    Loss 0.000019    Top1 100.000000    Top5 100.000000    
2024-06-07 09:07:04,384 - Epoch: [82][   20/   34]    Loss 0.000013    Top1 100.000000    Top5 100.000000    
2024-06-07 09:07:10,605 - Epoch: [82][   30/   34]    Loss 0.000013    Top1 100.000000    Top5 100.000000    
2024-06-07 09:07:12,911 - Epoch: [82][   34/   34]    Loss 0.000012    Top1 100.000000    Top5 100.000000    
2024-06-07 09:07:13,970 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 09:07:13,970 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 09:07:14,205 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 82]
2024-06-07 09:07:14,206 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 09:07:14,228 - 

2024-06-07 09:07:14,228 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 09:07:48,909 - Epoch: [83][   10/  193]    Overall Loss 0.000005    Objective Loss 0.000005                                        LR 0.000250    Time 3.468108    
2024-06-07 09:08:01,303 - Epoch: [83][   20/  193]    Overall Loss 0.000007    Objective Loss 0.000007                                        LR 0.000250    Time 2.351951    
2024-06-07 09:08:13,624 - Epoch: [83][   30/  193]    Overall Loss 0.000010    Objective Loss 0.000010                                        LR 0.000250    Time 1.976502    
2024-06-07 09:08:25,634 - Epoch: [83][   40/  193]    Overall Loss 0.000008    Objective Loss 0.000008                                        LR 0.000250    Time 1.781638    
2024-06-07 09:08:37,449 - Epoch: [83][   50/  193]    Overall Loss 0.000008    Objective Loss 0.000008                                        LR 0.000250    Time 1.660916    
2024-06-07 09:08:49,345 - Epoch: [83][   60/  193]    Overall Loss 0.000012    Objective Loss 0.000012                                        LR 0.000250    Time 1.581829    
2024-06-07 09:09:01,379 - Epoch: [83][   70/  193]    Overall Loss 0.000015    Objective Loss 0.000015                                        LR 0.000250    Time 1.527099    
2024-06-07 09:09:13,396 - Epoch: [83][   80/  193]    Overall Loss 0.000013    Objective Loss 0.000013                                        LR 0.000250    Time 1.485885    
2024-06-07 09:09:25,386 - Epoch: [83][   90/  193]    Overall Loss 0.000015    Objective Loss 0.000015                                        LR 0.000250    Time 1.453586    
2024-06-07 09:09:37,615 - Epoch: [83][  100/  193]    Overall Loss 0.000016    Objective Loss 0.000016                                        LR 0.000250    Time 1.430183    
2024-06-07 09:09:49,568 - Epoch: [83][  110/  193]    Overall Loss 0.000015    Objective Loss 0.000015                                        LR 0.000250    Time 1.408416    
2024-06-07 09:10:01,631 - Epoch: [83][  120/  193]    Overall Loss 0.000014    Objective Loss 0.000014                                        LR 0.000250    Time 1.391285    
2024-06-07 09:10:13,676 - Epoch: [83][  130/  193]    Overall Loss 0.000015    Objective Loss 0.000015                                        LR 0.000250    Time 1.376572    
2024-06-07 09:10:25,784 - Epoch: [83][  140/  193]    Overall Loss 0.000027    Objective Loss 0.000027                                        LR 0.000250    Time 1.364468    
2024-06-07 09:10:38,014 - Epoch: [83][  150/  193]    Overall Loss 0.000026    Objective Loss 0.000026                                        LR 0.000250    Time 1.354777    
2024-06-07 09:10:50,081 - Epoch: [83][  160/  193]    Overall Loss 0.000025    Objective Loss 0.000025                                        LR 0.000250    Time 1.345254    
2024-06-07 09:11:02,114 - Epoch: [83][  170/  193]    Overall Loss 0.000025    Objective Loss 0.000025                                        LR 0.000250    Time 1.336688    
2024-06-07 09:11:14,255 - Epoch: [83][  180/  193]    Overall Loss 0.000025    Objective Loss 0.000025                                        LR 0.000250    Time 1.329687    
2024-06-07 09:11:25,845 - Epoch: [83][  190/  193]    Overall Loss 0.000025    Objective Loss 0.000025                                        LR 0.000250    Time 1.320484    
2024-06-07 09:11:28,624 - Epoch: [83][  193/  193]    Overall Loss 0.000025    Objective Loss 0.000025    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.314143    
2024-06-07 09:11:29,341 - --- validate (epoch=83)-----------
2024-06-07 09:11:29,342 - 2172 samples (64 per mini-batch)
2024-06-07 09:11:58,427 - Epoch: [83][   10/   34]    Loss 0.000017    Top1 100.000000    Top5 100.000000    
2024-06-07 09:12:05,138 - Epoch: [83][   20/   34]    Loss 0.000064    Top1 100.000000    Top5 100.000000    
2024-06-07 09:12:11,674 - Epoch: [83][   30/   34]    Loss 0.000052    Top1 100.000000    Top5 100.000000    
2024-06-07 09:12:14,038 - Epoch: [83][   34/   34]    Loss 0.000047    Top1 100.000000    Top5 100.000000    
2024-06-07 09:12:15,132 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 09:12:15,133 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 09:12:15,373 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 83]
2024-06-07 09:12:15,374 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 09:12:15,402 - 

2024-06-07 09:12:15,402 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 09:12:50,097 - Epoch: [84][   10/  193]    Overall Loss 0.000013    Objective Loss 0.000013                                        LR 0.000250    Time 3.469493    
2024-06-07 09:13:02,428 - Epoch: [84][   20/  193]    Overall Loss 0.000079    Objective Loss 0.000079                                        LR 0.000250    Time 2.349180    
2024-06-07 09:13:14,493 - Epoch: [84][   30/  193]    Overall Loss 0.000074    Objective Loss 0.000074                                        LR 0.000250    Time 1.967141    
2024-06-07 09:13:26,739 - Epoch: [84][   40/  193]    Overall Loss 0.000068    Objective Loss 0.000068                                        LR 0.000250    Time 1.780483    
2024-06-07 09:13:39,610 - Epoch: [84][   50/  193]    Overall Loss 0.000093    Objective Loss 0.000093                                        LR 0.000250    Time 1.681050    
2024-06-07 09:13:52,198 - Epoch: [84][   60/  193]    Overall Loss 0.000079    Objective Loss 0.000079                                        LR 0.000250    Time 1.609930    
2024-06-07 09:14:04,226 - Epoch: [84][   70/  193]    Overall Loss 0.000219    Objective Loss 0.000219                                        LR 0.000250    Time 1.551250    
2024-06-07 09:14:17,339 - Epoch: [84][   80/  193]    Overall Loss 0.000197    Objective Loss 0.000197                                        LR 0.000250    Time 1.520744    
2024-06-07 09:14:30,714 - Epoch: [84][   90/  193]    Overall Loss 0.000187    Objective Loss 0.000187                                        LR 0.000250    Time 1.499905    
2024-06-07 09:14:43,027 - Epoch: [84][  100/  193]    Overall Loss 0.000172    Objective Loss 0.000172                                        LR 0.000250    Time 1.472688    
2024-06-07 09:14:55,227 - Epoch: [84][  110/  193]    Overall Loss 0.000158    Objective Loss 0.000158                                        LR 0.000250    Time 1.449273    
2024-06-07 09:15:07,851 - Epoch: [84][  120/  193]    Overall Loss 0.000148    Objective Loss 0.000148                                        LR 0.000250    Time 1.433421    
2024-06-07 09:15:21,863 - Epoch: [84][  130/  193]    Overall Loss 0.000137    Objective Loss 0.000137                                        LR 0.000250    Time 1.430660    
2024-06-07 09:15:35,798 - Epoch: [84][  140/  193]    Overall Loss 0.000128    Objective Loss 0.000128                                        LR 0.000250    Time 1.427685    
2024-06-07 09:15:48,913 - Epoch: [84][  150/  193]    Overall Loss 0.000120    Objective Loss 0.000120                                        LR 0.000250    Time 1.419632    
2024-06-07 09:16:01,308 - Epoch: [84][  160/  193]    Overall Loss 0.000114    Objective Loss 0.000114                                        LR 0.000250    Time 1.408046    
2024-06-07 09:16:13,639 - Epoch: [84][  170/  193]    Overall Loss 0.000108    Objective Loss 0.000108                                        LR 0.000250    Time 1.397490    
2024-06-07 09:16:25,742 - Epoch: [84][  180/  193]    Overall Loss 0.000102    Objective Loss 0.000102                                        LR 0.000250    Time 1.386924    
2024-06-07 09:16:37,524 - Epoch: [84][  190/  193]    Overall Loss 0.000097    Objective Loss 0.000097                                        LR 0.000250    Time 1.375745    
2024-06-07 09:16:40,321 - Epoch: [84][  193/  193]    Overall Loss 0.000096    Objective Loss 0.000096    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.368669    
2024-06-07 09:16:41,042 - --- validate (epoch=84)-----------
2024-06-07 09:16:41,042 - 2172 samples (64 per mini-batch)
2024-06-07 09:17:11,320 - Epoch: [84][   10/   34]    Loss 0.000008    Top1 100.000000    Top5 100.000000    
2024-06-07 09:17:18,126 - Epoch: [84][   20/   34]    Loss 0.000092    Top1 100.000000    Top5 100.000000    
2024-06-07 09:17:25,789 - Epoch: [84][   30/   34]    Loss 0.000064    Top1 100.000000    Top5 100.000000    
2024-06-07 09:17:28,189 - Epoch: [84][   34/   34]    Loss 0.000061    Top1 100.000000    Top5 100.000000    
2024-06-07 09:17:29,279 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 09:17:29,279 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 09:17:29,517 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 84]
2024-06-07 09:17:29,517 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 09:17:29,549 - 

2024-06-07 09:17:29,549 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 09:18:04,888 - Epoch: [85][   10/  193]    Overall Loss 0.000006    Objective Loss 0.000006                                        LR 0.000250    Time 3.533843    
2024-06-07 09:18:17,080 - Epoch: [85][   20/  193]    Overall Loss 0.000019    Objective Loss 0.000019                                        LR 0.000250    Time 2.374701    
2024-06-07 09:18:29,159 - Epoch: [85][   30/  193]    Overall Loss 0.000014    Objective Loss 0.000014                                        LR 0.000250    Time 1.984415    
2024-06-07 09:18:41,182 - Epoch: [85][   40/  193]    Overall Loss 0.000013    Objective Loss 0.000013                                        LR 0.000250    Time 1.787994    
2024-06-07 09:18:53,262 - Epoch: [85][   50/  193]    Overall Loss 0.000014    Objective Loss 0.000014                                        LR 0.000250    Time 1.671098    
2024-06-07 09:19:05,299 - Epoch: [85][   60/  193]    Overall Loss 0.000012    Objective Loss 0.000012                                        LR 0.000250    Time 1.592727    
2024-06-07 09:19:17,813 - Epoch: [85][   70/  193]    Overall Loss 0.000192    Objective Loss 0.000192                                        LR 0.000250    Time 1.543267    
2024-06-07 09:19:29,729 - Epoch: [85][   80/  193]    Overall Loss 0.000292    Objective Loss 0.000292                                        LR 0.000250    Time 1.498931    
2024-06-07 09:19:41,766 - Epoch: [85][   90/  193]    Overall Loss 0.000338    Objective Loss 0.000338                                        LR 0.000250    Time 1.465629    
2024-06-07 09:19:53,753 - Epoch: [85][  100/  193]    Overall Loss 0.000436    Objective Loss 0.000436                                        LR 0.000250    Time 1.438619    
2024-06-07 09:20:05,794 - Epoch: [85][  110/  193]    Overall Loss 0.000435    Objective Loss 0.000435                                        LR 0.000250    Time 1.416861    
2024-06-07 09:20:18,080 - Epoch: [85][  120/  193]    Overall Loss 0.000403    Objective Loss 0.000403                                        LR 0.000250    Time 1.400857    
2024-06-07 09:20:30,308 - Epoch: [85][  130/  193]    Overall Loss 0.000418    Objective Loss 0.000418                                        LR 0.000250    Time 1.386876    
2024-06-07 09:20:42,169 - Epoch: [85][  140/  193]    Overall Loss 0.000408    Objective Loss 0.000408                                        LR 0.000250    Time 1.372293    
2024-06-07 09:20:54,765 - Epoch: [85][  150/  193]    Overall Loss 0.000397    Objective Loss 0.000397                                        LR 0.000250    Time 1.364558    
2024-06-07 09:21:06,939 - Epoch: [85][  160/  193]    Overall Loss 0.000423    Objective Loss 0.000423                                        LR 0.000250    Time 1.354907    
2024-06-07 09:21:19,121 - Epoch: [85][  170/  193]    Overall Loss 0.000421    Objective Loss 0.000421                                        LR 0.000250    Time 1.346657    
2024-06-07 09:21:31,178 - Epoch: [85][  180/  193]    Overall Loss 0.000422    Objective Loss 0.000422                                        LR 0.000250    Time 1.338644    
2024-06-07 09:21:42,918 - Epoch: [85][  190/  193]    Overall Loss 0.000418    Objective Loss 0.000418                                        LR 0.000250    Time 1.329797    
2024-06-07 09:21:45,716 - Epoch: [85][  193/  193]    Overall Loss 0.000412    Objective Loss 0.000412    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.323396    
2024-06-07 09:21:46,457 - --- validate (epoch=85)-----------
2024-06-07 09:21:46,457 - 2172 samples (64 per mini-batch)
2024-06-07 09:22:25,222 - Epoch: [85][   10/   34]    Loss 0.000070    Top1 100.000000    Top5 100.000000    
2024-06-07 09:22:31,934 - Epoch: [85][   20/   34]    Loss 0.000073    Top1 100.000000    Top5 100.000000    
2024-06-07 09:22:38,264 - Epoch: [85][   30/   34]    Loss 0.000056    Top1 100.000000    Top5 100.000000    
2024-06-07 09:22:40,585 - Epoch: [85][   34/   34]    Loss 0.000050    Top1 100.000000    Top5 100.000000    
2024-06-07 09:22:41,619 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 09:22:41,620 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 09:22:41,863 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 85]
2024-06-07 09:22:41,863 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 09:22:41,885 - 

2024-06-07 09:22:41,885 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 09:23:17,790 - Epoch: [86][   10/  193]    Overall Loss 0.000039    Objective Loss 0.000039                                        LR 0.000250    Time 3.590275    
2024-06-07 09:23:30,230 - Epoch: [86][   20/  193]    Overall Loss 0.000078    Objective Loss 0.000078                                        LR 0.000250    Time 2.414205    
2024-06-07 09:23:42,217 - Epoch: [86][   30/  193]    Overall Loss 0.000078    Objective Loss 0.000078                                        LR 0.000250    Time 2.007725    
2024-06-07 09:23:54,271 - Epoch: [86][   40/  193]    Overall Loss 0.000068    Objective Loss 0.000068                                        LR 0.000250    Time 1.805936    
2024-06-07 09:24:06,241 - Epoch: [86][   50/  193]    Overall Loss 0.000058    Objective Loss 0.000058                                        LR 0.000250    Time 1.683370    
2024-06-07 09:24:18,626 - Epoch: [86][   60/  193]    Overall Loss 0.000050    Objective Loss 0.000050                                        LR 0.000250    Time 1.608393    
2024-06-07 09:24:30,706 - Epoch: [86][   70/  193]    Overall Loss 0.000045    Objective Loss 0.000045                                        LR 0.000250    Time 1.550577    
2024-06-07 09:24:42,695 - Epoch: [86][   80/  193]    Overall Loss 0.000041    Objective Loss 0.000041                                        LR 0.000250    Time 1.506174    
2024-06-07 09:24:54,626 - Epoch: [86][   90/  193]    Overall Loss 0.000039    Objective Loss 0.000039                                        LR 0.000250    Time 1.470866    
2024-06-07 09:25:06,527 - Epoch: [86][  100/  193]    Overall Loss 0.000036    Objective Loss 0.000036                                        LR 0.000250    Time 1.442475    
2024-06-07 09:25:18,746 - Epoch: [86][  110/  193]    Overall Loss 0.000034    Objective Loss 0.000034                                        LR 0.000250    Time 1.422058    
2024-06-07 09:25:31,253 - Epoch: [86][  120/  193]    Overall Loss 0.000031    Objective Loss 0.000031                                        LR 0.000250    Time 1.407448    
2024-06-07 09:25:43,382 - Epoch: [86][  130/  193]    Overall Loss 0.000030    Objective Loss 0.000030                                        LR 0.000250    Time 1.392007    
2024-06-07 09:25:55,315 - Epoch: [86][  140/  193]    Overall Loss 0.000028    Objective Loss 0.000028                                        LR 0.000250    Time 1.377474    
2024-06-07 09:26:07,346 - Epoch: [86][  150/  193]    Overall Loss 0.000026    Objective Loss 0.000026                                        LR 0.000250    Time 1.365582    
2024-06-07 09:26:19,496 - Epoch: [86][  160/  193]    Overall Loss 0.000027    Objective Loss 0.000027                                        LR 0.000250    Time 1.355939    
2024-06-07 09:26:31,529 - Epoch: [86][  170/  193]    Overall Loss 0.000026    Objective Loss 0.000026                                        LR 0.000250    Time 1.346716    
2024-06-07 09:26:43,550 - Epoch: [86][  180/  193]    Overall Loss 0.000028    Objective Loss 0.000028                                        LR 0.000250    Time 1.338497    
2024-06-07 09:26:55,246 - Epoch: [86][  190/  193]    Overall Loss 0.000027    Objective Loss 0.000027                                        LR 0.000250    Time 1.329440    
2024-06-07 09:26:58,014 - Epoch: [86][  193/  193]    Overall Loss 0.000027    Objective Loss 0.000027    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.322908    
2024-06-07 09:26:58,773 - --- validate (epoch=86)-----------
2024-06-07 09:26:58,773 - 2172 samples (64 per mini-batch)
2024-06-07 09:27:28,524 - Epoch: [86][   10/   34]    Loss 0.000040    Top1 100.000000    Top5 100.000000    
2024-06-07 09:27:35,272 - Epoch: [86][   20/   34]    Loss 0.000024    Top1 100.000000    Top5 100.000000    
2024-06-07 09:27:41,634 - Epoch: [86][   30/   34]    Loss 0.000018    Top1 100.000000    Top5 100.000000    
2024-06-07 09:27:44,042 - Epoch: [86][   34/   34]    Loss 0.000020    Top1 100.000000    Top5 100.000000    
2024-06-07 09:27:45,125 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 09:27:45,125 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 09:27:45,377 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 86]
2024-06-07 09:27:45,377 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 09:27:45,395 - 

2024-06-07 09:27:45,396 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 09:28:21,169 - Epoch: [87][   10/  193]    Overall Loss 0.000011    Objective Loss 0.000011                                        LR 0.000250    Time 3.577293    
2024-06-07 09:28:33,320 - Epoch: [87][   20/  193]    Overall Loss 0.000009    Objective Loss 0.000009                                        LR 0.000250    Time 2.394167    
2024-06-07 09:28:45,299 - Epoch: [87][   30/  193]    Overall Loss 0.000008    Objective Loss 0.000008                                        LR 0.000250    Time 1.994370    
2024-06-07 09:28:57,282 - Epoch: [87][   40/  193]    Overall Loss 0.000007    Objective Loss 0.000007                                        LR 0.000250    Time 1.794195    
2024-06-07 09:29:09,425 - Epoch: [87][   50/  193]    Overall Loss 0.000009    Objective Loss 0.000009                                        LR 0.000250    Time 1.677491    
2024-06-07 09:29:21,804 - Epoch: [87][   60/  193]    Overall Loss 0.000008    Objective Loss 0.000008                                        LR 0.000250    Time 1.603392    
2024-06-07 09:29:34,408 - Epoch: [87][   70/  193]    Overall Loss 0.000108    Objective Loss 0.000108                                        LR 0.000250    Time 1.553859    
2024-06-07 09:29:46,789 - Epoch: [87][   80/  193]    Overall Loss 0.000103    Objective Loss 0.000103                                        LR 0.000250    Time 1.513783    
2024-06-07 09:29:58,725 - Epoch: [87][   90/  193]    Overall Loss 0.000132    Objective Loss 0.000132                                        LR 0.000250    Time 1.477764    
2024-06-07 09:30:11,448 - Epoch: [87][  100/  193]    Overall Loss 0.000122    Objective Loss 0.000122                                        LR 0.000250    Time 1.456814    
2024-06-07 09:30:24,402 - Epoch: [87][  110/  193]    Overall Loss 0.000113    Objective Loss 0.000113                                        LR 0.000250    Time 1.441696    
2024-06-07 09:30:37,198 - Epoch: [87][  120/  193]    Overall Loss 0.000106    Objective Loss 0.000106                                        LR 0.000250    Time 1.427783    
2024-06-07 09:30:49,344 - Epoch: [87][  130/  193]    Overall Loss 0.000099    Objective Loss 0.000099                                        LR 0.000250    Time 1.411061    
2024-06-07 09:31:01,356 - Epoch: [87][  140/  193]    Overall Loss 0.000093    Objective Loss 0.000093                                        LR 0.000250    Time 1.395651    
2024-06-07 09:31:13,544 - Epoch: [87][  150/  193]    Overall Loss 0.000087    Objective Loss 0.000087                                        LR 0.000250    Time 1.383537    
2024-06-07 09:31:25,781 - Epoch: [87][  160/  193]    Overall Loss 0.000084    Objective Loss 0.000084                                        LR 0.000250    Time 1.373265    
2024-06-07 09:31:37,858 - Epoch: [87][  170/  193]    Overall Loss 0.000080    Objective Loss 0.000080                                        LR 0.000250    Time 1.363238    
2024-06-07 09:31:50,420 - Epoch: [87][  180/  193]    Overall Loss 0.000076    Objective Loss 0.000076                                        LR 0.000250    Time 1.357100    
2024-06-07 09:32:02,036 - Epoch: [87][  190/  193]    Overall Loss 0.000072    Objective Loss 0.000072                                        LR 0.000250    Time 1.346596    
2024-06-07 09:32:04,808 - Epoch: [87][  193/  193]    Overall Loss 0.000071    Objective Loss 0.000071    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.339873    
2024-06-07 09:32:05,515 - --- validate (epoch=87)-----------
2024-06-07 09:32:05,515 - 2172 samples (64 per mini-batch)
2024-06-07 09:32:35,177 - Epoch: [87][   10/   34]    Loss 0.000020    Top1 100.000000    Top5 100.000000    
2024-06-07 09:32:41,929 - Epoch: [87][   20/   34]    Loss 0.000026    Top1 100.000000    Top5 100.000000    
2024-06-07 09:32:48,329 - Epoch: [87][   30/   34]    Loss 0.000023    Top1 100.000000    Top5 100.000000    
2024-06-07 09:32:50,638 - Epoch: [87][   34/   34]    Loss 0.000024    Top1 100.000000    Top5 100.000000    
2024-06-07 09:32:51,726 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 09:32:51,727 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 09:32:51,985 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 87]
2024-06-07 09:32:51,986 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 09:32:52,004 - 

2024-06-07 09:32:52,004 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 09:33:26,726 - Epoch: [88][   10/  193]    Overall Loss 0.000011    Objective Loss 0.000011                                        LR 0.000250    Time 3.472114    
2024-06-07 09:33:39,004 - Epoch: [88][   20/  193]    Overall Loss 0.000014    Objective Loss 0.000014                                        LR 0.000250    Time 2.347419    
2024-06-07 09:33:51,132 - Epoch: [88][   30/  193]    Overall Loss 0.000014    Objective Loss 0.000014                                        LR 0.000250    Time 1.968261    
2024-06-07 09:34:03,138 - Epoch: [88][   40/  193]    Overall Loss 0.000013    Objective Loss 0.000013                                        LR 0.000250    Time 1.775406    
2024-06-07 09:34:15,421 - Epoch: [88][   50/  193]    Overall Loss 0.000012    Objective Loss 0.000012                                        LR 0.000250    Time 1.665250    
2024-06-07 09:34:27,681 - Epoch: [88][   60/  193]    Overall Loss 0.000011    Objective Loss 0.000011                                        LR 0.000250    Time 1.591425    
2024-06-07 09:34:40,370 - Epoch: [88][   70/  193]    Overall Loss 0.000012    Objective Loss 0.000012                                        LR 0.000250    Time 1.544762    
2024-06-07 09:34:52,433 - Epoch: [88][   80/  193]    Overall Loss 0.000011    Objective Loss 0.000011                                        LR 0.000250    Time 1.501983    
2024-06-07 09:35:04,440 - Epoch: [88][   90/  193]    Overall Loss 0.000011    Objective Loss 0.000011                                        LR 0.000250    Time 1.468113    
2024-06-07 09:35:16,727 - Epoch: [88][  100/  193]    Overall Loss 0.000014    Objective Loss 0.000014                                        LR 0.000250    Time 1.443824    
2024-06-07 09:35:29,246 - Epoch: [88][  110/  193]    Overall Loss 0.000014    Objective Loss 0.000014                                        LR 0.000250    Time 1.426025    
2024-06-07 09:35:41,855 - Epoch: [88][  120/  193]    Overall Loss 0.000014    Objective Loss 0.000014                                        LR 0.000250    Time 1.411956    
2024-06-07 09:35:53,987 - Epoch: [88][  130/  193]    Overall Loss 0.000013    Objective Loss 0.000013                                        LR 0.000250    Time 1.396387    
2024-06-07 09:36:06,014 - Epoch: [88][  140/  193]    Overall Loss 0.000013    Objective Loss 0.000013                                        LR 0.000250    Time 1.382273    
2024-06-07 09:36:18,128 - Epoch: [88][  150/  193]    Overall Loss 0.000012    Objective Loss 0.000012                                        LR 0.000250    Time 1.370664    
2024-06-07 09:36:30,712 - Epoch: [88][  160/  193]    Overall Loss 0.000012    Objective Loss 0.000012                                        LR 0.000250    Time 1.363379    
2024-06-07 09:36:42,770 - Epoch: [88][  170/  193]    Overall Loss 0.000011    Objective Loss 0.000011                                        LR 0.000250    Time 1.353844    
2024-06-07 09:36:54,899 - Epoch: [88][  180/  193]    Overall Loss 0.000012    Objective Loss 0.000012                                        LR 0.000250    Time 1.345826    
2024-06-07 09:37:06,659 - Epoch: [88][  190/  193]    Overall Loss 0.000012    Objective Loss 0.000012                                        LR 0.000250    Time 1.336645    
2024-06-07 09:37:09,471 - Epoch: [88][  193/  193]    Overall Loss 0.000012    Objective Loss 0.000012    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.330236    
2024-06-07 09:37:10,208 - --- validate (epoch=88)-----------
2024-06-07 09:37:10,208 - 2172 samples (64 per mini-batch)
2024-06-07 09:37:52,337 - Epoch: [88][   10/   34]    Loss 0.000014    Top1 100.000000    Top5 100.000000    
2024-06-07 09:37:59,077 - Epoch: [88][   20/   34]    Loss 0.000017    Top1 100.000000    Top5 100.000000    
2024-06-07 09:38:06,994 - Epoch: [88][   30/   34]    Loss 0.000014    Top1 100.000000    Top5 100.000000    
2024-06-07 09:38:09,372 - Epoch: [88][   34/   34]    Loss 0.000014    Top1 100.000000    Top5 100.000000    
2024-06-07 09:38:10,470 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 09:38:10,471 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 09:38:10,736 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 88]
2024-06-07 09:38:10,736 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 09:38:10,755 - 

2024-06-07 09:38:10,755 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 09:38:48,178 - Epoch: [89][   10/  193]    Overall Loss 0.000002    Objective Loss 0.000002                                        LR 0.000250    Time 3.742048    
2024-06-07 09:39:05,256 - Epoch: [89][   20/  193]    Overall Loss 0.000010    Objective Loss 0.000010                                        LR 0.000250    Time 2.720805    
2024-06-07 09:39:28,982 - Epoch: [89][   30/  193]    Overall Loss 0.000013    Objective Loss 0.000013                                        LR 0.000250    Time 2.598341    
2024-06-07 09:39:54,680 - Epoch: [89][   40/  193]    Overall Loss 0.000011    Objective Loss 0.000011                                        LR 0.000250    Time 2.588879    
2024-06-07 09:40:07,409 - Epoch: [89][   50/  193]    Overall Loss 0.000010    Objective Loss 0.000010                                        LR 0.000250    Time 2.324595    
2024-06-07 09:40:20,098 - Epoch: [89][   60/  193]    Overall Loss 0.000010    Objective Loss 0.000010                                        LR 0.000250    Time 2.147918    
2024-06-07 09:40:33,529 - Epoch: [89][   70/  193]    Overall Loss 0.000010    Objective Loss 0.000010                                        LR 0.000250    Time 2.032287    
2024-06-07 09:40:47,087 - Epoch: [89][   80/  193]    Overall Loss 0.000011    Objective Loss 0.000011                                        LR 0.000250    Time 1.946969    
2024-06-07 09:40:59,324 - Epoch: [89][   90/  193]    Overall Loss 0.000011    Objective Loss 0.000011                                        LR 0.000250    Time 1.866147    
2024-06-07 09:41:11,298 - Epoch: [89][  100/  193]    Overall Loss 0.000012    Objective Loss 0.000012                                        LR 0.000250    Time 1.798727    
2024-06-07 09:41:23,582 - Epoch: [89][  110/  193]    Overall Loss 0.000012    Objective Loss 0.000012                                        LR 0.000250    Time 1.746583    
2024-06-07 09:41:36,478 - Epoch: [89][  120/  193]    Overall Loss 0.000012    Objective Loss 0.000012                                        LR 0.000250    Time 1.708233    
2024-06-07 09:41:48,904 - Epoch: [89][  130/  193]    Overall Loss 0.000011    Objective Loss 0.000011                                        LR 0.000250    Time 1.672073    
2024-06-07 09:42:01,310 - Epoch: [89][  140/  193]    Overall Loss 0.000011    Objective Loss 0.000011                                        LR 0.000250    Time 1.640964    
2024-06-07 09:42:13,646 - Epoch: [89][  150/  193]    Overall Loss 0.000012    Objective Loss 0.000012                                        LR 0.000250    Time 1.613512    
2024-06-07 09:42:26,198 - Epoch: [89][  160/  193]    Overall Loss 0.000013    Objective Loss 0.000013                                        LR 0.000250    Time 1.590829    
2024-06-07 09:42:39,249 - Epoch: [89][  170/  193]    Overall Loss 0.000012    Objective Loss 0.000012                                        LR 0.000250    Time 1.573791    
2024-06-07 09:42:54,532 - Epoch: [89][  180/  193]    Overall Loss 0.000012    Objective Loss 0.000012                                        LR 0.000250    Time 1.571044    
2024-06-07 09:43:08,911 - Epoch: [89][  190/  193]    Overall Loss 0.000012    Objective Loss 0.000012                                        LR 0.000250    Time 1.563461    
2024-06-07 09:43:12,224 - Epoch: [89][  193/  193]    Overall Loss 0.000012    Objective Loss 0.000012    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.556046    
2024-06-07 09:43:13,069 - --- validate (epoch=89)-----------
2024-06-07 09:43:13,069 - 2172 samples (64 per mini-batch)
2024-06-07 09:43:45,064 - Epoch: [89][   10/   34]    Loss 0.000004    Top1 100.000000    Top5 100.000000    
2024-06-07 09:43:51,725 - Epoch: [89][   20/   34]    Loss 0.000006    Top1 100.000000    Top5 100.000000    
2024-06-07 09:43:57,928 - Epoch: [89][   30/   34]    Loss 0.000012    Top1 100.000000    Top5 100.000000    
2024-06-07 09:44:00,228 - Epoch: [89][   34/   34]    Loss 0.000011    Top1 100.000000    Top5 100.000000    
2024-06-07 09:44:01,274 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 09:44:01,275 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 09:44:01,510 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 89]
2024-06-07 09:44:01,511 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 09:44:01,531 - 

2024-06-07 09:44:01,531 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 09:44:35,829 - Epoch: [90][   10/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 3.429730    
2024-06-07 09:44:48,208 - Epoch: [90][   20/  193]    Overall Loss 0.000007    Objective Loss 0.000007                                        LR 0.000250    Time 2.331343    
2024-06-07 09:45:00,725 - Epoch: [90][   30/  193]    Overall Loss 0.000005    Objective Loss 0.000005                                        LR 0.000250    Time 1.970127    
2024-06-07 09:45:14,098 - Epoch: [90][   40/  193]    Overall Loss 0.000005    Objective Loss 0.000005                                        LR 0.000250    Time 1.810034    
2024-06-07 09:45:26,073 - Epoch: [90][   50/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.685965    
2024-06-07 09:45:38,323 - Epoch: [90][   60/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.607919    
2024-06-07 09:45:50,153 - Epoch: [90][   70/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.546737    
2024-06-07 09:46:02,095 - Epoch: [90][   80/  193]    Overall Loss 0.000005    Objective Loss 0.000005                                        LR 0.000250    Time 1.501986    
2024-06-07 09:46:13,775 - Epoch: [90][   90/  193]    Overall Loss 0.000006    Objective Loss 0.000006                                        LR 0.000250    Time 1.464422    
2024-06-07 09:46:25,439 - Epoch: [90][  100/  193]    Overall Loss 0.000006    Objective Loss 0.000006                                        LR 0.000250    Time 1.434264    
2024-06-07 09:46:37,432 - Epoch: [90][  110/  193]    Overall Loss 0.000006    Objective Loss 0.000006                                        LR 0.000250    Time 1.412457    
2024-06-07 09:46:49,720 - Epoch: [90][  120/  193]    Overall Loss 0.000006    Objective Loss 0.000006                                        LR 0.000250    Time 1.396858    
2024-06-07 09:47:01,672 - Epoch: [90][  130/  193]    Overall Loss 0.000006    Objective Loss 0.000006                                        LR 0.000250    Time 1.381085    
2024-06-07 09:47:13,685 - Epoch: [90][  140/  193]    Overall Loss 0.000008    Objective Loss 0.000008                                        LR 0.000250    Time 1.367973    
2024-06-07 09:47:25,920 - Epoch: [90][  150/  193]    Overall Loss 0.000008    Objective Loss 0.000008                                        LR 0.000250    Time 1.357957    
2024-06-07 09:47:38,436 - Epoch: [90][  160/  193]    Overall Loss 0.000010    Objective Loss 0.000010                                        LR 0.000250    Time 1.351048    
2024-06-07 09:47:50,622 - Epoch: [90][  170/  193]    Overall Loss 0.000010    Objective Loss 0.000010                                        LR 0.000250    Time 1.343047    
2024-06-07 09:48:02,852 - Epoch: [90][  180/  193]    Overall Loss 0.000010    Objective Loss 0.000010                                        LR 0.000250    Time 1.336164    
2024-06-07 09:48:14,766 - Epoch: [90][  190/  193]    Overall Loss 0.000012    Objective Loss 0.000012                                        LR 0.000250    Time 1.328349    
2024-06-07 09:48:17,562 - Epoch: [90][  193/  193]    Overall Loss 0.000012    Objective Loss 0.000012    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.321918    
2024-06-07 09:48:18,282 - --- validate (epoch=90)-----------
2024-06-07 09:48:18,282 - 2172 samples (64 per mini-batch)
2024-06-07 09:48:49,087 - Epoch: [90][   10/   34]    Loss 0.000003    Top1 100.000000    Top5 100.000000    
2024-06-07 09:48:55,865 - Epoch: [90][   20/   34]    Loss 0.000003    Top1 100.000000    Top5 100.000000    
2024-06-07 09:49:02,391 - Epoch: [90][   30/   34]    Loss 0.000005    Top1 100.000000    Top5 100.000000    
2024-06-07 09:49:05,076 - Epoch: [90][   34/   34]    Loss 0.000005    Top1 100.000000    Top5 100.000000    
2024-06-07 09:49:07,611 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 09:49:07,612 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 09:49:08,301 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 90]
2024-06-07 09:49:08,301 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 09:49:08,419 - 

2024-06-07 09:49:08,420 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 09:49:57,731 - Epoch: [91][   10/  193]    Overall Loss 0.000001    Objective Loss 0.000001                                        LR 0.000250    Time 4.930953    
2024-06-07 09:50:10,007 - Epoch: [91][   20/  193]    Overall Loss 0.000009    Objective Loss 0.000009                                        LR 0.000250    Time 3.077470    
2024-06-07 09:50:21,986 - Epoch: [91][   30/  193]    Overall Loss 0.000008    Objective Loss 0.000008                                        LR 0.000250    Time 2.449836    
2024-06-07 09:50:34,239 - Epoch: [91][   40/  193]    Overall Loss 0.000008    Objective Loss 0.000008                                        LR 0.000250    Time 2.142889    
2024-06-07 09:50:45,827 - Epoch: [91][   50/  193]    Overall Loss 0.000007    Objective Loss 0.000007                                        LR 0.000250    Time 1.945268    
2024-06-07 09:50:57,402 - Epoch: [91][   60/  193]    Overall Loss 0.000124    Objective Loss 0.000124                                        LR 0.000250    Time 1.813363    
2024-06-07 09:51:09,038 - Epoch: [91][   70/  193]    Overall Loss 0.000174    Objective Loss 0.000174                                        LR 0.000250    Time 1.720036    
2024-06-07 09:51:20,524 - Epoch: [91][   80/  193]    Overall Loss 0.000160    Objective Loss 0.000160                                        LR 0.000250    Time 1.648074    
2024-06-07 09:51:32,541 - Epoch: [91][   90/  193]    Overall Loss 0.000150    Objective Loss 0.000150                                        LR 0.000250    Time 1.598092    
2024-06-07 09:51:44,135 - Epoch: [91][  100/  193]    Overall Loss 0.000144    Objective Loss 0.000144                                        LR 0.000250    Time 1.553896    
2024-06-07 09:51:55,434 - Epoch: [91][  110/  193]    Overall Loss 0.000133    Objective Loss 0.000133                                        LR 0.000250    Time 1.514860    
2024-06-07 09:52:07,358 - Epoch: [91][  120/  193]    Overall Loss 0.000123    Objective Loss 0.000123                                        LR 0.000250    Time 1.487761    
2024-06-07 09:52:18,465 - Epoch: [91][  130/  193]    Overall Loss 0.000114    Objective Loss 0.000114                                        LR 0.000250    Time 1.458397    
2024-06-07 09:52:29,689 - Epoch: [91][  140/  193]    Overall Loss 0.000108    Objective Loss 0.000108                                        LR 0.000250    Time 1.434145    
2024-06-07 09:52:41,086 - Epoch: [91][  150/  193]    Overall Loss 0.000101    Objective Loss 0.000101                                        LR 0.000250    Time 1.414251    
2024-06-07 09:52:52,515 - Epoch: [91][  160/  193]    Overall Loss 0.000096    Objective Loss 0.000096                                        LR 0.000250    Time 1.397084    
2024-06-07 09:53:04,433 - Epoch: [91][  170/  193]    Overall Loss 0.000091    Objective Loss 0.000091                                        LR 0.000250    Time 1.384764    
2024-06-07 09:53:16,032 - Epoch: [91][  180/  193]    Overall Loss 0.000087    Objective Loss 0.000087                                        LR 0.000250    Time 1.371998    
2024-06-07 09:53:28,673 - Epoch: [91][  190/  193]    Overall Loss 0.000083    Objective Loss 0.000083                                        LR 0.000250    Time 1.366038    
2024-06-07 09:53:31,896 - Epoch: [91][  193/  193]    Overall Loss 0.000082    Objective Loss 0.000082    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.361191    
2024-06-07 09:53:32,702 - --- validate (epoch=91)-----------
2024-06-07 09:53:32,703 - 2172 samples (64 per mini-batch)
2024-06-07 09:54:01,354 - Epoch: [91][   10/   34]    Loss 0.000012    Top1 100.000000    Top5 100.000000    
2024-06-07 09:54:07,796 - Epoch: [91][   20/   34]    Loss 0.000008    Top1 100.000000    Top5 100.000000    
2024-06-07 09:54:14,023 - Epoch: [91][   30/   34]    Loss 0.000694    Top1 99.947917    Top5 100.000000    
2024-06-07 09:54:16,471 - Epoch: [91][   34/   34]    Loss 0.000613    Top1 99.953959    Top5 100.000000    
2024-06-07 09:54:17,585 - ==> Top1: 99.954    Top5: 100.000    Loss: 0.001

2024-06-07 09:54:17,586 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  1   0 227   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 09:54:17,814 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 90]
2024-06-07 09:54:17,814 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 09:54:17,822 - 

2024-06-07 09:54:17,823 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 09:54:53,190 - Epoch: [92][   10/  193]    Overall Loss 0.000047    Objective Loss 0.000047                                        LR 0.000250    Time 3.536604    
2024-06-07 09:55:04,728 - Epoch: [92][   20/  193]    Overall Loss 0.000026    Objective Loss 0.000026                                        LR 0.000250    Time 2.343538    
2024-06-07 09:55:16,278 - Epoch: [92][   30/  193]    Overall Loss 0.000020    Objective Loss 0.000020                                        LR 0.000250    Time 1.946160    
2024-06-07 09:55:29,085 - Epoch: [92][   40/  193]    Overall Loss 0.000017    Objective Loss 0.000017                                        LR 0.000250    Time 1.778688    
2024-06-07 09:55:41,401 - Epoch: [92][   50/  193]    Overall Loss 0.000014    Objective Loss 0.000014                                        LR 0.000250    Time 1.668498    
2024-06-07 09:55:54,517 - Epoch: [92][   60/  193]    Overall Loss 0.000018    Objective Loss 0.000018                                        LR 0.000250    Time 1.608359    
2024-06-07 09:56:07,309 - Epoch: [92][   70/  193]    Overall Loss 0.000016    Objective Loss 0.000016                                        LR 0.000250    Time 1.560598    
2024-06-07 09:56:23,955 - Epoch: [92][   80/  193]    Overall Loss 0.000015    Objective Loss 0.000015                                        LR 0.000250    Time 1.572450    
2024-06-07 09:56:46,684 - Epoch: [92][   90/  193]    Overall Loss 0.000014    Objective Loss 0.000014                                        LR 0.000250    Time 1.649713    
2024-06-07 09:57:05,273 - Epoch: [92][  100/  193]    Overall Loss 0.000015    Objective Loss 0.000015                                        LR 0.000250    Time 1.669789    
2024-06-07 09:57:26,953 - Epoch: [92][  110/  193]    Overall Loss 0.000014    Objective Loss 0.000014                                        LR 0.000250    Time 1.714377    
2024-06-07 09:57:43,953 - Epoch: [92][  120/  193]    Overall Loss 0.000013    Objective Loss 0.000013                                        LR 0.000250    Time 1.712457    
2024-06-07 09:57:59,121 - Epoch: [92][  130/  193]    Overall Loss 0.000013    Objective Loss 0.000013                                        LR 0.000250    Time 1.696885    
2024-06-07 09:58:14,459 - Epoch: [92][  140/  193]    Overall Loss 0.000012    Objective Loss 0.000012                                        LR 0.000250    Time 1.684734    
2024-06-07 09:58:27,676 - Epoch: [92][  150/  193]    Overall Loss 0.000018    Objective Loss 0.000018                                        LR 0.000250    Time 1.660148    
2024-06-07 09:58:40,854 - Epoch: [92][  160/  193]    Overall Loss 0.000018    Objective Loss 0.000018                                        LR 0.000250    Time 1.638263    
2024-06-07 09:58:53,689 - Epoch: [92][  170/  193]    Overall Loss 0.000017    Objective Loss 0.000017                                        LR 0.000250    Time 1.616989    
2024-06-07 09:59:06,464 - Epoch: [92][  180/  193]    Overall Loss 0.000017    Objective Loss 0.000017                                        LR 0.000250    Time 1.597812    
2024-06-07 09:59:18,941 - Epoch: [92][  190/  193]    Overall Loss 0.000017    Objective Loss 0.000017                                        LR 0.000250    Time 1.579108    
2024-06-07 09:59:21,895 - Epoch: [92][  193/  193]    Overall Loss 0.000017    Objective Loss 0.000017    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.569646    
2024-06-07 09:59:22,683 - --- validate (epoch=92)-----------
2024-06-07 09:59:22,683 - 2172 samples (64 per mini-batch)
2024-06-07 09:59:55,622 - Epoch: [92][   10/   34]    Loss 0.000004    Top1 100.000000    Top5 100.000000    
2024-06-07 10:00:02,187 - Epoch: [92][   20/   34]    Loss 0.000008    Top1 100.000000    Top5 100.000000    
2024-06-07 10:00:08,596 - Epoch: [92][   30/   34]    Loss 0.000009    Top1 100.000000    Top5 100.000000    
2024-06-07 10:00:10,937 - Epoch: [92][   34/   34]    Loss 0.000009    Top1 100.000000    Top5 100.000000    
2024-06-07 10:00:12,076 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 10:00:12,076 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 10:00:12,322 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 92]
2024-06-07 10:00:12,323 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 10:00:12,341 - 

2024-06-07 10:00:12,342 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 10:00:45,023 - Epoch: [93][   10/  193]    Overall Loss 0.000006    Objective Loss 0.000006                                        LR 0.000250    Time 3.268049    
2024-06-07 10:00:55,860 - Epoch: [93][   20/  193]    Overall Loss 0.000019    Objective Loss 0.000019                                        LR 0.000250    Time 2.173975    
2024-06-07 10:01:06,661 - Epoch: [93][   30/  193]    Overall Loss 0.000019    Objective Loss 0.000019                                        LR 0.000250    Time 1.808075    
2024-06-07 10:01:18,020 - Epoch: [93][   40/  193]    Overall Loss 0.000016    Objective Loss 0.000016                                        LR 0.000250    Time 1.639266    
2024-06-07 10:01:29,390 - Epoch: [93][   50/  193]    Overall Loss 0.000030    Objective Loss 0.000030                                        LR 0.000250    Time 1.538270    
2024-06-07 10:01:41,226 - Epoch: [93][   60/  193]    Overall Loss 0.000027    Objective Loss 0.000027                                        LR 0.000250    Time 1.478488    
2024-06-07 10:01:53,168 - Epoch: [93][   70/  193]    Overall Loss 0.000031    Objective Loss 0.000031                                        LR 0.000250    Time 1.437471    
2024-06-07 10:02:04,117 - Epoch: [93][   80/  193]    Overall Loss 0.000028    Objective Loss 0.000028                                        LR 0.000250    Time 1.394044    
2024-06-07 10:02:15,373 - Epoch: [93][   90/  193]    Overall Loss 0.000025    Objective Loss 0.000025                                        LR 0.000250    Time 1.363807    
2024-06-07 10:02:26,242 - Epoch: [93][  100/  193]    Overall Loss 0.000025    Objective Loss 0.000025                                        LR 0.000250    Time 1.335448    
2024-06-07 10:02:37,580 - Epoch: [93][  110/  193]    Overall Loss 0.000023    Objective Loss 0.000023                                        LR 0.000250    Time 1.316788    
2024-06-07 10:02:48,538 - Epoch: [93][  120/  193]    Overall Loss 0.000022    Objective Loss 0.000022                                        LR 0.000250    Time 1.298138    
2024-06-07 10:02:59,359 - Epoch: [93][  130/  193]    Overall Loss 0.000021    Objective Loss 0.000021                                        LR 0.000250    Time 1.281167    
2024-06-07 10:03:10,215 - Epoch: [93][  140/  193]    Overall Loss 0.000020    Objective Loss 0.000020                                        LR 0.000250    Time 1.266922    
2024-06-07 10:03:21,381 - Epoch: [93][  150/  193]    Overall Loss 0.000019    Objective Loss 0.000019                                        LR 0.000250    Time 1.256664    
2024-06-07 10:03:32,580 - Epoch: [93][  160/  193]    Overall Loss 0.000018    Objective Loss 0.000018                                        LR 0.000250    Time 1.247936    
2024-06-07 10:03:43,772 - Epoch: [93][  170/  193]    Overall Loss 0.000017    Objective Loss 0.000017                                        LR 0.000250    Time 1.240185    
2024-06-07 10:03:54,906 - Epoch: [93][  180/  193]    Overall Loss 0.000016    Objective Loss 0.000016                                        LR 0.000250    Time 1.232892    
2024-06-07 10:04:05,857 - Epoch: [93][  190/  193]    Overall Loss 0.000016    Objective Loss 0.000016                                        LR 0.000250    Time 1.225454    
2024-06-07 10:04:08,469 - Epoch: [93][  193/  193]    Overall Loss 0.000016    Objective Loss 0.000016    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.219771    
2024-06-07 10:04:09,172 - --- validate (epoch=93)-----------
2024-06-07 10:04:09,173 - 2172 samples (64 per mini-batch)
2024-06-07 10:04:49,169 - Epoch: [93][   10/   34]    Loss 0.000005    Top1 100.000000    Top5 100.000000    
2024-06-07 10:04:55,568 - Epoch: [93][   20/   34]    Loss 0.000005    Top1 100.000000    Top5 100.000000    
2024-06-07 10:05:01,698 - Epoch: [93][   30/   34]    Loss 0.000007    Top1 100.000000    Top5 100.000000    
2024-06-07 10:05:03,964 - Epoch: [93][   34/   34]    Loss 0.000006    Top1 100.000000    Top5 100.000000    
2024-06-07 10:05:04,991 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 10:05:04,991 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 10:05:05,235 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 93]
2024-06-07 10:05:05,235 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 10:05:05,253 - 

2024-06-07 10:05:05,253 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 10:05:40,559 - Epoch: [94][   10/  193]    Overall Loss 0.000003    Objective Loss 0.000003                                        LR 0.000250    Time 3.530368    
2024-06-07 10:05:52,570 - Epoch: [94][   20/  193]    Overall Loss 0.000003    Objective Loss 0.000003                                        LR 0.000250    Time 2.363373    
2024-06-07 10:06:04,575 - Epoch: [94][   30/  193]    Overall Loss 0.000003    Objective Loss 0.000003                                        LR 0.000250    Time 1.974352    
2024-06-07 10:06:16,382 - Epoch: [94][   40/  193]    Overall Loss 0.000005    Objective Loss 0.000005                                        LR 0.000250    Time 1.774413    
2024-06-07 10:06:28,809 - Epoch: [94][   50/  193]    Overall Loss 0.000005    Objective Loss 0.000005                                        LR 0.000250    Time 1.667338    
2024-06-07 10:06:40,685 - Epoch: [94][   60/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.586788    
2024-06-07 10:06:52,707 - Epoch: [94][   70/  193]    Overall Loss 0.000011    Objective Loss 0.000011                                        LR 0.000250    Time 1.531318    
2024-06-07 10:07:04,828 - Epoch: [94][   80/  193]    Overall Loss 0.000010    Objective Loss 0.000010                                        LR 0.000250    Time 1.490867    
2024-06-07 10:07:17,403 - Epoch: [94][   90/  193]    Overall Loss 0.000009    Objective Loss 0.000009                                        LR 0.000250    Time 1.464455    
2024-06-07 10:07:30,012 - Epoch: [94][  100/  193]    Overall Loss 0.000009    Objective Loss 0.000009                                        LR 0.000250    Time 1.443692    
2024-06-07 10:07:42,620 - Epoch: [94][  110/  193]    Overall Loss 0.000008    Objective Loss 0.000008                                        LR 0.000250    Time 1.426654    
2024-06-07 10:07:55,210 - Epoch: [94][  120/  193]    Overall Loss 0.000008    Objective Loss 0.000008                                        LR 0.000250    Time 1.412322    
2024-06-07 10:08:07,517 - Epoch: [94][  130/  193]    Overall Loss 0.000007    Objective Loss 0.000007                                        LR 0.000250    Time 1.397999    
2024-06-07 10:08:19,680 - Epoch: [94][  140/  193]    Overall Loss 0.000007    Objective Loss 0.000007                                        LR 0.000250    Time 1.384728    
2024-06-07 10:08:32,076 - Epoch: [94][  150/  193]    Overall Loss 0.000007    Objective Loss 0.000007                                        LR 0.000250    Time 1.374674    
2024-06-07 10:08:44,480 - Epoch: [94][  160/  193]    Overall Loss 0.000007    Objective Loss 0.000007                                        LR 0.000250    Time 1.365981    
2024-06-07 10:08:56,793 - Epoch: [94][  170/  193]    Overall Loss 0.000007    Objective Loss 0.000007                                        LR 0.000250    Time 1.357823    
2024-06-07 10:09:09,147 - Epoch: [94][  180/  193]    Overall Loss 0.000007    Objective Loss 0.000007                                        LR 0.000250    Time 1.350794    
2024-06-07 10:09:21,039 - Epoch: [94][  190/  193]    Overall Loss 0.000007    Objective Loss 0.000007                                        LR 0.000250    Time 1.342061    
2024-06-07 10:09:23,892 - Epoch: [94][  193/  193]    Overall Loss 0.000007    Objective Loss 0.000007    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.335701    
2024-06-07 10:09:24,618 - --- validate (epoch=94)-----------
2024-06-07 10:09:24,618 - 2172 samples (64 per mini-batch)
2024-06-07 10:09:55,017 - Epoch: [94][   10/   34]    Loss 0.000003    Top1 100.000000    Top5 100.000000    
2024-06-07 10:10:01,785 - Epoch: [94][   20/   34]    Loss 0.000005    Top1 100.000000    Top5 100.000000    
2024-06-07 10:10:08,168 - Epoch: [94][   30/   34]    Loss 0.000004    Top1 100.000000    Top5 100.000000    
2024-06-07 10:10:10,574 - Epoch: [94][   34/   34]    Loss 0.000004    Top1 100.000000    Top5 100.000000    
2024-06-07 10:10:11,664 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 10:10:11,665 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 10:10:11,922 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 94]
2024-06-07 10:10:11,923 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 10:10:11,941 - 

2024-06-07 10:10:11,941 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 10:10:47,842 - Epoch: [95][   10/  193]    Overall Loss 0.000002    Objective Loss 0.000002                                        LR 0.000250    Time 3.589958    
2024-06-07 10:11:00,573 - Epoch: [95][   20/  193]    Overall Loss 0.000003    Objective Loss 0.000003                                        LR 0.000250    Time 2.429117    
2024-06-07 10:11:12,682 - Epoch: [95][   30/  193]    Overall Loss 0.000003    Objective Loss 0.000003                                        LR 0.000250    Time 2.021876    
2024-06-07 10:11:26,839 - Epoch: [95][   40/  193]    Overall Loss 0.000003    Objective Loss 0.000003                                        LR 0.000250    Time 1.868923    
2024-06-07 10:11:39,277 - Epoch: [95][   50/  193]    Overall Loss 0.000003    Objective Loss 0.000003                                        LR 0.000250    Time 1.742657    
2024-06-07 10:11:51,736 - Epoch: [95][   60/  193]    Overall Loss 0.000003    Objective Loss 0.000003                                        LR 0.000250    Time 1.659311    
2024-06-07 10:12:04,059 - Epoch: [95][   70/  193]    Overall Loss 0.000006    Objective Loss 0.000006                                        LR 0.000250    Time 1.597723    
2024-06-07 10:12:16,175 - Epoch: [95][   80/  193]    Overall Loss 0.000007    Objective Loss 0.000007                                        LR 0.000250    Time 1.549028    
2024-06-07 10:12:28,366 - Epoch: [95][   90/  193]    Overall Loss 0.000006    Objective Loss 0.000006                                        LR 0.000250    Time 1.511929    
2024-06-07 10:12:40,702 - Epoch: [95][  100/  193]    Overall Loss 0.000006    Objective Loss 0.000006                                        LR 0.000250    Time 1.483781    
2024-06-07 10:12:53,278 - Epoch: [95][  110/  193]    Overall Loss 0.000006    Objective Loss 0.000006                                        LR 0.000250    Time 1.462856    
2024-06-07 10:13:05,612 - Epoch: [95][  120/  193]    Overall Loss 0.000006    Objective Loss 0.000006                                        LR 0.000250    Time 1.443448    
2024-06-07 10:13:18,033 - Epoch: [95][  130/  193]    Overall Loss 0.000005    Objective Loss 0.000005                                        LR 0.000250    Time 1.427680    
2024-06-07 10:13:30,803 - Epoch: [95][  140/  193]    Overall Loss 0.000005    Objective Loss 0.000005                                        LR 0.000250    Time 1.416503    
2024-06-07 10:13:43,182 - Epoch: [95][  150/  193]    Overall Loss 0.000005    Objective Loss 0.000005                                        LR 0.000250    Time 1.404160    
2024-06-07 10:13:56,610 - Epoch: [95][  160/  193]    Overall Loss 0.000005    Objective Loss 0.000005                                        LR 0.000250    Time 1.400130    
2024-06-07 10:14:09,024 - Epoch: [95][  170/  193]    Overall Loss 0.000005    Objective Loss 0.000005                                        LR 0.000250    Time 1.390517    
2024-06-07 10:14:21,567 - Epoch: [95][  180/  193]    Overall Loss 0.000005    Objective Loss 0.000005                                        LR 0.000250    Time 1.382748    
2024-06-07 10:14:34,489 - Epoch: [95][  190/  193]    Overall Loss 0.000006    Objective Loss 0.000006                                        LR 0.000250    Time 1.377785    
2024-06-07 10:14:37,351 - Epoch: [95][  193/  193]    Overall Loss 0.000005    Objective Loss 0.000005    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.371020    
2024-06-07 10:14:38,113 - --- validate (epoch=95)-----------
2024-06-07 10:14:38,113 - 2172 samples (64 per mini-batch)
2024-06-07 10:15:11,502 - Epoch: [95][   10/   34]    Loss 0.000009    Top1 100.000000    Top5 100.000000    
2024-06-07 10:15:21,545 - Epoch: [95][   20/   34]    Loss 0.000013    Top1 100.000000    Top5 100.000000    
2024-06-07 10:15:29,006 - Epoch: [95][   30/   34]    Loss 0.000010    Top1 100.000000    Top5 100.000000    
2024-06-07 10:15:31,715 - Epoch: [95][   34/   34]    Loss 0.000009    Top1 100.000000    Top5 100.000000    
2024-06-07 10:15:33,026 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 10:15:33,026 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 10:15:33,303 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 95]
2024-06-07 10:15:33,304 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 10:15:33,335 - 

2024-06-07 10:15:33,335 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 10:16:16,868 - Epoch: [96][   10/  193]    Overall Loss 0.000006    Objective Loss 0.000006                                        LR 0.000250    Time 4.353218    
2024-06-07 10:16:29,261 - Epoch: [96][   20/  193]    Overall Loss 0.000006    Objective Loss 0.000006                                        LR 0.000250    Time 2.792909    
2024-06-07 10:16:40,227 - Epoch: [96][   30/  193]    Overall Loss 0.000005    Objective Loss 0.000005                                        LR 0.000250    Time 2.226414    
2024-06-07 10:16:51,815 - Epoch: [96][   40/  193]    Overall Loss 0.000005    Objective Loss 0.000005                                        LR 0.000250    Time 1.958683    
2024-06-07 10:17:02,772 - Epoch: [96][   50/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.785371    
2024-06-07 10:17:13,734 - Epoch: [96][   60/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.669861    
2024-06-07 10:17:24,900 - Epoch: [96][   70/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.590376    
2024-06-07 10:17:35,911 - Epoch: [96][   80/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.528630    
2024-06-07 10:17:46,907 - Epoch: [96][   90/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.480560    
2024-06-07 10:17:58,005 - Epoch: [96][  100/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.443126    
2024-06-07 10:18:09,627 - Epoch: [96][  110/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.417278    
2024-06-07 10:18:21,056 - Epoch: [96][  120/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.394188    
2024-06-07 10:18:32,296 - Epoch: [96][  130/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.373126    
2024-06-07 10:18:43,177 - Epoch: [96][  140/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.352417    
2024-06-07 10:18:54,601 - Epoch: [96][  150/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.338218    
2024-06-07 10:19:06,022 - Epoch: [96][  160/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.325749    
2024-06-07 10:19:16,969 - Epoch: [96][  170/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.311977    
2024-06-07 10:19:28,447 - Epoch: [96][  180/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.302645    
2024-06-07 10:19:39,045 - Epoch: [96][  190/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.289661    
2024-06-07 10:19:41,563 - Epoch: [96][  193/  193]    Overall Loss 0.000004    Objective Loss 0.000004    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.282479    
2024-06-07 10:19:42,343 - --- validate (epoch=96)-----------
2024-06-07 10:19:42,343 - 2172 samples (64 per mini-batch)
2024-06-07 10:20:38,376 - Epoch: [96][   10/   34]    Loss 0.000004    Top1 100.000000    Top5 100.000000    
2024-06-07 10:20:43,574 - Epoch: [96][   20/   34]    Loss 0.000018    Top1 100.000000    Top5 100.000000    
2024-06-07 10:20:49,373 - Epoch: [96][   30/   34]    Loss 0.000013    Top1 100.000000    Top5 100.000000    
2024-06-07 10:20:51,496 - Epoch: [96][   34/   34]    Loss 0.000012    Top1 100.000000    Top5 100.000000    
2024-06-07 10:20:52,660 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 10:20:52,661 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 10:20:52,872 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 96]
2024-06-07 10:20:52,872 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 10:20:52,908 - 

2024-06-07 10:20:52,908 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 10:21:27,922 - Epoch: [97][   10/  193]    Overall Loss 0.000007    Objective Loss 0.000007                                        LR 0.000250    Time 3.501314    
2024-06-07 10:21:39,369 - Epoch: [97][   20/  193]    Overall Loss 0.000006    Objective Loss 0.000006                                        LR 0.000250    Time 2.320704    
2024-06-07 10:21:50,479 - Epoch: [97][   30/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.916327    
2024-06-07 10:22:01,708 - Epoch: [97][   40/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.716835    
2024-06-07 10:22:12,766 - Epoch: [97][   50/  193]    Overall Loss 0.000003    Objective Loss 0.000003                                        LR 0.000250    Time 1.593783    
2024-06-07 10:22:23,747 - Epoch: [97][   60/  193]    Overall Loss 0.000003    Objective Loss 0.000003                                        LR 0.000250    Time 1.510407    
2024-06-07 10:22:34,785 - Epoch: [97][   70/  193]    Overall Loss 0.000003    Objective Loss 0.000003                                        LR 0.000250    Time 1.451688    
2024-06-07 10:22:45,759 - Epoch: [97][   80/  193]    Overall Loss 0.000003    Objective Loss 0.000003                                        LR 0.000250    Time 1.406975    
2024-06-07 10:22:56,864 - Epoch: [97][   90/  193]    Overall Loss 0.000003    Objective Loss 0.000003                                        LR 0.000250    Time 1.373569    
2024-06-07 10:23:07,822 - Epoch: [97][  100/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.345392    
2024-06-07 10:23:18,704 - Epoch: [97][  110/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.321663    
2024-06-07 10:23:30,061 - Epoch: [97][  120/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.305853    
2024-06-07 10:23:41,159 - Epoch: [97][  130/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.290458    
2024-06-07 10:23:51,958 - Epoch: [97][  140/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.274945    
2024-06-07 10:24:03,125 - Epoch: [97][  150/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.264151    
2024-06-07 10:24:14,107 - Epoch: [97][  160/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.253545    
2024-06-07 10:24:25,120 - Epoch: [97][  170/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.244303    
2024-06-07 10:24:36,070 - Epoch: [97][  180/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.235773    
2024-06-07 10:24:46,754 - Epoch: [97][  190/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.226757    
2024-06-07 10:24:49,290 - Epoch: [97][  193/  193]    Overall Loss 0.000004    Objective Loss 0.000004    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.220590    
2024-06-07 10:24:50,085 - --- validate (epoch=97)-----------
2024-06-07 10:24:50,086 - 2172 samples (64 per mini-batch)
2024-06-07 10:25:20,030 - Epoch: [97][   10/   34]    Loss 0.000002    Top1 100.000000    Top5 100.000000    
2024-06-07 10:25:26,241 - Epoch: [97][   20/   34]    Loss 0.000004    Top1 100.000000    Top5 100.000000    
2024-06-07 10:25:32,511 - Epoch: [97][   30/   34]    Loss 0.000003    Top1 100.000000    Top5 100.000000    
2024-06-07 10:25:34,724 - Epoch: [97][   34/   34]    Loss 0.000003    Top1 100.000000    Top5 100.000000    
2024-06-07 10:25:35,895 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 10:25:35,895 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 10:25:36,075 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 97]
2024-06-07 10:25:36,075 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 10:25:36,103 - 

2024-06-07 10:25:36,104 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 10:26:12,305 - Epoch: [98][   10/  193]    Overall Loss 0.000002    Objective Loss 0.000002                                        LR 0.000250    Time 3.620012    
2024-06-07 10:26:23,418 - Epoch: [98][   20/  193]    Overall Loss 0.000003    Objective Loss 0.000003                                        LR 0.000250    Time 2.363954    
2024-06-07 10:26:34,393 - Epoch: [98][   30/  193]    Overall Loss 0.000002    Objective Loss 0.000002                                        LR 0.000250    Time 1.940462    
2024-06-07 10:26:45,454 - Epoch: [98][   40/  193]    Overall Loss 0.000003    Objective Loss 0.000003                                        LR 0.000250    Time 1.731043    
2024-06-07 10:26:56,535 - Epoch: [98][   50/  193]    Overall Loss 0.000003    Objective Loss 0.000003                                        LR 0.000250    Time 1.605844    
2024-06-07 10:27:07,620 - Epoch: [98][   60/  193]    Overall Loss 0.000003    Objective Loss 0.000003                                        LR 0.000250    Time 1.522193    
2024-06-07 10:27:18,617 - Epoch: [98][   70/  193]    Overall Loss 0.000012    Objective Loss 0.000012                                        LR 0.000250    Time 1.461458    
2024-06-07 10:27:29,549 - Epoch: [98][   80/  193]    Overall Loss 0.000011    Objective Loss 0.000011                                        LR 0.000250    Time 1.414912    
2024-06-07 10:27:40,486 - Epoch: [98][   90/  193]    Overall Loss 0.000025    Objective Loss 0.000025                                        LR 0.000250    Time 1.378867    
2024-06-07 10:27:51,385 - Epoch: [98][  100/  193]    Overall Loss 0.000025    Objective Loss 0.000025                                        LR 0.000250    Time 1.349615    
2024-06-07 10:28:02,442 - Epoch: [98][  110/  193]    Overall Loss 0.000023    Objective Loss 0.000023                                        LR 0.000250    Time 1.327093    
2024-06-07 10:28:13,296 - Epoch: [98][  120/  193]    Overall Loss 0.000022    Objective Loss 0.000022                                        LR 0.000250    Time 1.306683    
2024-06-07 10:28:24,588 - Epoch: [98][  130/  193]    Overall Loss 0.000020    Objective Loss 0.000020                                        LR 0.000250    Time 1.292739    
2024-06-07 10:28:35,633 - Epoch: [98][  140/  193]    Overall Loss 0.000019    Objective Loss 0.000019                                        LR 0.000250    Time 1.279022    
2024-06-07 10:28:46,580 - Epoch: [98][  150/  193]    Overall Loss 0.000019    Objective Loss 0.000019                                        LR 0.000250    Time 1.266502    
2024-06-07 10:28:57,687 - Epoch: [98][  160/  193]    Overall Loss 0.000018    Objective Loss 0.000018                                        LR 0.000250    Time 1.256566    
2024-06-07 10:29:08,584 - Epoch: [98][  170/  193]    Overall Loss 0.000017    Objective Loss 0.000017                                        LR 0.000250    Time 1.246471    
2024-06-07 10:29:19,644 - Epoch: [98][  180/  193]    Overall Loss 0.000016    Objective Loss 0.000016                                        LR 0.000250    Time 1.238468    
2024-06-07 10:29:30,484 - Epoch: [98][  190/  193]    Overall Loss 0.000016    Objective Loss 0.000016                                        LR 0.000250    Time 1.230033    
2024-06-07 10:29:32,973 - Epoch: [98][  193/  193]    Overall Loss 0.000016    Objective Loss 0.000016    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.223627    
2024-06-07 10:29:33,719 - --- validate (epoch=98)-----------
2024-06-07 10:29:33,720 - 2172 samples (64 per mini-batch)
2024-06-07 10:30:04,566 - Epoch: [98][   10/   34]    Loss 0.000019    Top1 100.000000    Top5 100.000000    
2024-06-07 10:30:10,724 - Epoch: [98][   20/   34]    Loss 0.000011    Top1 100.000000    Top5 100.000000    
2024-06-07 10:30:16,530 - Epoch: [98][   30/   34]    Loss 0.000008    Top1 100.000000    Top5 100.000000    
2024-06-07 10:30:18,668 - Epoch: [98][   34/   34]    Loss 0.000008    Top1 100.000000    Top5 100.000000    
2024-06-07 10:30:19,820 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 10:30:19,821 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   0 313   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 10:30:20,019 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 98]
2024-06-07 10:30:20,020 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 10:30:20,047 - 

2024-06-07 10:30:20,048 - Training epoch: 12313 samples (64 per mini-batch)
2024-06-07 10:30:58,030 - Epoch: [99][   10/  193]    Overall Loss 0.000005    Objective Loss 0.000005                                        LR 0.000250    Time 3.798078    
2024-06-07 10:31:09,193 - Epoch: [99][   20/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 2.454841    
2024-06-07 10:31:20,184 - Epoch: [99][   30/  193]    Overall Loss 0.000003    Objective Loss 0.000003                                        LR 0.000250    Time 2.001843    
2024-06-07 10:31:31,369 - Epoch: [99][   40/  193]    Overall Loss 0.000005    Objective Loss 0.000005                                        LR 0.000250    Time 1.780276    
2024-06-07 10:31:42,546 - Epoch: [99][   50/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.646979    
2024-06-07 10:31:53,585 - Epoch: [99][   60/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.555771    
2024-06-07 10:32:04,715 - Epoch: [99][   70/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.492064    
2024-06-07 10:32:15,734 - Epoch: [99][   80/  193]    Overall Loss 0.000004    Objective Loss 0.000004                                        LR 0.000250    Time 1.442914    
2024-06-07 10:32:27,859 - Epoch: [99][   90/  193]    Overall Loss 0.000109    Objective Loss 0.000109                                        LR 0.000250    Time 1.416939    
2024-06-07 10:32:39,148 - Epoch: [99][  100/  193]    Overall Loss 0.001408    Objective Loss 0.001408                                        LR 0.000250    Time 1.387732    
2024-06-07 10:32:58,365 - Epoch: [99][  110/  193]    Overall Loss 0.001476    Objective Loss 0.001476                                        LR 0.000250    Time 1.435869    
2024-06-07 10:33:18,209 - Epoch: [99][  120/  193]    Overall Loss 0.001419    Objective Loss 0.001419                                        LR 0.000250    Time 1.480979    
2024-06-07 10:33:38,222 - Epoch: [99][  130/  193]    Overall Loss 0.001326    Objective Loss 0.001326                                        LR 0.000250    Time 1.520429    
2024-06-07 10:33:57,922 - Epoch: [99][  140/  193]    Overall Loss 0.001258    Objective Loss 0.001258                                        LR 0.000250    Time 1.551992    
2024-06-07 10:34:16,175 - Epoch: [99][  150/  193]    Overall Loss 0.001182    Objective Loss 0.001182                                        LR 0.000250    Time 1.569691    
2024-06-07 10:34:32,327 - Epoch: [99][  160/  193]    Overall Loss 0.001125    Objective Loss 0.001125                                        LR 0.000250    Time 1.572107    
2024-06-07 10:34:50,687 - Epoch: [99][  170/  193]    Overall Loss 0.001063    Objective Loss 0.001063                                        LR 0.000250    Time 1.587186    
2024-06-07 10:35:05,232 - Epoch: [99][  180/  193]    Overall Loss 0.001031    Objective Loss 0.001031                                        LR 0.000250    Time 1.579569    
2024-06-07 10:35:24,801 - Epoch: [99][  190/  193]    Overall Loss 0.000989    Objective Loss 0.000989                                        LR 0.000250    Time 1.599044    
2024-06-07 10:35:29,562 - Epoch: [99][  193/  193]    Overall Loss 0.000976    Objective Loss 0.000976    Top1 100.000000    Top5 100.000000    LR 0.000250    Time 1.598478    
2024-06-07 10:35:30,595 - --- validate (epoch=99)-----------
2024-06-07 10:35:30,596 - 2172 samples (64 per mini-batch)
2024-06-07 10:36:33,210 - Epoch: [99][   10/   34]    Loss 0.000178    Top1 100.000000    Top5 100.000000    
2024-06-07 10:36:41,823 - Epoch: [99][   20/   34]    Loss 0.000294    Top1 100.000000    Top5 100.000000    
2024-06-07 10:36:51,029 - Epoch: [99][   30/   34]    Loss 0.002080    Top1 99.947917    Top5 100.000000    
2024-06-07 10:36:54,934 - Epoch: [99][   34/   34]    Loss 0.001903    Top1 99.953959    Top5 100.000000    
2024-06-07 10:36:56,544 - ==> Top1: 99.954    Top5: 100.000    Loss: 0.002

2024-06-07 10:36:56,544 - ==> Confusion:
[[227   0   0   0   0   0   0   0   0]
 [  0 231   0   0   0   0   0   0   0]
 [  0   0 228   0   0   0   0   0   0]
 [  0   0   0 319   0   0   0   0   0]
 [  0   0   0   1 312   0   0   0   0]
 [  0   0   0   0   0  54   0   0   0]
 [  0   0   0   0   0   0 246   0   0]
 [  0   0   0   0   0   0   0 263   0]
 [  0   0   0   0   0   0   0   0 291]]

2024-06-07 10:36:56,967 - ==> Best [Top1: 100.000   Top5: 100.000   Sparsity:0.00   Params: 64944 on epoch: 98]
2024-06-07 10:36:56,967 - Saving checkpoint to: logs\2024.06.06-144451\qat_checkpoint.pth.tar
2024-06-07 10:36:56,979 - --- test ---------------------
2024-06-07 10:36:56,979 - 725 samples (64 per mini-batch)
2024-06-07 10:37:34,904 - Test: [   10/   12]    Loss 0.000002    Top1 100.000000    Top5 100.000000    
2024-06-07 10:37:36,182 - Test: [   12/   12]    Loss 0.000005    Top1 100.000000    Top5 100.000000    
2024-06-07 10:37:37,112 - ==> Top1: 100.000    Top5: 100.000    Loss: 0.000

2024-06-07 10:37:37,113 - ==> Confusion:
[[ 89   0   0   0   0   0   0   0   0]
 [  0  77   0   0   0   0   0   0   0]
 [  0   0  77   0   0   0   0   0   0]
 [  0   0   0  97   0   0   0   0   0]
 [  0   0   0   0 112   0   0   0   0]
 [  0   0   0   0   0  10   0   0   0]
 [  0   0   0   0   0   0 100   0   0]
 [  0   0   0   0   0   0   0  79   0]
 [  0   0   0   0   0   0   0   0  84]]

2024-06-07 10:37:37,174 - 
2024-06-07 10:37:37,175 - Log file for this run: C:\BChimgam\projectAI\ai8x-training\logs\2024.06.06-144451\2024.06.06-144451.log
